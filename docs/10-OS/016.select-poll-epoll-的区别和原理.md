---
title: select、poll、epoll 的区别和原理
tags:
  - 操作系统
status: robot
class: 操作系统
slug: select-poll-epoll-difference-principle
ref:
---

## 核心要点

**三者都是 Linux 下的 IO 多路复用机制,用一个线程监控多个文件描述符的就绪状态:**
- **select**: 基于数组,有 1024 上限,每次调用需拷贝整个 fd_set,内核需遍历所有 fd
- **poll**: 基于链表,无数量限制,但同样需要拷贝和遍历所有 fd
- **epoll**: 基于红黑树+就绪队列,只返回活跃 fd,支持边缘触发,性能最优

**性能对比**: `epoll >> poll > select` (连接数越多差距越大)

---

## 详细回答

### 一、什么是 IO 多路复用

在高并发服务器中,如果为每个连接创建一个线程处理 IO,会导致:
- 线程上下文切换开销大
- 内存消耗高(每个线程需要独立栈空间)

**IO 多路复用** 允许单个线程同时监听多个 socket,当某个 socket 有数据到达时,内核通知应用程序,应用程序再去处理对应的 socket。

---

### 二、select 机制

#### 原理

```c
int select(int nfds, fd_set *readfds, fd_set *writefds,
           fd_set *exceptfds, struct timeval *timeout);
```

**工作流程:**
1. 应用程序将需要监控的 fd 集合从用户态拷贝到内核态
2. 内核遍历所有 fd,检查是否就绪(可读/可写/异常)
3. 内核将整个 fd_set 拷贝回用户态
4. 应用程序再次遍历 fd_set,找出就绪的 fd

**数据结构:**
```c
typedef struct {
    long fds_bits[1024/8/sizeof(long)];  // 位图,每个 bit 代表一个 fd
} fd_set;
```

#### 缺点

1. **文件描述符数量限制**: 默认 1024 (由 `FD_SETSIZE` 宏定义)
2. **性能问题**:
   - 每次调用都需要将 fd_set 从用户态拷贝到内核态
   - 内核需要遍历整个 fd_set,时间复杂度 O(n)
   - 返回时需要再次遍历找出就绪的 fd
3. **fd_set 可能被修改**: 返回后 fd_set 会被内核修改,下次调用前需要重新设置

#### 适用场景

- 监控少量 fd (< 100)
- 需要跨平台支持(Windows 的 Winsock 也支持 select)

---

### 三、poll 机制

#### 原理

```c
int poll(struct pollfd *fds, nfds_t nfds, int timeout);

struct pollfd {
    int fd;         // 文件描述符
    short events;   // 关心的事件(POLLIN/POLLOUT)
    short revents;  // 实际发生的事件(内核填充)
};
```

**改进点:**
- 使用 `pollfd` 结构体数组,没有文件描述符数量限制
- 将"关心的事件"和"实际发生的事件"分离,避免重复设置

**工作流程:**
1. 应用程序将 `pollfd` 数组拷贝到内核态
2. 内核遍历数组,检查每个 fd 的状态
3. 将结果填充到 `revents` 字段,拷贝回用户态
4. 应用程序遍历数组,检查 `revents` 字段

#### 缺点

1. **仍需拷贝整个数组**: 每次调用都需要拷贝 nfds 个 pollfd 结构
2. **仍需遍历所有 fd**: 内核需要遍历所有 fd,时间复杂度仍是 O(n)
3. **水平触发**: 只要 fd 就绪就会通知,可能重复通知

#### 相比 select 的优势

- 没有 fd 数量限制
- events 和 revents 分离,使用更清晰
- 可以监听更多事件类型(如 POLLPRI、POLLHUP)

---

### 四、epoll 机制

#### 原理

epoll 使用三个系统调用:

```c
// 1. 创建 epoll 实例
int epoll_create(int size);  // size 参数已忽略,但必须 > 0

// 2. 注册/修改/删除监控的 fd
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

// 3. 等待事件发生
int epoll_wait(int epfd, struct epoll_event *events,
               int maxevents, int timeout);
```

**核心数据结构:**
1. **红黑树** (epitem): 存储所有被监控的 fd
2. **就绪队列** (rdllist): 存储已就绪的 fd (双向链表)

**工作流程:**

```
应用程序              内核 epoll 实例
    |                     |
    | epoll_ctl(ADD)      |
    |-------------------->| 添加 fd 到红黑树
    |                     | 注册回调函数到 fd 的等待队列
    |                     |
    | epoll_wait()        |
    |-------------------->| 检查就绪队列
    |       阻塞          |
    |                     |
    |                [数据到达]
    |                     |
    |                 驱动程序/协议栈
    |                 调用回调函数
    |                 将 fd 加入就绪队列
    |                     |
    | 返回就绪 fd 数组    |
    |<--------------------|
```

#### 关键优化

**1. 避免重复拷贝**
- fd 只需通过 `epoll_ctl` 注册一次,内核会一直记住
- `epoll_wait` 只返回就绪的 fd,而不是全部 fd

**2. 避免遍历**
- 使用回调机制,当 fd 就绪时,内核主动将其加入就绪队列
- `epoll_wait` 只需检查就绪队列,时间复杂度 O(1)

**3. 支持大量并发连接**
- 使用红黑树管理 fd,查找/插入/删除时间复杂度 O(log n)
- 内存映射技术 (mmap) 减少内存拷贝

#### epoll 的两种触发模式

**水平触发 (Level Triggered, LT)** - 默认模式
- 只要 fd 处于就绪状态,每次 `epoll_wait` 都会返回
- 即使数据没读完,下次调用仍会通知
- 安全,不易丢失事件

**边缘触发 (Edge Triggered, ET)**
- 只在 fd 状态变化时通知(从未就绪→就绪)
- 必须一次性读完所有数据,否则剩余数据不会再通知
- 性能更高,但编程更复杂(需配合非阻塞 IO 和循环读取)

```c
struct epoll_event ev;
ev.events = EPOLLIN | EPOLLET;  // 边缘触发
ev.data.fd = sockfd;
epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);
```

---

### 五、性能对比

| 特性 | select | poll | epoll |
|------|--------|------|-------|
| **fd 数量限制** | 1024 (可修改但影响性能) | 无限制 | 无限制 |
| **fd 拷贝** | 每次都拷贝整个 fd_set | 每次都拷贝整个 pollfd 数组 | fd 只拷贝一次 |
| **查找就绪 fd** | 用户态遍历 O(n) | 用户态遍历 O(n) | 内核直接返回就绪 fd O(1) |
| **内核遍历** | 遍历所有 fd O(n) | 遍历所有 fd O(n) | 回调机制 O(1) |
| **触发模式** | 水平触发 | 水平触发 | 水平触发 + 边缘触发 |
| **内存管理** | 栈上位图 | 堆上数组 | 红黑树 + mmap |
| **适用场景** | < 100 连接 | < 1000 连接 | > 1000 连接 |

**性能测试数据** (100 万并发连接,其中 1000 活跃):
- select/poll: 遍历 100 万次,耗时 ~ 数秒
- epoll: 只返回 1000 个活跃 fd,耗时 ~ 毫秒级

---

### 六、实际应用示例

#### select 示例
```c
fd_set readfds;
FD_ZERO(&readfds);
FD_SET(sockfd, &readfds);

int ret = select(sockfd + 1, &readfds, NULL, NULL, NULL);
if (ret > 0 && FD_ISSET(sockfd, &readfds)) {
    // sockfd 可读
    read(sockfd, buf, sizeof(buf));
}
```

#### epoll 示例
```c
int epfd = epoll_create(1);
struct epoll_event ev, events[MAX_EVENTS];

ev.events = EPOLLIN;
ev.data.fd = sockfd;
epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);

while (1) {
    int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);
    for (int i = 0; i < nfds; i++) {
        if (events[i].events & EPOLLIN) {
            int fd = events[i].data.fd;
            read(fd, buf, sizeof(buf));
        }
    }
}
```

---

### 七、总结

**选择建议:**
- **少量连接 (< 100)**: select 足够,代码简单,跨平台
- **中等连接 (100 ~ 1000)**: poll,避免 select 的 fd 数量限制
- **海量连接 (> 1000)**: epoll,高性能服务器首选 (Nginx、Redis)

**为什么 epoll 性能最高?**
1. 只拷贝活跃 fd,而不是全部 fd
2. 使用回调机制,避免遍历
3. 红黑树管理 fd,支持海量连接
4. 支持边缘触发,减少无效唤醒

**面试加分项:**
- 理解 epoll 的红黑树和就绪队列数据结构
- 能说明 ET 模式下为什么必须一次性读完数据
- 知道 Nginx 使用 epoll 的 ET 模式实现高并发
