---
title: Linux 的五种 IO 模型（阻塞、非阻塞、IO多路复用、信号驱动、异步）
tags:
  - 操作系统
status: robot
class: 操作系统
slug: linux-five-io-models
ref:
---

## 核心要点

Linux的五种IO模型按效率递增：**阻塞IO**（最简单，等待数据）、**非阻塞IO**（轮询检查）、**IO多路复用**（select/poll/epoll，单线程管理多连接）、**信号驱动IO**（异步通知，很少用）、**异步IO**（真正异步，内核完成全部操作）。核心区别在于**等待数据和拷贝数据**两阶段的阻塞情况。实际高性能服务器多用epoll（IO多路复用）。

---

## 详细回答

### 一、IO操作的两个阶段

在理解IO模型前，需要明确任何IO操作都包含两个阶段：

```
阶段1: 等待数据就绪 (Waiting for data)
  - 对于网络IO：等待数据从网络到达内核缓冲区
  - 对于磁盘IO：等待数据从磁盘读取到内核缓冲区

阶段2: 数据拷贝 (Copying data)
  - 将数据从内核缓冲区拷贝到用户空间
```

**五种IO模型的区别**：就在于这两个阶段中，进程是否阻塞、如何处理。

**关键概念**
- **同步IO**：进程会在某个阶段阻塞等待（包括阻塞、非阻塞、IO多路复用、信号驱动）
- **异步IO**：进程完全不阻塞，内核完成所有操作后通知进程

---

### 二、阻塞IO（Blocking I/O）

**最传统、最简单的IO模型**

**工作流程**
```
应用进程                     内核
   |                          |
   |------ recvfrom() ------→ |
   |                          |  等待数据
   |        阻塞等待           |  ↓ 阶段1
   |                          |  数据到达内核缓冲区
   |                          |  ↓ 阶段2
   |                          |  拷贝数据到用户空间
   |←----- 返回数据 ----------|
   |                          |
   | 处理数据                  |
```

**代码示例**
```c
int sockfd = socket(AF_INET, SOCK_STREAM, 0);
// ... bind, listen, accept ...

char buf[1024];
int n = recv(sockfd, buf, sizeof(buf), 0);  // 阻塞在这里
// 直到有数据到达，recv才返回
printf("Received: %s\n", buf);
```

**特点**
- 进程在两个阶段都**阻塞等待**
- 最简单直观，代码易懂
- 一个线程只能处理一个连接（需要多线程/多进程处理并发）

**优点**
- 编程简单，逻辑清晰
- 不占用CPU（阻塞时线程休眠）

**缺点**
- **并发能力差**：C10K问题（10000个并发连接需要10000个线程，资源消耗巨大）
- 线程切换开销大
- 内存占用高（每个线程需要栈空间，如1MB × 10000 = 10GB）

**适用场景**
- 连接数少的场景（如命令行工具）
- 与线程池结合，处理中等并发

---

### 三、非阻塞IO（Non-blocking I/O）

**通过轮询检查数据是否就绪**

**工作流程**
```
应用进程                     内核
   |                          |
   |------ recvfrom() ------→ |  无数据
   |←---- EWOULDBLOCK --------|
   |                          |
   |------ recvfrom() ------→ |  无数据
   |←---- EWOULDBLOCK --------|
   |                          |
   |------ recvfrom() ------→ |  数据到达
   |        阻塞等待           |  拷贝数据 (阶段2阻塞)
   |←----- 返回数据 ----------|
   |                          |
```

**代码示例**
```c
int sockfd = socket(AF_INET, SOCK_STREAM, 0);
// 设置为非阻塞
int flags = fcntl(sockfd, F_GETFL, 0);
fcntl(sockfd, F_SETFL, flags | O_NONBLOCK);

// 轮询检查
while (1) {
    int n = recv(sockfd, buf, sizeof(buf), 0);
    if (n < 0) {
        if (errno == EWOULDBLOCK || errno == EAGAIN) {
            // 没有数据，继续轮询
            usleep(1000);  // 稍微等待，避免CPU 100%
            continue;
        } else {
            // 真正的错误
            perror("recv");
            break;
        }
    } else if (n == 0) {
        // 连接关闭
        break;
    } else {
        // 收到数据
        printf("Received: %s\n", buf);
        break;
    }
}
```

**特点**
- **阶段1不阻塞**（轮询检查）
- **阶段2仍然阻塞**（数据拷贝时）
- 需要不断发起系统调用询问

**优点**
- 不会一直阻塞，可以同时检查多个socket
- 单线程可处理多个连接

**缺点**
- **CPU占用高**：不断轮询，即使没有数据也在消耗CPU
- **轮询效率低**：需要主动反复询问
- 很少单独使用（通常与IO多路复用结合）

**适用场景**
- 几乎不单独使用
- 通常作为IO多路复用的基础（将socket设为非阻塞，配合epoll使用）

---

### 四、IO多路复用（I/O Multiplexing）

**最常用的高性能IO模型**（select/poll/epoll）

**核心思想**
- 使用单个线程监控多个文件描述符
- 通过系统调用（如epoll）等待多个socket中的任意一个变为可读

**工作流程**
```
应用进程                     内核
   |                          |
   |------ epoll_wait() ----→ |  监控多个socket
   |        阻塞等待           |  ↓ 等待任意一个就绪
   |                          |  socket A 数据到达
   |←---- 返回就绪列表 --------|
   |                          |
   |------ recvfrom(A) -----→ |
   |        阻塞等待           |  拷贝数据 (阶段2阻塞)
   |←----- 返回数据 ----------|
   |                          |
```

**select/poll/epoll对比**

| 特性 | select | poll | epoll |
|------|--------|------|-------|
| **文件描述符限制** | 1024（FD_SETSIZE） | 无限制 | 无限制 |
| **性能** | O(n) | O(n) | O(1) |
| **数据结构** | bitmap | 数组 | 红黑树 + 双向链表 |
| **事件通知** | 遍历所有fd | 遍历所有fd | 只返回就绪的fd |
| **跨平台** | 是（POSIX） | 是 | 否（Linux专属） |
| **适用场景** | 少量连接 | 中等连接 | **高并发（推荐）** |

**epoll代码示例**
```c
// 1. 创建epoll实例
int epfd = epoll_create1(0);

// 2. 添加监听的socket
struct epoll_event ev;
ev.events = EPOLLIN;  // 监听可读事件
ev.data.fd = sockfd;
epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);

// 3. 等待事件
struct epoll_event events[MAX_EVENTS];
while (1) {
    int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);  // 阻塞等待

    for (int i = 0; i < nfds; i++) {
        if (events[i].events & EPOLLIN) {
            int fd = events[i].data.fd;
            // 处理可读事件
            recv(fd, buf, sizeof(buf), 0);
        }
    }
}
```

**epoll的两种触发模式**

**1. 水平触发（Level Triggered, LT）** - 默认模式
- 只要fd就绪，epoll_wait就返回
- 即使没有读完数据，下次epoll_wait仍会通知
- 类似"电平信号"，持续通知
- **适合阻塞IO**，更安全

**2. 边缘触发（Edge Triggered, ET）**
- 只在fd状态变化时通知一次
- 必须一次性读完所有数据（需配合非阻塞IO）
- 类似"脉冲信号"，只通知一次
- **性能更高**，但编程复杂

**特点**
- **阶段1阻塞在select/epoll**（等待任意socket就绪）
- **阶段2仍然阻塞**（recvfrom拷贝数据）
- 单线程可处理海量并发连接

**优点**
- **高并发能力**：单线程管理数万连接（C10K问题的解决方案）
- 无需线程切换
- epoll性能优秀（O(1)复杂度）

**缺点**
- 仍然是同步IO（recvfrom时阻塞）
- 编程复杂度高于阻塞IO

**适用场景**
- **高性能网络服务器**（Nginx、Redis、Node.js）
- 需要同时处理大量连接
- 目前最主流的高性能IO方案

---

### 五、信号驱动IO（Signal-driven I/O）

**使用信号机制通知数据就绪**

**工作流程**
```
应用进程                     内核
   |                          |
   |--- 注册SIGIO信号处理 ---→ |
   |←------ 立即返回 ----------|
   |                          |
   | 继续执行其他任务           |  等待数据
   |                          |  ↓
   |                          |  数据到达
   |←---- 发送SIGIO信号 -------|
   | (信号处理函数)             |
   |                          |
   |------ recvfrom() ------→ |
   |        阻塞等待           |  拷贝数据 (阶段2阻塞)
   |←----- 返回数据 ----------|
```

**代码示例**
```c
void sigio_handler(int sig) {
    // 信号处理函数
    char buf[1024];
    recv(sockfd, buf, sizeof(buf), 0);
    printf("Received: %s\n", buf);
}

int main() {
    // 注册信号处理函数
    signal(SIGIO, sigio_handler);

    // 设置socket为信号驱动
    fcntl(sockfd, F_SETOWN, getpid());
    int flags = fcntl(sockfd, F_GETFL);
    fcntl(sockfd, F_SETFL, flags | O_ASYNC);

    // 继续执行其他任务
    while (1) {
        // 主循环做其他事情
        // 数据到达时会触发sigio_handler
    }
}
```

**特点**
- **阶段1异步**（内核通过信号通知）
- **阶段2同步阻塞**（recvfrom拷贝数据）
- 进程可以继续执行其他任务

**优点**
- 不需要轮询或阻塞等待
- 进程可以继续工作

**缺点**
- **信号处理复杂**：信号可能丢失、合并
- **可移植性差**：不同系统实现差异大
- **实际很少使用**（不如IO多路复用成熟）

**适用场景**
- 几乎不用（理论意义大于实际意义）
- 被IO多路复用和异步IO取代

---

### 六、异步IO（Asynchronous I/O）

**真正的异步IO，POSIX AIO**

**工作流程**
```
应用进程                     内核
   |                          |
   |------ aio_read() ------→ |  发起异步读
   |←------ 立即返回 ----------|  （不阻塞）
   |                          |
   | 继续执行其他任务           |  ↓ 等待数据 (阶段1)
   |                          |  ↓ 拷贝数据 (阶段2)
   |                          |  完成全部操作
   |←--- 发送完成信号 ---------|
   | (处理已完成的数据)         |
```

**代码示例**
```c
#include <aio.h>

struct aiocb cb;
memset(&cb, 0, sizeof(cb));
cb.aio_fildes = fd;
cb.aio_buf = buf;
cb.aio_nbytes = sizeof(buf);
cb.aio_sigevent.sigev_notify = SIGEV_SIGNAL;
cb.aio_sigevent.sigev_signo = SIGUSR1;

// 发起异步读（立即返回）
aio_read(&cb);

// 继续执行其他任务
while (1) {
    // ...

    // 检查是否完成
    if (aio_error(&cb) == 0) {
        // 读取完成
        int ret = aio_return(&cb);
        printf("Read %d bytes\n", ret);
        break;
    }
}
```

**Linux的io_uring**（现代异步IO）
```c
// io_uring是Linux 5.1+引入的新异步IO框架
// 比传统AIO性能更好、功能更强

struct io_uring ring;
io_uring_queue_init(256, &ring, 0);

// 提交异步读请求
struct io_uring_sqe *sqe = io_uring_get_sqe(&ring);
io_uring_prep_read(sqe, fd, buf, sizeof(buf), 0);
io_uring_submit(&ring);

// 等待完成
struct io_uring_cqe *cqe;
io_uring_wait_cqe(&ring, &cqe);
printf("Read %d bytes\n", cqe->res);
io_uring_cqe_seen(&ring, cqe);
```

**特点**
- **两个阶段都不阻塞**
- 内核完成全部操作（等待 + 拷贝）后通知应用
- 真正的异步

**优点**
- **性能最高**：完全无阻塞
- 充分利用CPU和IO
- 适合IO密集型应用

**缺点**
- **编程复杂度最高**
- Linux传统AIO实现不完善（仅支持直接IO）
- **io_uring较新**（需要Linux 5.1+）

**适用场景**
- 高性能数据库、文件服务器
- 需要极致性能的场景
- io_uring是未来趋势

---

### 七、五种IO模型总结对比

| IO模型 | 阶段1（等待数据） | 阶段2（拷贝数据） | 同步/异步 | 并发能力 | 使用场景 |
|--------|-----------------|-----------------|---------|---------|---------|
| **阻塞IO** | 阻塞 | 阻塞 | 同步 | 低（需多线程） | 简单场景、客户端 |
| **非阻塞IO** | 不阻塞（轮询） | 阻塞 | 同步 | 中（CPU占用高） | 几乎不单独用 |
| **IO多路复用** | 阻塞在select/epoll | 阻塞 | 同步 | **高** | **服务器（推荐）** |
| **信号驱动IO** | 异步（信号通知） | 阻塞 | 同步 | 中 | 几乎不用 |
| **异步IO** | 不阻塞 | 不阻塞 | **异步** | **极高** | 高性能场景、未来趋势 |

**关键理解**
- **同步IO**：进程参与IO过程，至少在数据拷贝阶段阻塞（前4种）
- **异步IO**：进程完全不参与，内核完成全部操作后通知（第5种）

---

### 八、实际应用案例

**Nginx**
- 使用epoll（Linux）/ kqueue（BSD）
- IO多路复用 + 非阻塞IO
- 单个worker进程处理数千连接

**Redis**
- 单线程 + IO多路复用（epoll）
- 纯内存操作 + 高效IO
- 10万+ QPS

**Node.js**
- 底层用libuv（封装epoll/kqueue/IOCP）
- 事件驱动 + 非阻塞IO
- 适合高并发Web服务

**Apache（传统模式）**
- 阻塞IO + 多进程/多线程
- 每个连接一个线程/进程
- C10K问题的"反面教材"（现已支持event模式）

---

### 九、面试回答策略

**简洁版本**（3分钟）
"Linux有五种IO模型，区别在于等待数据和拷贝数据两个阶段的阻塞情况：

1. **阻塞IO**：两个阶段都阻塞，最简单但并发差，需要多线程
2. **非阻塞IO**：轮询检查数据，CPU占用高，很少单独使用
3. **IO多路复用**：使用select/poll/epoll监控多个socket，单线程处理高并发，是目前主流高性能方案，Nginx、Redis都用这个
4. **信号驱动IO**：数据到达时发信号通知，实际很少用
5. **异步IO**：唯一真正异步的模型，内核完成全部操作后通知，性能最高但编程复杂，io_uring是未来趋势

前四种都是同步IO（至少在数据拷贝阶段阻塞），只有异步IO完全不阻塞。实际开发中，高性能服务器主要用IO多路复用（epoll），未来会更多采用异步IO（io_uring）。"

**深入版本**（5-7分钟）
在简洁版基础上，可以展开：
1. **详细流程**：画出epoll的工作流程图，对比阻塞IO
2. **epoll优势**：对比select/poll/epoll的性能差异（O(1) vs O(n)）
3. **实际案例**：Nginx如何用epoll实现高并发（C10K问题）
4. **触发模式**：epoll的LT和ET模式区别
5. **新技术**：io_uring的改进点和未来展望

**追问应对**
- "select和epoll的区别？"→ 详述文件描述符限制、性能复杂度、数据结构（见对比表）
- "epoll为什么高效？"→ 红黑树管理fd、就绪链表直接返回、避免遍历
- "什么是LT和ET？"→ 水平触发vs边缘触发，配合代码示例说明
- "同步IO和异步IO的本质区别？"→ 数据拷贝阶段是否阻塞
- "为什么Nginx用epoll不用异步IO？"→ 传统AIO不成熟、网络IO场景IO多路复用足够高效
- "io_uring相比传统AIO的优势？"→ 零拷贝、批量提交、支持所有IO类型
