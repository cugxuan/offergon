---
title: 设计一个分布式限流系统
tags:
  - 分布式
  - 系统设计
status: robot
class: 系统设计
slug: distributed-rate-limiting-system-design
ref:
---

## 核心要点

**关键技术**: 令牌桶算法、滑动窗口、Redis+Lua、分布式一致性、热点限流
**性能目标**: 毫秒级响应、99.99%可用性、支持百万级QPS、误差<1%
**核心挑战**: 分布式环境下的精确计数、高并发性能、配置动态更新、多维度限流

## 详细设计方案

### 一、系统架构设计

#### 1.1 整体架构
```
┌──────────────────────────────────────────────────────────┐
│                      客户端应用层                         │
└─────────────────────┬────────────────────────────────────┘
                      ↓
         ┌────────────────────────────┐
         │    限流SDK (本地+远程)      │
         └────────┬───────────────────┘
                  ↓
    ┌─────────────┼─────────────┐
    ↓             ↓             ↓
┌────────┐  ┌──────────┐  ┌──────────┐
│限流网关│  │限流服务集群│  │配置中心  │
└───┬────┘  └────┬─────┘  └────┬─────┘
    ↓            ↓             ↓
┌──────────────────────────────────┐
│         Redis集群 (存储计数)      │
└──────────────────────────────────┘
         ↓                    ↓
┌─────────────┐      ┌──────────────┐
│  监控告警   │      │  持久化存储  │
└─────────────┘      └──────────────┘
```

#### 1.2 核心组件
- **限流SDK**: 客户端集成,支持本地限流+远程调用
- **限流网关**: 统一入口,集中式限流决策
- **限流服务**: 核心计算引擎,处理限流逻辑
- **配置中心**: 动态配置管理,实时生效
- **Redis集群**: 分布式计数存储,高性能读写
- **监控告警**: 实时监控限流状态,异常报警

### 二、限流算法设计

#### 2.1 令牌桶算法(Token Bucket) - 推荐

**原理**: 以固定速率生成令牌,请求消耗令牌,桶满则丢弃新令牌

**优点**:
- 支持突发流量(桶容量大小)
- 平滑限流,不会瞬时拒绝
- 适合API限流场景

**Go实现**:
```go
type TokenBucket struct {
    capacity  int64     // 桶容量
    rate      float64   // 令牌生成速率(个/秒)
    tokens    float64   // 当前令牌数
    lastTime  time.Time // 上次更新时间
    mu        sync.Mutex
}

func NewTokenBucket(capacity int64, rate float64) *TokenBucket {
    return &TokenBucket{
        capacity: capacity,
        rate:     rate,
        tokens:   float64(capacity),
        lastTime: time.Now(),
    }
}

func (tb *TokenBucket) Allow() bool {
    tb.mu.Lock()
    defer tb.mu.Unlock()

    now := time.Now()
    // 计算这段时间内生成的令牌数
    elapsed := now.Sub(tb.lastTime).Seconds()
    tb.tokens += elapsed * tb.rate

    // 令牌数不能超过桶容量
    if tb.tokens > float64(tb.capacity) {
        tb.tokens = float64(tb.capacity)
    }

    tb.lastTime = now

    // 如果有令牌则消耗一个
    if tb.tokens >= 1 {
        tb.tokens -= 1
        return true
    }

    return false
}
```

**Redis+Lua分布式实现**:
```lua
-- key: rate_limit:{resource}
-- argv[1]: capacity (桶容量)
-- argv[2]: rate (令牌速率)
-- argv[3]: requested (请求令牌数,默认1)
-- argv[4]: now (当前时间戳,毫秒)

local key = KEYS[1]
local capacity = tonumber(ARGV[1])
local rate = tonumber(ARGV[2])
local requested = tonumber(ARGV[3])
local now = tonumber(ARGV[4])

-- 获取当前令牌数和上次更新时间
local tokens = tonumber(redis.call('HGET', key, 'tokens'))
local last_time = tonumber(redis.call('HGET', key, 'last_time'))

if tokens == nil then
    tokens = capacity
    last_time = now
end

-- 计算新增令牌数
local elapsed = math.max(0, now - last_time) / 1000
local new_tokens = math.min(capacity, tokens + elapsed * rate)

-- 判断是否有足够令牌
local allowed = 0
if new_tokens >= requested then
    new_tokens = new_tokens - requested
    allowed = 1
end

-- 更新Redis
redis.call('HMSET', key, 'tokens', new_tokens, 'last_time', now)
redis.call('EXPIRE', key, 3600)

return allowed
```

**Go调用Lua脚本**:
```go
type RedisTokenBucket struct {
    client   *redis.Client
    script   *redis.Script
}

func (rtb *RedisTokenBucket) Allow(resource string, capacity int64, rate float64) (bool, error) {
    now := time.Now().UnixMilli()

    result, err := rtb.script.Run(
        context.Background(),
        rtb.client,
        []string{fmt.Sprintf("rate_limit:%s", resource)},
        capacity,
        rate,
        1,  // 请求1个令牌
        now,
    ).Int()

    if err != nil {
        return false, err
    }

    return result == 1, nil
}
```

#### 2.2 滑动窗口算法(Sliding Window)

**原理**: 将时间分成多个小窗口,统计滑动窗口内的请求数

**优点**:
- 精确控制时间窗口内的请求量
- 避免固定窗口的临界问题
- 适合QPS限流场景

**Redis实现(基于ZSet)**:
```lua
-- key: rate_limit:{resource}
-- argv[1]: limit (限流阈值)
-- argv[2]: window (时间窗口,秒)
-- argv[3]: now (当前时间戳)

local key = KEYS[1]
local limit = tonumber(ARGV[1])
local window = tonumber(ARGV[2])
local now = tonumber(ARGV[3])

-- 移除窗口外的记录
redis.call('ZREMRANGEBYSCORE', key, 0, now - window * 1000)

-- 统计当前窗口内的请求数
local current = redis.call('ZCARD', key)

if current < limit then
    -- 添加当前请求
    redis.call('ZADD', key, now, now .. math.random())
    redis.call('EXPIRE', key, window + 1)
    return 1
else
    return 0
end
```

**Go实现**:
```go
type SlidingWindowLimiter struct {
    client *redis.Client
    script *redis.Script
}

func (sw *SlidingWindowLimiter) Allow(resource string, limit int64, window int) (bool, error) {
    now := time.Now().UnixMilli()

    result, err := sw.script.Run(
        context.Background(),
        sw.client,
        []string{fmt.Sprintf("rate_limit:%s", resource)},
        limit,
        window,
        now,
    ).Int()

    if err != nil {
        return false, err
    }

    return result == 1, nil
}
```

#### 2.3 漏桶算法(Leaky Bucket)

**原理**: 请求进入队列,以固定速率流出处理

**优点**:
- 强制限速,流量绝对平滑
- 适合需要严格控制处理速率的场景

**Go实现**:
```go
type LeakyBucket struct {
    capacity    int64       // 桶容量
    rate        float64     // 流出速率
    water       int64       // 当前水量
    lastLeakTime time.Time  // 上次漏水时间
    mu          sync.Mutex
}

func (lb *LeakyBucket) Allow() bool {
    lb.mu.Lock()
    defer lb.mu.Unlock()

    now := time.Now()
    // 计算漏出的水量
    elapsed := now.Sub(lb.lastLeakTime).Seconds()
    leaked := int64(elapsed * lb.rate)

    lb.water = max(0, lb.water - leaked)
    lb.lastLeakTime = now

    // 如果还有空间,则允许进入
    if lb.water < lb.capacity {
        lb.water++
        return true
    }

    return false
}
```

### 三、数据库设计

#### 3.1 限流规则表
```sql
CREATE TABLE rate_limit_rule (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    resource_id VARCHAR(255) UNIQUE NOT NULL,  -- 资源标识(API路径/用户ID等)
    limit_type ENUM('QPS', 'QPM', 'QPH', 'QPD'), -- 限流类型
    threshold INT NOT NULL,                     -- 限流阈值
    algorithm ENUM('TOKEN_BUCKET', 'SLIDING_WINDOW', 'LEAKY_BUCKET'),
    capacity INT,                               -- 令牌桶容量
    rate FLOAT,                                 -- 生成速率
    window_size INT,                            -- 窗口大小(秒)
    dimension VARCHAR(50),                      -- 限流维度(IP/USER/APP)
    enabled BOOLEAN DEFAULT TRUE,               -- 是否启用
    priority INT DEFAULT 0,                     -- 优先级(多规则匹配时)
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_resource (resource_id, enabled),
    INDEX idx_dimension (dimension)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

**示例数据**:
```sql
-- API限流: /api/user/info 每秒100次
INSERT INTO rate_limit_rule (resource_id, limit_type, threshold, algorithm, capacity, rate, dimension)
VALUES ('/api/user/info', 'QPS', 100, 'TOKEN_BUCKET', 200, 100, 'API');

-- 用户限流: 单用户每分钟1000次
INSERT INTO rate_limit_rule (resource_id, limit_type, threshold, algorithm, window_size, dimension)
VALUES ('user:*', 'QPM', 1000, 'SLIDING_WINDOW', 60, 'USER');

-- IP限流: 单IP每秒10次
INSERT INTO rate_limit_rule (resource_id, limit_type, threshold, algorithm, window_size, dimension)
VALUES ('ip:*', 'QPS', 10, 'SLIDING_WINDOW', 1, 'IP');
```

#### 3.2 限流日志表(分区表)
```sql
CREATE TABLE rate_limit_log (
    id BIGINT AUTO_INCREMENT,
    resource_id VARCHAR(255),
    dimension_key VARCHAR(255),        -- 维度值(具体IP/USER_ID)
    result ENUM('ALLOW', 'DENY'),
    current_count INT,                 -- 当前计数
    threshold INT,                     -- 限流阈值
    timestamp BIGINT,                  -- 时间戳(毫秒)
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (id, created_at),
    INDEX idx_resource_time (resource_id, timestamp)
) ENGINE=InnoDB
PARTITION BY RANGE (UNIX_TIMESTAMP(created_at)) (
    PARTITION p202501 VALUES LESS THAN (UNIX_TIMESTAMP('2025-02-01')),
    PARTITION p202502 VALUES LESS THAN (UNIX_TIMESTAMP('2025-03-01')),
    PARTITION p202503 VALUES LESS THAN (UNIX_TIMESTAMP('2025-04-01')),
    PARTITION p_max VALUES LESS THAN MAXVALUE
);
```

### 四、多维度限流设计

#### 4.1 限流维度
```go
type RateLimitDimension string

const (
    DimensionGlobal  RateLimitDimension = "global"   // 全局限流
    DimensionAPI     RateLimitDimension = "api"      // API级别
    DimensionUser    RateLimitDimension = "user"     // 用户级别
    DimensionIP      RateLimitDimension = "ip"       // IP级别
    DimensionApp     RateLimitDimension = "app"      // 应用级别
    DimensionTenant  RateLimitDimension = "tenant"   // 租户级别
)

type RateLimitKey struct {
    Dimension   RateLimitDimension
    ResourceID  string
    DimensionKey string  // 具体值,如user_id=123
}

func (k *RateLimitKey) String() string {
    return fmt.Sprintf("ratelimit:%s:%s:%s", k.Dimension, k.ResourceID, k.DimensionKey)
}
```

#### 4.2 多维度组合限流
```go
type MultiDimensionLimiter struct {
    limiters map[RateLimitDimension]RateLimiter
    rules    *RuleManager
}

func (m *MultiDimensionLimiter) Allow(ctx context.Context, req *Request) (bool, error) {
    // 按优先级检查多个维度
    dimensions := []struct {
        dim   RateLimitDimension
        key   string
    }{
        {DimensionGlobal, "all"},
        {DimensionAPI, req.Path},
        {DimensionUser, req.UserID},
        {DimensionIP, req.ClientIP},
    }

    for _, d := range dimensions {
        // 获取该维度的限流规则
        rule := m.rules.GetRule(d.dim, d.key)
        if rule == nil || !rule.Enabled {
            continue
        }

        // 执行限流检查
        limiter := m.limiters[d.dim]
        allowed, err := limiter.Allow(rule.ResourceID, rule.Threshold, rule.Window)
        if err != nil {
            return false, err
        }

        if !allowed {
            // 记录限流日志
            logRateLimitDeny(d.dim, d.key, rule)
            return false, nil
        }
    }

    return true, nil
}
```

### 五、分布式一致性设计

#### 5.1 主从架构
```
┌─────────────────────────────────────┐
│         限流服务主节点 (Master)      │
│  - 配置管理                         │
│  - 规则下发                         │
│  - 计数聚合                         │
└──────────┬──────────────────────────┘
           ↓ (Raft/Etcd选举)
    ┌──────┴──────┬─────────┐
    ↓             ↓         ↓
┌────────┐   ┌────────┐  ┌────────┐
│Worker 1│   │Worker 2│  │Worker 3│
└───┬────┘   └───┬────┘  └───┬────┘
    ↓            ↓           ↓
┌──────────────────────────────┐
│      Redis Cluster           │
└──────────────────────────────┘
```

#### 5.2 最终一致性方案
```go
// 本地计数 + 定期同步
type HybridLimiter struct {
    localCounter  *LocalCounter   // 进程内计数器
    redisLimiter  *RedisLimiter   // Redis分布式计数器
    syncInterval  time.Duration   // 同步间隔
}

func (h *HybridLimiter) Allow(resource string, limit int64) (bool, error) {
    // 1. 先检查本地计数(快速路径)
    localCount := h.localCounter.Incr(resource)
    localLimit := limit / int64(workerCount) // 按worker数分配配额

    if localCount > localLimit {
        // 2. 本地配额用尽,向Redis申请
        allowed, err := h.redisLimiter.Allow(resource, limit)
        if err != nil || !allowed {
            h.localCounter.Decr(resource) // 回滚本地计数
            return false, err
        }
    }

    return true, nil
}

// 定期同步本地计数到Redis
func (h *HybridLimiter) syncLoop() {
    ticker := time.NewTicker(h.syncInterval)
    for range ticker.C {
        counters := h.localCounter.GetAndReset()
        for resource, count := range counters {
            h.redisLimiter.AddCount(resource, count)
        }
    }
}
```

#### 5.3 热点限流优化
```go
// 使用本地缓存减少Redis压力
type HotKeyLimiter struct {
    localCache   *sync.Map  // resource -> *TokenBucket
    redisLimiter *RedisLimiter
    hotThreshold int64      // 热点阈值(QPS)
}

func (h *HotKeyLimiter) Allow(resource string, limit int64, rate float64) (bool, error) {
    // 如果是热点资源,优先使用本地限流
    if limit > h.hotThreshold {
        if bucket, ok := h.localCache.Load(resource); ok {
            return bucket.(*TokenBucket).Allow(), nil
        }

        // 初始化本地令牌桶
        bucket := NewTokenBucket(limit, rate)
        h.localCache.Store(resource, bucket)
        return bucket.Allow(), nil
    }

    // 非热点资源使用Redis限流
    return h.redisLimiter.Allow(resource, limit, rate)
}
```

### 六、配置动态更新

#### 6.1 配置中心集成
```go
type ConfigWatcher struct {
    etcdClient *clientv3.Client
    ruleCache  *sync.Map
    callbacks  []func(*RateLimitRule)
}

func (w *ConfigWatcher) Watch(ctx context.Context) {
    watchChan := w.etcdClient.Watch(ctx, "rate_limit/rules/", clientv3.WithPrefix())

    for resp := range watchChan {
        for _, event := range resp.Events {
            switch event.Type {
            case mvccpb.PUT:
                var rule RateLimitRule
                json.Unmarshal(event.Kv.Value, &rule)

                // 更新本地缓存
                w.ruleCache.Store(rule.ResourceID, &rule)

                // 触发回调
                for _, cb := range w.callbacks {
                    cb(&rule)
                }

            case mvccpb.DELETE:
                w.ruleCache.Delete(string(event.Kv.Key))
            }
        }
    }
}
```

#### 6.2 热更新实现
```go
type RateLimiterService struct {
    limiters  map[string]RateLimiter
    rules     *sync.Map
    mu        sync.RWMutex
}

func (s *RateLimiterService) UpdateRule(rule *RateLimitRule) {
    s.mu.Lock()
    defer s.mu.Unlock()

    // 更新规则
    s.rules.Store(rule.ResourceID, rule)

    // 重新初始化限流器
    var limiter RateLimiter
    switch rule.Algorithm {
    case "TOKEN_BUCKET":
        limiter = NewRedisTokenBucket(rule.Capacity, rule.Rate)
    case "SLIDING_WINDOW":
        limiter = NewSlidingWindowLimiter(rule.Threshold, rule.WindowSize)
    }

    s.limiters[rule.ResourceID] = limiter

    log.Infof("Rate limit rule updated: %s", rule.ResourceID)
}
```

### 七、高可用设计

#### 7.1 故障降级
```go
type FallbackLimiter struct {
    primary   RateLimiter
    fallback  RateLimiter  // 本地限流器
    breaker   *CircuitBreaker
}

func (f *FallbackLimiter) Allow(resource string, limit int64) (bool, error) {
    // 检查熔断器状态
    if f.breaker.IsOpen() {
        // 降级到本地限流
        return f.fallback.Allow(resource, limit)
    }

    allowed, err := f.primary.Allow(resource, limit)
    if err != nil {
        f.breaker.RecordFailure()
        // 失败时降级
        return f.fallback.Allow(resource, limit)
    }

    f.breaker.RecordSuccess()
    return allowed, nil
}
```

#### 7.2 Redis高可用
```go
// 使用Redis哨兵模式
func NewRedisClient() *redis.Client {
    return redis.NewFailoverClient(&redis.FailoverOptions{
        MasterName:    "rate-limiter-master",
        SentinelAddrs: []string{"sentinel1:26379", "sentinel2:26379", "sentinel3:26379"},
        DB:            0,
        PoolSize:      100,
        MinIdleConns:  10,
        MaxRetries:    3,
    })
}

// Redis集群模式
func NewRedisCluster() *redis.ClusterClient {
    return redis.NewClusterClient(&redis.ClusterOptions{
        Addrs: []string{
            "redis1:6379",
            "redis2:6379",
            "redis3:6379",
        },
        PoolSize:     200,
        MinIdleConns: 20,
    })
}
```

### 八、监控告警

#### 8.1 关键指标
```go
type RateLimitMetrics struct {
    TotalRequests    int64   // 总请求数
    AllowedRequests  int64   // 通过请求数
    DeniedRequests   int64   // 拒绝请求数
    AllowRate        float64 // 通过率
    AvgLatency       int64   // 平均延迟(ms)
    P99Latency       int64   // P99延迟
    RedisErrors      int64   // Redis错误数
    FallbackCount    int64   // 降级次数
}

// Prometheus监控
var (
    rateLimitTotal = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "rate_limit_requests_total",
            Help: "Total rate limit requests",
        },
        []string{"resource", "dimension", "result"},
    )

    rateLimitLatency = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "rate_limit_latency_ms",
            Help:    "Rate limit latency distribution",
            Buckets: []float64{1, 5, 10, 50, 100, 500},
        },
        []string{"resource"},
    )
)

func recordMetrics(resource string, dimension string, allowed bool, latency time.Duration) {
    result := "allow"
    if !allowed {
        result = "deny"
    }

    rateLimitTotal.WithLabelValues(resource, dimension, result).Inc()
    rateLimitLatency.WithLabelValues(resource).Observe(float64(latency.Milliseconds()))
}
```

#### 8.2 告警规则
```yaml
groups:
  - name: rate_limiter_alerts
    rules:
      - alert: HighDenyRate
        expr: rate(rate_limit_requests_total{result="deny"}[1m]) / rate(rate_limit_requests_total[1m]) > 0.5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High rate limit deny rate: {{ $value }}"

      - alert: RedisConnectionError
        expr: rate(rate_limit_redis_errors_total[1m]) > 10
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis connection errors detected"

      - alert: HighLatency
        expr: histogram_quantile(0.99, rate(rate_limit_latency_ms_bucket[1m])) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "P99 latency > 100ms"
```

### 九、性能优化

#### 9.1 批量处理
```go
type BatchLimiter struct {
    requests chan *RateLimitRequest
    batchSize int
    flushInterval time.Duration
}

type RateLimitRequest struct {
    Resource string
    Limit    int64
    Result   chan bool
}

func (b *BatchLimiter) Allow(resource string, limit int64) (bool, error) {
    req := &RateLimitRequest{
        Resource: resource,
        Limit:    limit,
        Result:   make(chan bool, 1),
    }

    b.requests <- req

    // 等待结果
    select {
    case result := <-req.Result:
        return result, nil
    case <-time.After(100 * time.Millisecond):
        return false, errors.New("timeout")
    }
}

func (b *BatchLimiter) processBatch() {
    batch := make([]*RateLimitRequest, 0, b.batchSize)
    ticker := time.NewTicker(b.flushInterval)

    for {
        select {
        case req := <-b.requests:
            batch = append(batch, req)
            if len(batch) >= b.batchSize {
                b.flush(batch)
                batch = batch[:0]
            }

        case <-ticker.C:
            if len(batch) > 0 {
                b.flush(batch)
                batch = batch[:0]
            }
        }
    }
}

func (b *BatchLimiter) flush(batch []*RateLimitRequest) {
    // 使用Pipeline批量执行
    pipe := redisClient.Pipeline()

    for _, req := range batch {
        pipe.Eval(rateLimitScript, []string{req.Resource}, req.Limit)
    }

    results, _ := pipe.Exec(context.Background())

    for i, req := range batch {
        allowed := results[i].(*redis.Cmd).Val().(int64) == 1
        req.Result <- allowed
    }
}
```

#### 9.2 连接池优化
```go
// Redis连接池配置
redisClient := redis.NewClient(&redis.Options{
    Addr:         "localhost:6379",
    PoolSize:     500,         // 连接池大小
    MinIdleConns: 50,          // 最小空闲连接
    MaxRetries:   3,           // 重试次数
    DialTimeout:  5 * time.Second,
    ReadTimeout:  3 * time.Second,
    WriteTimeout: 3 * time.Second,
    PoolTimeout:  4 * time.Second,
})
```

### 十、容量规划

#### 10.1 QPS评估
```
假设:
- 目标QPS: 100万
- 限流操作耗时: 1ms (Redis Lua脚本)
- 单台Redis QPS: 10万

计算:
- Redis集群节点数: 100万 / 10万 = 10台
- 限流服务实例数: 100万 / (1000/1) = 1000台 (按1ms耗时)
- 考虑2倍冗余: Redis 20台, 服务2000台

网络带宽:
- 单次限流请求: 0.5KB
- 单次限流响应: 0.1KB
- 总带宽: 100万 × 0.6KB = 600MB/s = 4.8Gbps
```

#### 10.2 存储容量
```
Redis存储估算:
- 假设1000万个活跃限流key
- 每个key平均占用: 200字节 (key + value + 过期时间)
- 总存储: 1000万 × 200B = 2GB

建议配置:
- 单台Redis内存: 8GB (预留buffer)
- 采用Redis Cluster, 10主10从
- 总容量: 80GB
```

## 面试回答要点

1. **算法选择**: "我推荐令牌桶算法,支持突发流量且平滑限流。对于精确QPS控制,可用滑动窗口。核心是用Redis+Lua保证原子性"

2. **分布式实现**: "采用Redis集中计数 + 本地缓存热点数据。对于超高QPS,可用本地限流 + 定期同步到Redis的混合方案,牺牲少量精度换取性能"

3. **多维度限流**: "支持全局/API/用户/IP多维度,按优先级逐层检查。使用组合key如'ratelimit:user:123:api:/order'实现细粒度控制"

4. **高可用**: "Redis哨兵或集群模式,限流服务无状态水平扩展。故障时降级到本地限流器,保证服务可用性优先于限流精度"

5. **优化点**: "批量处理减少Redis调用、连接池复用、热点数据本地缓存、配置中心动态更新规则、完善监控告警"
