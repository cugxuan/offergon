---
title: 设计一个分布式锁服务
tags:
  - 分布式
  - 系统设计
status: robot
class: 系统设计
slug: distributed-lock-service-design
ref:
---

## 核心要点

**互斥性保证**：同一资源同时只有一个客户端能持有锁
**防死锁机制**：锁必须有过期时间，避免客户端崩溃导致永久占用
**可重入支持**：同一客户端可以多次获取同一把锁
**高可用设计**：采用主从复制或共识算法保证服务不中断
**性能优化**：通过本地缓存、异步续期降低网络开销

---

## 一、系统设计思路

分布式锁服务的核心是在多个进程或节点之间协调对共享资源的访问。这个系统需要解决以下关键问题：

### 1. 核心功能需求

**基础操作**：
- `TryLock(resource, timeout)`: 尝试获取锁，支持超时等待
- `Unlock(resource, token)`: 释放锁，需要验证 token 防止误解锁
- `Renew(resource, token)`: 续期锁的过期时间
- `IsLocked(resource)`: 查询锁状态

**高级特性**：
- 可重入锁：同一客户端可重复获取
- 读写锁：允许多个读操作并发，写操作独占
- 公平锁：按请求顺序分配锁，防止饥饿
- 锁降级：写锁可以降级为读锁

### 2. 非功能性需求

**可靠性**：
- 单点故障不能导致服务不可用
- 网络分区时保证安全性优于可用性（CP 模型）
- 锁不能被错误释放或丢失

**性能**：
- 加锁/解锁操作延迟 < 10ms (P99)
- 支持 10万+ QPS 的锁操作
- 支持 100万+ 并发锁实例

**易用性**：
- 提供多语言 SDK
- 自动续期机制，业务无感知
- 清晰的错误提示和监控

---

## 二、技术方案选型

### 方案一：基于 Redis 的实现

**核心原理**：
利用 Redis 的 `SET key value NX EX seconds` 原子命令实现加锁，通过 Lua 脚本保证解锁的原子性。

**加锁流程**：
```
1. 生成唯一 token (UUID + ThreadID)
2. 执行 SET lock:{resource} {token} NX EX 30
3. 如果返回 OK，加锁成功
4. 如果返回 nil，表示锁已被占用
```

**解锁流程**：
```
使用 Lua 脚本保证原子性：
if redis.call("GET", KEYS[1]) == ARGV[1] then
    return redis.call("DEL", KEYS[1])
else
    return 0
end
```

**优点**：
- 实现简单，性能极高（单机 10万+ QPS）
- Redis 普及度高，运维成熟
- 支持 Lua 脚本保证原子性

**缺点**：
- Redis 主从复制是异步的，主节点故障时可能丢失锁
- Redis Cluster 在网络分区时可能出现脑裂
- 不适合长时间持有的锁（需要频繁续期）

**改进方案：Redlock 算法**
为了解决单点故障问题，Redis 作者提出了 Redlock 算法：

1. 部署 5 个独立的 Redis 主节点（不是主从复制）
2. 客户端向所有节点发起加锁请求
3. 只有当超过半数节点（3/5）加锁成功，且总耗时小于锁的有效期，才认为加锁成功
4. 解锁时向所有节点发起删除请求

这样即使 2 个节点宕机，系统仍能正常工作。但 Redlock 也存在争议，在极端网络分区场景下仍可能违反互斥性。

---

### 方案二：基于 Etcd/ZooKeeper 的实现

**核心原理**：
利用强一致性的分布式协调服务（基于 Raft/ZAB 共识算法）实现锁。

**Etcd 实现方式**：

1. **创建租约**（Lease）：
   - 创建一个 30 秒的租约
   - 租约到期后，与其关联的 Key 会自动删除（防死锁）

2. **尝试获取锁**：
   - 创建 Key：`/locks/{resource}/{lease-id}` 并绑定租约
   - 获取 `/locks/{resource}/` 下的所有 Key，按创建版本号排序
   - 如果自己的 Key 是最小的，表示获取锁成功
   - 否则，监听（Watch）前一个 Key 的删除事件

3. **等待通知**：
   - 当前面的客户端释放锁时，Watch 会收到通知
   - 重新检查自己是否是最小的 Key

4. **释放锁**：
   - 删除自己的 Key 或撤销租约

**优点**：
- 强一致性保证，不会出现双主问题
- 自动故障恢复，Leader 宕机后自动选举
- 天然支持公平锁（按创建顺序排队）
- 天然防死锁（租约过期自动释放）

**缺点**：
- 性能不如 Redis（单机 1-2 万 QPS）
- 部署复杂度较高（需要 3-5 节点的集群）
- 延迟相对较高（涉及 Raft 共识）

---

### 方案三：基于数据库的实现

**核心原理**：
利用数据库的唯一索引或行锁实现分布式锁。

**MySQL 实现示例**：

```sql
-- 表结构
CREATE TABLE distributed_lock (
    lock_name VARCHAR(64) PRIMARY KEY,
    lock_token VARCHAR(64) NOT NULL,
    owner VARCHAR(128),
    expire_time TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 加锁（利用主键唯一性）
INSERT INTO distributed_lock (lock_name, lock_token, owner, expire_time)
VALUES ('order:123', 'uuid-xxx', 'server-1', NOW() + INTERVAL 30 SECOND)
ON DUPLICATE KEY UPDATE
    lock_token = IF(expire_time < NOW(), VALUES(lock_token), lock_token),
    expire_time = IF(expire_time < NOW(), VALUES(expire_time), expire_time);

-- 检查是否加锁成功
SELECT lock_token FROM distributed_lock
WHERE lock_name = 'order:123' AND lock_token = 'uuid-xxx';

-- 解锁
DELETE FROM distributed_lock
WHERE lock_name = 'order:123' AND lock_token = 'uuid-xxx';
```

**优点**：
- 无需引入新组件，降低系统复杂度
- 强一致性保证（事务 ACID）
- 容易理解和调试

**缺点**：
- 性能较差（受限于数据库 TPS）
- 存在单点问题（除非使用主从或分布式数据库）
- 锁表可能成为热点，需要定期清理过期数据

---

## 三、核心问题与解决方案

### 问题 1：如何防止误删其他客户端的锁？

**场景**：
```
1. 客户端 A 获取锁，设置过期时间 30s
2. 客户端 A 执行业务逻辑超过 30s，锁自动过期
3. 客户端 B 获取到锁
4. 客户端 A 完成业务，执行 DEL 命令，误删了 B 的锁
```

**解决方案**：
- 加锁时生成唯一 token（UUID + 线程ID + 时间戳）
- 解锁时验证 token 是否匹配
- 使用 Lua 脚本保证"检查 + 删除"的原子性

---

### 问题 2：如何处理锁过期但业务未完成？

**方案 A：业务层主动续期**
客户端启动后台线程，每隔 10 秒执行一次续期操作（锁过期时间为 30 秒）。

**方案 B：自动续期（Watchdog 机制）**
Redisson 等成熟框架提供的方案：
- 加锁成功后，启动定时任务
- 每隔 `expireTime / 3` 时间检查锁是否还被持有
- 如果持有，自动续期到 `expireTime`
- 解锁时或客户端崩溃时，定时任务自动取消

**方案 C：预估业务时间**
根据业务特点设置合理的过期时间，P99 耗时 × 2，并设置监控告警。

---

### 问题 3：如何实现可重入锁？

**Redis 实现思路**：

使用 Hash 结构存储锁信息：
```
Key: lock:{resource}
Value: {
    "clientId": "uuid-xxx",
    "threadId": 12345,
    "count": 3  // 重入次数
}
Expire: 30s
```

加锁逻辑：
```lua
local key = KEYS[1]
local clientId = ARGV[1]
local expireTime = ARGV[2]

if redis.call('exists', key) == 0 then
    redis.call('hset', key, clientId, 1)
    redis.call('expire', key, expireTime)
    return 1
elseif redis.call('hexists', key, clientId) == 1 then
    redis.call('hincrby', key, clientId, 1)
    redis.call('expire', key, expireTime)
    return 1
else
    return 0
end
```

解锁逻辑：
```lua
local key = KEYS[1]
local clientId = ARGV[1]

if redis.call('hexists', key, clientId) == 0 then
    return nil
end

local count = redis.call('hincrby', key, clientId, -1)
if count > 0 then
    return 0
else
    redis.call('del', key)
    return 1
end
```

---

### 问题 4：如何实现公平锁？

**Redis 实现方案**：

使用 List 或 Sorted Set 维护等待队列：

```
1. 客户端加锁时，先将自己的 clientId 加入队列
   ZADD lock:{resource}:queue {timestamp} {clientId}

2. 检查队列头部是否是自己
   ZRANGE lock:{resource}:queue 0 0

3. 如果是自己，尝试获取锁
   SET lock:{resource} {clientId} NX EX 30

4. 如果不是，使用 BLPOP 阻塞等待通知
   或轮询检查队列状态

5. 释放锁时，通知下一个等待者
   PUBLISH lock:{resource}:channel "released"
```

**Etcd 实现**：
天然支持公平锁，因为按照 Key 的创建版本号（Revision）排序，先创建的先获得锁。

---

### 问题 5：如何处理网络分区？

**场景**：
```
1. 客户端 A 获取锁成功
2. 网络分区导致 A 与 Redis 失联
3. 锁过期后，客户端 B 获取锁
4. 网络恢复，A 和 B 同时持有锁
```

**解决方案**：

**方案 A：Fencing Token**
每次加锁时分配单调递增的令牌号：
```
1. 客户端 A 获取锁，得到 token=100
2. 客户端 B 获取锁，得到 token=101
3. 资源服务器只接受 token 更大的请求
4. A 的请求（token=100）会被拒绝
```

实现方式：
- Redis：使用 INCR 命令生成全局递增 ID
- Etcd：使用 Key 的 Revision（创建版本号）

**方案 B：牺牲可用性**
使用强一致性系统（Etcd/ZooKeeper），在网络分区时宁可拒绝服务也不违反互斥性。

---

## 四、高可用架构设计

### 1. 部署架构

**Redis 方案**：
```
┌─────────────┐
│ 客户端 SDK   │
└──────┬──────┘
       │
       ├─────────┐
       │         │
   ┌───▼───┐ ┌──▼────┐
   │Redis 1│ │Redis 2│  ... (5个独立节点)
   └───────┘ └───────┘

   Redlock 算法：
   - 向所有节点发起请求
   - 超过半数成功才认为加锁成功
```

**Etcd 方案**：
```
┌─────────────┐
│ 客户端 SDK   │
└──────┬──────┘
       │
       │  (通过 API Gateway 或负载均衡)
       │
  ┌────▼────┐
  │ Etcd 集群│
  ├─────────┤
  │ Node 1  │ (Leader)
  │ Node 2  │ (Follower)
  │ Node 3  │ (Follower)
  └─────────┘

  Raft 共识算法：
  - 写操作必须在 Leader 上执行
  - 超过半数节点确认才提交
```

### 2. 客户端 SDK 设计

**核心功能**：
```go
type DistributedLock interface {
    // 尝试获取锁
    TryLock(ctx context.Context, opts ...Option) error

    // 阻塞获取锁
    Lock(ctx context.Context, opts ...Option) error

    // 释放锁
    Unlock(ctx context.Context) error

    // 自动续期（后台任务）
    autoRenew()
}

type Option func(*LockConfig)

func WithExpireTime(duration time.Duration) Option
func WithRetryInterval(duration time.Duration) Option
func WithWatchdog(enable bool) Option  // 自动续期
```

**使用示例**：
```go
lock := NewRedisLock(client, "order:123")

// 方式1：try-lock 模式
err := lock.TryLock(ctx, WithExpireTime(30*time.Second))
if err != nil {
    return errors.New("获取锁失败")
}
defer lock.Unlock(ctx)

// 方式2：阻塞等待模式
err := lock.Lock(ctx,
    WithExpireTime(30*time.Second),
    WithTimeout(10*time.Second),
    WithWatchdog(true),  // 开启自动续期
)
defer lock.Unlock(ctx)

// 执行业务逻辑
processOrder()
```

### 3. 监控与告警

**关键指标**：
- 锁等待时间：P50/P99/P999
- 锁持有时长：识别长时间占用的锁
- 加锁失败率：超过阈值告警
- 续期失败次数：可能预示网络问题
- 过期自动释放次数：可能表示业务超时

**日志记录**：
```json
{
  "action": "lock_acquired",
  "resource": "order:123",
  "client_id": "server-1-thread-456",
  "token": "uuid-xxx",
  "expire_time": 30,
  "wait_time_ms": 125,
  "timestamp": "2025-01-15T10:30:00Z"
}
```

---

## 五、性能优化

### 1. 减少网络往返

**本地缓存锁状态**：
客户端缓存已持有的锁信息，可重入时无需访问网络。

**批量操作**：
使用 Pipeline 或事务批量处理多个锁操作。

### 2. 降低锁粒度

**分段锁**：
```
// 不好的设计
lock("inventory")  // 锁整个库存表

// 好的设计
lock("inventory:item:12345")  // 只锁特定商品
lock("inventory:warehouse:beijing")  // 按仓库分段
```

**读写锁分离**：
允许多个读操作并发，只有写操作需要独占锁。

### 3. 降级策略

**本地锁降级**：
在流量激增时，对于非关键业务，降级为单机锁（sync.Mutex），牺牲一致性换取性能。

**乐观锁替代**：
对于冲突概率低的场景，使用 CAS（Compare-And-Swap）或版本号机制。

---

## 六、总结

### 技术选型建议

| 场景 | 推荐方案 | 理由 |
|------|---------|------|
| 高性能、短时锁（秒级） | Redis | 性能最优，延迟低 |
| 强一致性要求 | Etcd/ZooKeeper | 共识算法保证安全性 |
| 已有数据库，流量不高 | MySQL | 降低架构复杂度 |
| 多数据中心部署 | Redlock + Fencing Token | 跨区域容灾 |

### 最佳实践

1. **永远设置过期时间**，防止死锁
2. **使用唯一 Token**，防止误解锁
3. **合理估算锁时长**，避免频繁续期
4. **优先使用成熟框架**（Redisson/etcd clientv3），避免重复造轮子
5. **监控锁的持有时长**，及时发现异常
6. **考虑降级方案**，高可用优先于强一致性

### 常见陷阱

- ❌ 忘记设置过期时间，导致死锁
- ❌ 解锁时不验证 Token，误删其他客户端的锁
- ❌ 过度依赖 Redis 主从复制，忽视异步复制的丢锁风险
- ❌ 锁粒度太粗，影响并发度
- ❌ 业务逻辑超时但未续期，导致重复执行
