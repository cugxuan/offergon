---
title: 设计一个新闻推送系统（Feed 流）
tags:
  - 系统设计
status: robot
class: 系统设计
slug: news-feed-system-design
ref:
---

## 核心要点

**推模式 vs 拉模式权衡**:大V用拉,普通用户用推,混合模式兼顾性能和实时性。**读扩散**解决写扩散的存储爆炸问题。**多级缓存+分库分表**应对百万级并发,Timeline合并采用**归并排序**优化性能。

---

## 详细回答

### 一、系统设计目标与挑战

**核心功能需求**:
1. 用户发布动态(文字、图片、视频)
2. 关注/粉丝机制
3. Feed流展示(按时间倒序)
4. 支持点赞、评论、转发
5. 实时性要求(秒级可见)

**技术挑战**:
- **读多写少**:微博场景下,读写比可达100:1
- **热点问题**:大V发微博,百万粉丝同时刷新
- **数据量爆炸**:千万用户×平均关注200人×每天10条动态
- **实时性与一致性**:CAP理论下的权衡

---

### 二、Feed流核心模型选择

#### 1. 推模式(Push/写扩散/Fan-out on Write)

**原理**:
用户发布动态时,立即写入所有粉丝的收件箱(Inbox)。

**优点**:
- 读取快:直接从自己的Inbox读,无需聚合
- 实时性强:发布即分发

**缺点**:
- 写放大严重:大V发一条微博需要写入百万份
- 存储成本高:每个粉丝都存一份副本
- 发布延迟高:千万粉丝需要数秒才能完成写入

**适用场景**: 粉丝数≤5000的普通用户

#### 2. 拉模式(Pull/读扩散/Fan-out on Read)

**原理**:
用户发布动态只写自己的发件箱(Outbox),粉丝读取时实时拉取关注列表的最新动态并合并。

**优点**:
- 写入快:只写一次,无扩散
- 存储省:无冗余数据
- 无需关心粉丝数

**缺点**:
- 读取慢:需要查询N个关注对象+归并排序
- 实时性差:每次读都要计算
- 读放大:关注500人,每次刷新需要500次查询

**适用场景**: 粉丝数>100万的大V

#### 3. 推拉结合(混合模式)

**核心策略**:
```go
// 发布动态时的分发逻辑
func PublishPost(userID int64, content string) {
    // 1. 写入自己的发件箱
    SaveToOutbox(userID, content)

    // 2. 根据粉丝数决定推送策略
    fansCount := GetFansCount(userID)

    if fansCount <= 5000 {
        // 普通用户:推模式(写扩散)
        fans := GetActiveFans(userID, 5000) // 只推给活跃粉丝
        for _, fanID := range fans {
            PushToInbox(fanID, userID, content)
        }
    } else {
        // 大V:拉模式(读扩散)
        // 不做任何推送,等粉丝主动拉取
    }
}

// 读取Feed流的逻辑
func GetFeedList(userID int64, page int) []Post {
    var posts []Post

    // 1. 从Inbox读取推模式的内容(普通用户的动态)
    inboxPosts := GetFromInbox(userID, page)
    posts = append(posts, inboxPosts...)

    // 2. 实时拉取大V的最新动态
    bigVList := GetFollowedBigV(userID)
    for _, bigV := range bigVList {
        latestPosts := GetFromOutbox(bigV, 10)
        posts = append(posts, latestPosts...)
    }

    // 3. 归并排序(按时间倒序)
    sort.Slice(posts, func(i, j int) bool {
        return posts[i].CreateTime > posts[j].CreateTime
    })

    return posts[:20] // 返回前20条
}
```

**优化策略**:
- **活跃粉丝过滤**:只推送给30天内活跃的粉丝,减少70%写入
- **延迟推送**:非实时粉丝延迟推送(如定时Job批量推)
- **缓存预热**:大V的最新动态缓存在Redis,TTL=5分钟

---

### 三、数据库设计

#### 1. 核心表结构

```sql
-- 发件箱(Outbox):用户发布的所有动态
CREATE TABLE post_outbox (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    content TEXT,
    media_urls JSON,          -- 图片/视频URL数组
    create_time BIGINT NOT NULL,
    INDEX idx_user_time (user_id, create_time DESC)
) PARTITION BY HASH(user_id) PARTITIONS 256;

-- 收件箱(Inbox):用户的Feed流缓存
CREATE TABLE feed_inbox (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT NOT NULL,         -- 接收者ID
    author_id BIGINT NOT NULL,       -- 发布者ID
    post_id BIGINT NOT NULL,
    create_time BIGINT NOT NULL,
    INDEX idx_user_time (user_id, create_time DESC)
) PARTITION BY HASH(user_id) PARTITIONS 256;

-- 关注关系表
CREATE TABLE user_follow (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT NOT NULL,         -- 关注者
    follow_id BIGINT NOT NULL,       -- 被关注者
    create_time BIGINT NOT NULL,
    UNIQUE KEY uk_user_follow (user_id, follow_id),
    INDEX idx_follow (follow_id)     -- 查粉丝列表
) PARTITION BY HASH(user_id) PARTITIONS 256;
```

#### 2. 分库分表策略

**分片维度**:
- **Outbox**: 按`user_id`分片(发布者维度)
- **Inbox**: 按`user_id`分片(接收者维度)
- **Follow**: 按`user_id`分片(关注者维度)

**分表数量**: 256个物理表,支持单表5000万行数据

**路由算法**:
```go
func GetShardID(userID int64) int {
    return int(userID % 256) // 取模分片
}
```

---

### 四、缓存架构设计

#### 1. 多级缓存策略

```
用户请求 → 本地缓存(Caffeine) → Redis缓存 → MySQL
   ↓           (10秒TTL)      (5分钟TTL)
100ms        1ms               10ms        100ms
```

**L1缓存(本地缓存)**:
```go
// 使用Caffeine缓存最近访问的Feed
var feedCache = caffeine.NewCache(
    caffeine.WithMaximumSize(10000),
    caffeine.WithExpireAfterWrite(10 * time.Second),
)

func GetFeedWithCache(userID int64) []Post {
    // 先查本地缓存
    if val, ok := feedCache.Get(userID); ok {
        return val.([]Post)
    }

    // 查Redis
    posts := getFromRedis(userID)
    feedCache.Set(userID, posts)
    return posts
}
```

**L2缓存(Redis)**:
```bash
# Inbox缓存:用户的Feed流(推模式内容)
# Key: feed:inbox:{user_id}
# Type: Sorted Set (按时间戳排序)
ZADD feed:inbox:123 1633024800 "post:456"
ZADD feed:inbox:123 1633024900 "post:789"
ZREVRANGE feed:inbox:123 0 19  # 取最新20条

# Outbox缓存:大V的最新动态
# Key: feed:outbox:{user_id}
# Type: Sorted Set
ZADD feed:outbox:999 1633024800 "post:111"
ZREVRANGE feed:outbox:999 0 9  # 取最新10条

# 动态详情缓存
# Key: post:{post_id}
# Type: Hash
HSET post:456 user_id 123 content "Hello" create_time 1633024800
```

**缓存更新策略**:
- **发布动态**:先写DB,再写Redis,异步推送Inbox
- **读取Feed**:缓存未命中时,查DB后回写缓存(Cache Aside)
- **缓存失效**:删除/编辑动态时,删除相关缓存

#### 2. 缓存一致性问题

**问题场景**:
1. 删除动态后,粉丝的Inbox缓存仍存在
2. 用户取关后,Feed中仍显示对方动态

**解决方案**:
```go
// 删除动态时的缓存清理
func DeletePost(postID int64) error {
    // 1. 查询哪些人的Inbox包含这条动态
    inboxRecords := db.Query(
        "SELECT user_id FROM feed_inbox WHERE post_id = ?", postID)

    // 2. 删除DB记录
    db.Exec("DELETE FROM post_outbox WHERE id = ?", postID)
    db.Exec("DELETE FROM feed_inbox WHERE post_id = ?", postID)

    // 3. 异步删除Redis缓存(消息队列)
    for _, record := range inboxRecords {
        mq.Send("cache_delete", map[string]interface{}{
            "key": fmt.Sprintf("feed:inbox:%d", record.UserID),
            "member": fmt.Sprintf("post:%d", postID),
        })
    }

    return nil
}
```

**最终一致性策略**:
- 允许短时间不一致(5分钟内)
- 定时任务兜底:每小时清理过期缓存
- 用户刷新时校验:发现异常数据立即删除

---

### 五、高并发优化方案

#### 1. 热点问题处理

**场景**:大V发微博,1000万粉丝同时刷新

**解决方案**:

**(1) 本地缓存+CDN**:
```go
// 大V的动态直接缓存在CDN边缘节点
func GetBigVLatestPosts(userID int64) []Post {
    // 从CDN读取(带版本号)
    cacheKey := fmt.Sprintf("bigv_posts_%d_v%d", userID, getVersion(userID))
    if posts := cdn.Get(cacheKey); posts != nil {
        return posts
    }

    // 从数据库读取并回写CDN
    posts := db.GetOutbox(userID, 10)
    cdn.Set(cacheKey, posts, 60) // 缓存60秒
    return posts
}
```

**(2) 请求合并(Request Coalescing)**:
```go
// 使用singleflight防止缓存击穿
var sg singleflight.Group

func GetFeed(userID int64) ([]Post, error) {
    key := fmt.Sprintf("feed_%d", userID)

    // 多个并发请求只执行一次DB查询
    val, err, _ := sg.Do(key, func() (interface{}, error) {
        return queryFeedFromDB(userID)
    })

    return val.([]Post), err
}
```

**(3) 异步化处理**:
```go
// 大V发布动态时,不做任何推送
// 粉丝刷新时,异步拉取最新动态
func GetFeed(userID int64) []Post {
    // 立即返回缓存的旧数据
    cachedPosts := getFromCache(userID)

    // 异步拉取最新数据(非阻塞)
    go func() {
        latestPosts := pullLatestPosts(userID)
        updateCache(userID, latestPosts)
    }()

    return cachedPosts
}
```

#### 2. 数据库读写分离

```
写操作 → 主库(Master) → 异步复制 → 从库1(Slave1)
                                   → 从库2(Slave2)
                                   → 从库3(Slave3)

读操作 → 负载均衡(Round Robin) → 从库1/2/3
```

**读写分离策略**:
```go
// 写操作强制走主库
func PublishPost(userID int64, content string) {
    masterDB.Exec(
        "INSERT INTO post_outbox (user_id, content, create_time) VALUES (?, ?, ?)",
        userID, content, time.Now().Unix())
}

// 读操作走从库(允许短暂延迟)
func GetFeed(userID int64) []Post {
    // 从库有1-3秒延迟,但能承载99%的读请求
    slaveDB := getSlaveDB() // 随机选择一个从库
    return slaveDB.Query(
        "SELECT * FROM feed_inbox WHERE user_id = ? ORDER BY create_time DESC LIMIT 20",
        userID)
}
```

**延迟容忍度**:
- Feed流:允许3秒延迟(最终一致性)
- 评论数/点赞数:允许5秒延迟
- 用户自己发的动态:强制走主库(读己之写一致性)

---

### 六、Timeline合并算法优化

**问题**:用户关注500人,如何高效合并生成Feed流?

**多路归并排序**:
```go
type PostHeap []Post

func (h PostHeap) Len() int           { return len(h) }
func (h PostHeap) Less(i, j int) bool { return h[i].CreateTime > h[j].CreateTime }
func (h PostHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }
func (h *PostHeap) Push(x interface{}) { *h = append(*h, x.(Post)) }
func (h *PostHeap) Pop() interface{} {
    old := *h
    n := len(old)
    x := old[n-1]
    *h = old[0 : n-1]
    return x
}

// 归并500个用户的Timeline(优先队列优化)
func MergeTimelines(userIDs []int64, limit int) []Post {
    h := &PostHeap{}
    heap.Init(h)

    // 初始化:每个用户的最新一条动态入堆
    iterators := make([]*Iterator, len(userIDs))
    for i, uid := range userIDs {
        iter := NewOutboxIterator(uid) // 每个用户一个迭代器
        if iter.HasNext() {
            heap.Push(h, iter.Next())
            iterators[i] = iter
        }
    }

    // 每次取堆顶(时间最新的),然后从对应迭代器补充下一条
    result := make([]Post, 0, limit)
    for h.Len() > 0 && len(result) < limit {
        // 弹出最新的动态
        post := heap.Pop(h).(Post)
        result = append(result, post)

        // 从同一个用户补充下一条动态
        iter := iterators[post.UserID]
        if iter.HasNext() {
            heap.Push(h, iter.Next())
        }
    }

    return result
}
```

**时间复杂度**:
- 朴素方法:查询500次DB + 排序50000条数据 = O(500*log(500*100))
- 优先队列优化:只需要维护500个元素的堆 = O(20*log(500)) ≈ 180次比较

**实际优化**:
```go
// 只查询每个用户的最新10条,而不是全部
func MergeTimelinesOptimized(userIDs []int64) []Post {
    // 1. 并行查询(goroutine池)
    postsChan := make(chan []Post, len(userIDs))
    var wg sync.WaitGroup

    for _, uid := range userIDs {
        wg.Add(1)
        go func(id int64) {
            defer wg.Done()
            posts := redis.ZRevRange(fmt.Sprintf("feed:outbox:%d", id), 0, 9)
            postsChan <- posts
        }(uid)
    }

    go func() {
        wg.Wait()
        close(postsChan)
    }()

    // 2. 归并排序
    allPosts := []Post{}
    for posts := range postsChan {
        allPosts = append(allPosts, posts...)
    }

    sort.Slice(allPosts, func(i, j int) bool {
        return allPosts[i].CreateTime > allPosts[j].CreateTime
    })

    return allPosts[:20]
}
```

---

### 七、系统架构图

```
┌──────────┐      ┌──────────┐      ┌──────────┐
│  客户端   │─────>│  CDN/网关 │─────>│ Feed服务  │
└──────────┘      └──────────┘      └──────────┘
                                          │
                  ┌───────────────────────┼───────────────────────┐
                  │                       │                       │
                  ▼                       ▼                       ▼
            ┌──────────┐          ┌──────────┐          ┌──────────┐
            │本地缓存  │          │  Redis   │          │  MySQL   │
            │Caffeine  │          │集群(主从) │          │分库分表  │
            └──────────┘          └──────────┘          └──────────┘
                                          │
                                          ▼
                                  ┌──────────────┐
                                  │  消息队列    │
                                  │ (Kafka/RMQ)  │
                                  └──────────────┘
                                          │
                  ┌───────────────────────┼───────────────────────┐
                  ▼                       ▼                       ▼
            ┌──────────┐          ┌──────────┐          ┌──────────┐
            │推送Worker│          │统计Worker│          │清理Worker│
            │(Fan-out) │          │(计数异步) │          │(缓存过期)│
            └──────────┘          └──────────┘          └──────────┘
```

**关键组件**:
1. **Feed服务**:核心业务逻辑(推拉结合、Timeline合并)
2. **推送Worker**:异步处理写扩散任务(Kafka消费)
3. **统计Worker**:异步更新点赞数、评论数(削峰填谷)
4. **清理Worker**:定时清理过期缓存和软删除数据

---

### 八、容量评估与成本

**假设**:
- 用户数:1亿
- 日活:2000万(20%)
- 平均关注数:200人
- 日人均刷新:50次
- 日人均发帖:0.5条

**QPS计算**:
- 读QPS: 2000万 × 50 / 86400 ≈ **11,574 QPS**
- 写QPS: 2000万 × 0.5 / 86400 ≈ **116 QPS**

**存储容量**:
- Outbox: 1亿用户 × 平均500条历史动态 × 1KB ≈ **50TB**
- Inbox(推模式): 2000万活跃用户 × 200关注 × 100条缓存 × 1KB ≈ **400TB**
- Redis: 2000万活跃用户 × 20条Feed × 1KB ≈ **400GB**

**成本优化**:
- Inbox只保留最近7天数据(超过7天查Outbox)
- 非活跃用户不占用Inbox(按需生成)
- 历史动态归档到对象存储(S3/OSS),成本降低90%

---

### 九、总结

| **模式** | **写性能** | **读性能** | **存储成本** | **适用场景** |
|---------|----------|----------|------------|------------|
| 推模式   | 差(写放大) | 优(直接读) | 高(冗余存储) | 普通用户    |
| 拉模式   | 优(单次写) | 差(归并慢) | 低(无冗余)  | 大V用户     |
| 混合模式 | 中        | 优        | 中         | **推荐方案** |

**设计原则**:
1. **读多写少优化**:用缓存扛住99%的读请求
2. **CAP权衡**:放弃强一致性,选择最终一致性
3. **分而治之**:分库分表+缓存分片,水平扩展
4. **异步解耦**:消息队列削峰填谷,提升吞吐量
5. **降级策略**:高峰期降级非核心功能(如推荐、广告)
