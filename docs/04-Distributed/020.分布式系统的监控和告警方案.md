---
title: 分布式系统的监控和告警方案
tags:
  - 分布式系统
  - 分布式
status: robot
class: 分布式系统
slug: distributed-system-monitoring-alerting
ref:
---

## 核心要点

- **监控三支柱**: Metrics(指标)、Logging(日志)、Tracing(链路追踪),构建完整可观测性体系
- **监控层次**: 基础设施层(CPU/内存/网络) → 应用层(QPS/延迟/错误率) → 业务层(订单量/GMV/转化率)
- **告警策略**: 分级告警(P0-P3)、智能降噪(相关性分析/静默期)、闭环管理(告警→处理→复盘)
- **工具选型**: Prometheus+Grafana(指标)、ELK/Loki(日志)、Jaeger/Zipkin(链路)、AlertManager(告警)

---

## 一、监控体系设计

### 1.1 可观测性三支柱

**架构全景:**

```
                    +-------------------+
                    |   数据采集层       |
                    +-------------------+
                    /        |          \
                   /         |           \
          +--------+    +--------+    +---------+
          | Metrics|    | Logging|    | Tracing |
          +--------+    +--------+    +---------+
              |             |              |
              v             v              v
       Prometheus        ELK/Loki       Jaeger
              |             |              |
              +-------------+--------------+
                           |
                    +-------------+
                    |   Grafana   |  (统一可视化)
                    +-------------+
                           |
                    +-------------+
                    | AlertManager|  (告警分发)
                    +-------------+
```

**三支柱对比:**

| 维度 | Metrics(指标) | Logging(日志) | Tracing(链路) |
|------|---------------|---------------|---------------|
| **数据类型** | 时序数据(数值) | 文本数据(事件) | 调用链数据 |
| **存储成本** | 低 | 高 | 中 |
| **查询性能** | 快(聚合查询) | 慢(全文检索) | 中(trace_id索引) |
| **适用场景** | 趋势分析、告警 | 问题排查、审计 | 性能优化、依赖分析 |
| **典型工具** | Prometheus | ELK/Loki | Jaeger/Zipkin |

### 1.2 监控指标设计

**Google SRE四大黄金信号:**

```go
// 1. Latency(延迟)
var requestDuration = prometheus.NewHistogramVec(
    prometheus.HistogramOpts{
        Name:    "http_request_duration_seconds",
        Help:    "HTTP request latency",
        Buckets: []float64{0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10},
    },
    []string{"method", "endpoint", "status"},
)

// 2. Traffic(流量)
var requestTotal = prometheus.NewCounterVec(
    prometheus.CounterOpts{
        Name: "http_requests_total",
        Help: "Total HTTP requests",
    },
    []string{"method", "endpoint", "status"},
)

// 3. Errors(错误)
var errorTotal = prometheus.NewCounterVec(
    prometheus.CounterOpts{
        Name: "http_errors_total",
        Help: "Total HTTP errors",
    },
    []string{"method", "endpoint", "error_type"},
)

// 4. Saturation(饱和度)
var (
    cpuUsage = prometheus.NewGauge(prometheus.GaugeOpts{
        Name: "process_cpu_usage_percent",
    })

    memoryUsage = prometheus.NewGauge(prometheus.GaugeOpts{
        Name: "process_memory_usage_bytes",
    })

    goroutineCount = prometheus.NewGauge(prometheus.GaugeOpts{
        Name: "go_goroutines",
    })
)
```

**RED方法(面向请求的监控):**

```go
// Rate(请求速率)
rate(http_requests_total[5m])

// Errors(错误率)
rate(http_errors_total[5m]) / rate(http_requests_total[5m])

// Duration(请求耗时)
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

**USE方法(面向资源的监控):**

```yaml
# Utilization(使用率)
cpu_utilization: (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100
memory_utilization: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100

# Saturation(饱和度)
load_average: node_load1 / count(node_cpu_seconds_total{mode="idle"})
disk_io_saturation: rate(node_disk_io_time_seconds_total[5m])

# Errors(错误)
network_errors: rate(node_network_receive_errs_total[5m])
disk_errors: rate(node_disk_io_errors_total[5m])
```

### 1.3 分层监控架构

**监控金字塔:**

```
        业务层监控 (Business)
        ↑
        应用层监控 (Application)
        ↑
        中间件监控 (Middleware)
        ↑
        基础设施监控 (Infrastructure)
```

**Go应用监控埋点:**

```go
package monitoring

import (
    "context"
    "time"
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promhttp"
)

// 监控中间件
func PrometheusMiddleware(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()

        // 包装ResponseWriter以捕获状态码
        ww := &responseWriter{ResponseWriter: w, statusCode: 200}

        next.ServeHTTP(ww, r)

        duration := time.Since(start).Seconds()
        endpoint := r.URL.Path
        method := r.Method
        status := ww.statusCode

        // 记录指标
        requestTotal.WithLabelValues(method, endpoint, fmt.Sprint(status)).Inc()
        requestDuration.WithLabelValues(method, endpoint, fmt.Sprint(status)).Observe(duration)

        if status >= 400 {
            errorTotal.WithLabelValues(method, endpoint, "http_error").Inc()
        }
    })
}

// 业务指标埋点
type OrderMetrics struct {
    orderCreated    *prometheus.CounterVec
    orderAmount     *prometheus.HistogramVec
    orderProcessTime *prometheus.HistogramVec
}

func NewOrderMetrics() *OrderMetrics {
    return &OrderMetrics{
        orderCreated: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "orders_created_total",
                Help: "Total orders created",
            },
            []string{"product_type", "payment_method"},
        ),

        orderAmount: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "order_amount_yuan",
                Buckets: []float64{10, 50, 100, 500, 1000, 5000, 10000},
            },
            []string{"product_type"},
        ),

        orderProcessTime: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "order_process_duration_seconds",
                Buckets: prometheus.DefBuckets,
            },
            []string{"stage"},  // create, pay, fulfill
        ),
    }
}

func (m *OrderMetrics) RecordOrder(order *Order) {
    m.orderCreated.WithLabelValues(order.ProductType, order.PaymentMethod).Inc()
    m.orderAmount.WithLabelValues(order.ProductType).Observe(order.Amount)
}

func (m *OrderMetrics) RecordProcessStage(stage string, duration time.Duration) {
    m.orderProcessTime.WithLabelValues(stage).Observe(duration.Seconds())
}
```

---

## 二、日志监控方案

### 2.1 日志架构设计

**ELK Stack架构:**

```
应用服务(Filebeat) → Kafka(缓冲) → Logstash(处理) → Elasticsearch(存储) → Kibana(可视化)
```

**轻量级方案(Loki):**

```
应用服务(Promtail) → Loki(存储) → Grafana(可视化)
```

**优势对比:**

| 方案 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **ELK** | 功能强大、生态丰富 | 资源消耗大、运维复杂 | 大规模日志分析 |
| **Loki** | 轻量级、成本低 | 功能相对简单 | 中小规模、与Prometheus集成 |

### 2.2 结构化日志实践

**日志规范:**

```go
package logger

import (
    "context"
    "github.com/sirupsen/logrus"
    "go.opentelemetry.io/otel/trace"
)

// 统一日志格式
type LogEntry struct {
    Timestamp   string                 `json:"timestamp"`
    Level       string                 `json:"level"`
    Message     string                 `json:"message"`
    Service     string                 `json:"service"`
    TraceID     string                 `json:"trace_id"`
    SpanID      string                 `json:"span_id"`
    UserID      string                 `json:"user_id,omitempty"`
    RequestID   string                 `json:"request_id"`
    Fields      map[string]interface{} `json:"fields,omitempty"`
    Stacktrace  string                 `json:"stacktrace,omitempty"`
}

// 上下文日志
func WithContext(ctx context.Context) *logrus.Entry {
    entry := logrus.WithFields(logrus.Fields{
        "service": "order-service",
    })

    // 注入TraceID
    if span := trace.SpanFromContext(ctx); span.SpanContext().IsValid() {
        entry = entry.WithFields(logrus.Fields{
            "trace_id": span.SpanContext().TraceID().String(),
            "span_id":  span.SpanContext().SpanID().String(),
        })
    }

    // 注入UserID
    if userID := ctx.Value("user_id"); userID != nil {
        entry = entry.WithField("user_id", userID)
    }

    // 注入RequestID
    if reqID := ctx.Value("request_id"); reqID != nil {
        entry = entry.WithField("request_id", reqID)
    }

    return entry
}

// 使用示例
func CreateOrder(ctx context.Context, req *CreateOrderRequest) (*Order, error) {
    logger := WithContext(ctx)

    logger.WithFields(logrus.Fields{
        "product_id": req.ProductID,
        "amount":     req.Amount,
    }).Info("开始创建订单")

    order, err := orderService.Create(req)
    if err != nil {
        logger.WithError(err).Error("订单创建失败")
        return nil, err
    }

    logger.WithFields(logrus.Fields{
        "order_id": order.ID,
        "duration": time.Since(start).Milliseconds(),
    }).Info("订单创建成功")

    return order, nil
}
```

### 2.3 日志采集与处理

**Filebeat配置(ELK):**

```yaml
# filebeat.yml
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/app/*.log

    # JSON日志解析
    json.keys_under_root: true
    json.add_error_key: true

    # 字段处理
    fields:
      service: order-service
      env: production
    fields_under_root: true

    # 多行日志处理(堆栈跟踪)
    multiline.pattern: '^\s'
    multiline.negate: false
    multiline.match: after

# 输出到Kafka
output.kafka:
  hosts: ["kafka1:9092", "kafka2:9092"]
  topic: 'app-logs-%{+yyyy.MM.dd}'
  partition.round_robin:
    reachable_only: false
  compression: gzip
  max_message_bytes: 1000000

# 输出到Elasticsearch
output.elasticsearch:
  hosts: ["es1:9200", "es2:9200"]
  index: "app-logs-%{+yyyy.MM.dd}"

  # 索引生命周期管理
  ilm.enabled: true
  ilm.rollover_alias: "app-logs"
  ilm.pattern: "{now/d}-000001"
```

**Promtail配置(Loki):**

```yaml
# promtail-config.yml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: app-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: order-service
          env: production
          __path__: /var/log/app/*.log

    # JSON日志解析
    pipeline_stages:
      - json:
          expressions:
            level: level
            trace_id: trace_id
            user_id: user_id
            msg: message

      - labels:
          level:
          trace_id:

      - output:
          source: msg
```

### 2.4 日志查询与分析

**Kibana KQL查询:**

```
# 查询特定用户的错误日志
level:ERROR AND user_id:"12345" AND @timestamp:[now-1h TO now]

# 查询慢请求(耗时>1s)
duration:>1000 AND service:"order-service"

# 查询某个TraceID的完整调用链
trace_id:"4bf92f3577b34da6a3ce929d0e0e4736"
```

**Loki LogQL查询:**

```promql
# 查询错误日志
{service="order-service"} |= "ERROR" | json | level="ERROR"

# 统计QPS
rate({service="order-service"}[5m])

# 按TraceID聚合
{service="order-service"} | json | trace_id="xxx"

# 提取延迟指标
{service="order-service"} | json | unwrap duration | quantile_over_time(0.99, [5m])
```

---

## 三、链路追踪方案

### 3.1 分布式追踪原理

**OpenTelemetry架构:**

```
应用(SDK埋点) → Collector(采集) → Jaeger/Zipkin(存储) → UI(可视化)
```

**Trace数据结构:**

```
Trace (TraceID: abc123)
  ├─ Span (API Gateway) [100ms]
  │   ├─ Span (Order Service) [80ms]
  │   │   ├─ Span (MySQL Query) [30ms]
  │   │   └─ Span (Redis Get) [5ms]
  │   └─ Span (Payment Service) [50ms]
  │       └─ Span (Third-party API) [40ms]
```

### 3.2 Go应用集成OpenTelemetry

**初始化配置:**

```go
package tracing

import (
    "context"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/exporters/jaeger"
    "go.opentelemetry.io/otel/propagation"
    "go.opentelemetry.io/otel/sdk/resource"
    "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.4.0"
)

func InitTracer(serviceName string) (*trace.TracerProvider, error) {
    // 创建Jaeger Exporter
    exporter, err := jaeger.New(
        jaeger.WithCollectorEndpoint(jaeger.WithEndpoint("http://jaeger:14268/api/traces")),
    )
    if err != nil {
        return nil, err
    }

    // 创建TracerProvider
    tp := trace.NewTracerProvider(
        trace.WithBatcher(exporter),
        trace.WithResource(resource.NewWithAttributes(
            semconv.SchemaURL,
            semconv.ServiceNameKey.String(serviceName),
            semconv.DeploymentEnvironmentKey.String("production"),
        )),
        // 采样策略: 100%采样(生产环境建议10-20%)
        trace.WithSampler(trace.TraceIDRatioBased(1.0)),
    )

    otel.SetTracerProvider(tp)

    // 设置全局Propagator(用于跨服务传递TraceID)
    otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(
        propagation.TraceContext{},
        propagation.Baggage{},
    ))

    return tp, nil
}
```

**HTTP中间件埋点:**

```go
func TracingMiddleware(next http.Handler) http.Handler {
    tracer := otel.Tracer("http-server")

    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        // 从HTTP Header提取TraceID(跨服务传播)
        ctx := otel.GetTextMapPropagator().Extract(r.Context(), propagation.HeaderCarrier(r.Header))

        // 创建Span
        ctx, span := tracer.Start(ctx, fmt.Sprintf("%s %s", r.Method, r.URL.Path),
            trace.WithSpanKind(trace.SpanKindServer),
            trace.WithAttributes(
                semconv.HTTPMethodKey.String(r.Method),
                semconv.HTTPURLKey.String(r.URL.String()),
                semconv.HTTPUserAgentKey.String(r.UserAgent()),
            ),
        )
        defer span.End()

        // 包装ResponseWriter
        ww := &statusRecorder{ResponseWriter: w, statusCode: 200}

        next.ServeHTTP(ww, r.WithContext(ctx))

        // 记录响应信息
        span.SetAttributes(semconv.HTTPStatusCodeKey.Int(ww.statusCode))
        if ww.statusCode >= 400 {
            span.RecordError(fmt.Errorf("HTTP %d", ww.statusCode))
        }
    })
}
```

**数据库查询追踪:**

```go
func TracedDBQuery(ctx context.Context, db *gorm.DB, query string) error {
    tracer := otel.Tracer("database")
    ctx, span := tracer.Start(ctx, "db.query",
        trace.WithSpanKind(trace.SpanKindClient),
        trace.WithAttributes(
            semconv.DBSystemMySQL,
            semconv.DBStatementKey.String(query),
        ),
    )
    defer span.End()

    start := time.Now()
    err := db.WithContext(ctx).Exec(query).Error

    span.SetAttributes(
        semconv.DBOperationKey.String("query"),
        attribute.Int64("db.duration_ms", time.Since(start).Milliseconds()),
    )

    if err != nil {
        span.RecordError(err)
    }

    return err
}
```

**跨服务调用传播:**

```go
func CallDownstreamService(ctx context.Context, url string) (*http.Response, error) {
    tracer := otel.Tracer("http-client")
    ctx, span := tracer.Start(ctx, "http.request",
        trace.WithSpanKind(trace.SpanKindClient),
        trace.WithAttributes(
            semconv.HTTPMethodKey.String("POST"),
            semconv.HTTPURLKey.String(url),
        ),
    )
    defer span.End()

    req, _ := http.NewRequestWithContext(ctx, "POST", url, nil)

    // 注入TraceID到HTTP Header
    otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(req.Header))

    resp, err := http.DefaultClient.Do(req)
    if err != nil {
        span.RecordError(err)
        return nil, err
    }

    span.SetAttributes(semconv.HTTPStatusCodeKey.Int(resp.StatusCode))
    return resp, nil
}
```

### 3.3 链路分析实践

**性能瓶颈分析:**

```go
// 在Jaeger UI中查询慢Trace
// 1. 搜索条件: duration > 1s
// 2. 分析关键路径: 找出耗时最长的Span
// 3. 优化方向:
//    - 数据库慢查询 → 添加索引/优化SQL
//    - 外部API慢 → 增加超时/异步化
//    - 串行调用 → 并行化

// 示例: 并行优化
func GetOrderDetail(ctx context.Context, orderID int64) (*OrderDetail, error) {
    tracer := otel.Tracer("order-service")
    ctx, span := tracer.Start(ctx, "GetOrderDetail")
    defer span.End()

    var (
        order    *Order
        products []*Product
        user     *User
        wg       sync.WaitGroup
        mu       sync.Mutex
        errs     []error
    )

    // 并行查询(原本串行耗时300ms,并行后100ms)
    wg.Add(3)

    go func() {
        defer wg.Done()
        ctx, span := tracer.Start(ctx, "query.order")
        defer span.End()

        o, err := orderDAO.Get(ctx, orderID)
        if err != nil {
            mu.Lock()
            errs = append(errs, err)
            mu.Unlock()
            return
        }
        order = o
    }()

    go func() {
        defer wg.Done()
        ctx, span := tracer.Start(ctx, "query.products")
        defer span.End()

        p, err := productDAO.GetByOrderID(ctx, orderID)
        if err != nil {
            mu.Lock()
            errs = append(errs, err)
            mu.Unlock()
            return
        }
        products = p
    }()

    go func() {
        defer wg.Done()
        ctx, span := tracer.Start(ctx, "query.user")
        defer span.End()

        u, err := userDAO.Get(ctx, order.UserID)
        if err != nil {
            mu.Lock()
            errs = append(errs, err)
            mu.Unlock()
            return
        }
        user = u
    }()

    wg.Wait()

    if len(errs) > 0 {
        return nil, errs[0]
    }

    return &OrderDetail{
        Order:    order,
        Products: products,
        User:     user,
    }, nil
}
```

---

## 四、告警方案设计

### 4.1 告警规则设计

**Prometheus告警规则:**

```yaml
# alerts.yml
groups:
  - name: service_alerts
    interval: 30s
    rules:
      # P0: 服务完全不可用
      - alert: ServiceDown
        expr: up{job="order-service"} == 0
        for: 1m
        labels:
          severity: critical
          priority: P0
        annotations:
          summary: "服务{{ $labels.instance }}宕机"
          description: "订单服务已宕机超过1分钟"

      # P1: 错误率过高
      - alert: HighErrorRate
        expr: |
          (
            rate(http_errors_total[5m])
            /
            rate(http_requests_total[5m])
          ) > 0.05
        for: 5m
        labels:
          severity: critical
          priority: P1
        annotations:
          summary: "错误率过高: {{ $value | humanizePercentage }}"
          description: "订单服务错误率超过5%,当前值: {{ $value }}"

      # P2: 延迟过高
      - alert: HighLatency
        expr: |
          histogram_quantile(0.99,
            rate(http_request_duration_seconds_bucket[5m])
          ) > 1
        for: 10m
        labels:
          severity: warning
          priority: P2
        annotations:
          summary: "P99延迟过高: {{ $value }}s"

      # P3: 资源使用异常
      - alert: HighMemoryUsage
        expr: |
          (
            process_resident_memory_bytes
            /
            node_memory_MemTotal_bytes
          ) > 0.8
        for: 15m
        labels:
          severity: warning
          priority: P3
        annotations:
          summary: "内存使用率过高: {{ $value | humanizePercentage }}"

      # 业务告警: 订单下跌
      - alert: OrderDropSignificant
        expr: |
          (
            rate(orders_created_total[10m])
            /
            rate(orders_created_total[10m] offset 1d)
          ) < 0.5
        for: 15m
        labels:
          severity: warning
          priority: P2
          team: business
        annotations:
          summary: "订单量下跌50%"
          description: "当前10分钟订单量对比昨天同时段下跌超过50%"
```

### 4.2 AlertManager告警路由

**告警分发策略:**

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  smtp_from: 'alert@example.com'
  smtp_smarthost: 'smtp.example.com:587'
  smtp_auth_username: 'alert@example.com'
  smtp_auth_password: 'password'

# 路由树
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s        # 首次告警等待30s(聚合同类告警)
  group_interval: 5m     # 同组告警间隔5分钟
  repeat_interval: 4h    # 重复告警间隔4小时

  routes:
    # P0告警: 立即通知所有人
    - match:
        priority: P0
      receiver: 'oncall-all'
      group_wait: 10s
      repeat_interval: 1h

    # P1告警: 通知技术负责人
    - match:
        priority: P1
      receiver: 'tech-lead'
      group_wait: 30s
      repeat_interval: 2h

    # P2告警: 通知开发团队
    - match:
        priority: P2
      receiver: 'dev-team'
      repeat_interval: 4h

    # 业务告警: 通知业务团队
    - match:
        team: business
      receiver: 'business-team'

    # 工作时间外静默P3告警
    - match:
        priority: P3
      receiver: 'dev-team'
      mute_time_intervals:
        - offhours

# 静默时间段
mute_time_intervals:
  - name: offhours
    time_intervals:
      - weekdays: ['saturday', 'sunday']
      - times:
          - start_time: '00:00'
            end_time: '09:00'
          - start_time: '18:00'
            end_time: '23:59'

# 接收器配置
receivers:
  - name: 'default'
    email_configs:
      - to: 'dev-team@example.com'

  - name: 'oncall-all'
    pagerduty_configs:
      - service_key: '<pagerduty-key>'

    webhook_configs:
      - url: 'http://alert-gateway/v1/phone-call'  # 电话告警
        send_resolved: false

    email_configs:
      - to: 'cto@example.com,tech-lead@example.com'
        headers:
          Subject: '[P0紧急] {{ .GroupLabels.alertname }}'

  - name: 'tech-lead'
    slack_configs:
      - api_url: '<slack-webhook>'
        channel: '#alerts-critical'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

    email_configs:
      - to: 'tech-lead@example.com'

  - name: 'dev-team'
    slack_configs:
      - api_url: '<slack-webhook>'
        channel: '#alerts'

  - name: 'business-team'
    webhook_configs:
      - url: 'http://lark-bot/webhook'  # 飞书机器人

# 告警抑制(避免告警风暴)
inhibit_rules:
  # 服务宕机时,抑制该服务的其他告警
  - source_match:
      alertname: ServiceDown
    target_match_re:
      alertname: (HighErrorRate|HighLatency)
    equal: ['service', 'instance']

  # P0告警时,抑制低优先级告警
  - source_match:
      priority: P0
    target_match_re:
      priority: (P1|P2|P3)
    equal: ['service']
```

### 4.3 智能告警与降噪

**告警聚合策略:**

```go
package alerting

import (
    "time"
)

// 告警聚合器
type AlertAggregator struct {
    alerts map[string]*Alert
    rules  []AggregationRule
}

type AggregationRule struct {
    // 聚合维度
    GroupBy []string  // ["service", "region"]

    // 聚合窗口
    Window time.Duration  // 5分钟

    // 触发条件
    Threshold int  // 同组告警>=5条则合并
}

func (a *AlertAggregator) Process(alert *Alert) {
    key := a.getGroupKey(alert)

    if existing, ok := a.alerts[key]; ok {
        // 合并同组告警
        existing.Count++
        existing.LastOccurred = time.Now()

        // 达到阈值则发送聚合告警
        if existing.Count >= a.getThreshold(alert) {
            a.sendAggregatedAlert(existing)
            delete(a.alerts, key)
        }
    } else {
        a.alerts[key] = alert
    }
}

func (a *AlertAggregator) getGroupKey(alert *Alert) string {
    // 基于规则生成聚合key
    var parts []string
    for _, field := range a.rules[0].GroupBy {
        parts = append(parts, alert.Labels[field])
    }
    return strings.Join(parts, ":")
}

func (a *AlertAggregator) sendAggregatedAlert(alert *Alert) {
    summary := fmt.Sprintf(
        "[聚合告警] %s服务异常,共%d个实例受影响",
        alert.Labels["service"],
        alert.Count,
    )

    sendAlert(&Alert{
        Name:    "AggregatedAlert",
        Summary: summary,
        Details: alert,
    })
}
```

**告警相关性分析:**

```go
// 基于时间窗口的关联分析
type CorrelationAnalyzer struct {
    window time.Duration
    events []Event
}

type Event struct {
    Timestamp time.Time
    Type      string  // "alert", "deploy", "config_change"
    Service   string
    Details   map[string]interface{}
}

func (c *CorrelationAnalyzer) AnalyzeRootCause(alert *Alert) *RootCause {
    // 查找告警前10分钟内的相关事件
    start := alert.Timestamp.Add(-10 * time.Minute)

    var relatedEvents []Event
    for _, event := range c.events {
        if event.Timestamp.After(start) && event.Timestamp.Before(alert.Timestamp) {
            if c.isRelated(event, alert) {
                relatedEvents = append(relatedEvents, event)
            }
        }
    }

    // 分析根因
    if len(relatedEvents) > 0 {
        return &RootCause{
            Alert:         alert,
            PossibleCause: c.inferCause(relatedEvents),
            Evidence:      relatedEvents,
            Confidence:    0.8,
        }
    }

    return nil
}

func (c *CorrelationAnalyzer) isRelated(event Event, alert *Alert) bool {
    // 同一服务
    if event.Service != alert.Labels["service"] {
        return false
    }

    // 部署事件
    if event.Type == "deploy" {
        return true
    }

    // 配置变更
    if event.Type == "config_change" {
        return true
    }

    // 依赖服务告警
    if event.Type == "alert" {
        deps := getDependencies(alert.Labels["service"])
        for _, dep := range deps {
            if event.Service == dep {
                return true
            }
        }
    }

    return false
}

func (c *CorrelationAnalyzer) inferCause(events []Event) string {
    // 优先级: 部署 > 配置变更 > 依赖故障
    for _, event := range events {
        if event.Type == "deploy" {
            return fmt.Sprintf("可能由部署引起: %v", event.Details["version"])
        }
    }

    for _, event := range events {
        if event.Type == "config_change" {
            return fmt.Sprintf("可能由配置变更引起: %v", event.Details["config"])
        }
    }

    return "可能由依赖服务故障引起"
}
```

### 4.4 告警闭环管理

**告警生命周期:**

```
触发 → 分发 → 认领 → 处理 → 解决 → 复盘
```

**Go实现告警闭环:**

```go
package incident

import (
    "context"
    "time"
)

type IncidentStatus string

const (
    StatusOpen       IncidentStatus = "open"
    StatusAcknowledged IncidentStatus = "acknowledged"
    StatusInProgress   IncidentStatus = "in_progress"
    StatusResolved     IncidentStatus = "resolved"
    StatusClosed       IncidentStatus = "closed"
)

type Incident struct {
    ID          string
    Alert       *Alert
    Status      IncidentStatus
    Assignee    string
    CreatedAt   time.Time
    AckedAt     *time.Time
    ResolvedAt  *time.Time
    ClosedAt    *time.Time
    Timeline    []TimelineEvent
    RootCause   string
    Resolution  string
    PostMortem  *PostMortem
}

type TimelineEvent struct {
    Timestamp time.Time
    Action    string  // "created", "acked", "escalated", "resolved"
    Actor     string
    Comment   string
}

// 告警生命周期管理
type IncidentManager struct {
    incidents map[string]*Incident
}

func (m *IncidentManager) CreateIncident(alert *Alert) *Incident {
    incident := &Incident{
        ID:        generateID(),
        Alert:     alert,
        Status:    StatusOpen,
        CreatedAt: time.Now(),
        Timeline:  []TimelineEvent{
            {
                Timestamp: time.Now(),
                Action:    "created",
                Comment:   "Incident created from alert",
            },
        },
    }

    m.incidents[incident.ID] = incident

    // 自动分配On-call工程师
    incident.Assignee = m.getOncallEngineer(alert)

    // 启动SLA计时器
    go m.startSLATimer(incident)

    return incident
}

func (m *IncidentManager) Acknowledge(incidentID, engineer string) {
    incident := m.incidents[incidentID]
    incident.Status = StatusAcknowledged
    now := time.Now()
    incident.AckedAt = &now

    incident.Timeline = append(incident.Timeline, TimelineEvent{
        Timestamp: now,
        Action:    "acknowledged",
        Actor:     engineer,
        Comment:   fmt.Sprintf("%s acknowledged the incident", engineer),
    })

    // 停止升级计时器
    m.stopEscalation(incidentID)
}

func (m *IncidentManager) Resolve(incidentID, resolution string) {
    incident := m.incidents[incidentID]
    incident.Status = StatusResolved
    now := time.Now()
    incident.ResolvedAt = &now
    incident.Resolution = resolution

    incident.Timeline = append(incident.Timeline, TimelineEvent{
        Timestamp: now,
        Action:    "resolved",
        Comment:   resolution,
    })

    // 24小时后自动关闭
    time.AfterFunc(24*time.Hour, func() {
        m.Close(incidentID)
    })
}

func (m *IncidentManager) startSLATimer(incident *Incident) {
    // P0: 15分钟内必须响应
    var timeout time.Duration
    switch incident.Alert.Priority {
    case "P0":
        timeout = 15 * time.Minute
    case "P1":
        timeout = 30 * time.Minute
    case "P2":
        timeout = 2 * time.Hour
    default:
        return
    }

    timer := time.NewTimer(timeout)
    <-timer.C

    if incident.Status == StatusOpen {
        // 超时未响应,升级
        m.Escalate(incident.ID)
    }
}

func (m *IncidentManager) Escalate(incidentID string) {
    incident := m.incidents[incidentID]

    // 升级通知更高级别
    nextLevel := m.getNextEscalationLevel(incident.Assignee)

    sendAlert(&Alert{
        Name:     "IncidentEscalation",
        Priority: "P0",
        Summary:  fmt.Sprintf("Incident %s escalated due to SLA breach", incidentID),
        Details: map[string]interface{}{
            "incident_id": incidentID,
            "original_assignee": incident.Assignee,
        },
    })

    incident.Assignee = nextLevel
    incident.Timeline = append(incident.Timeline, TimelineEvent{
        Timestamp: time.Now(),
        Action:    "escalated",
        Comment:   fmt.Sprintf("Escalated to %s due to SLA breach", nextLevel),
    })
}

// 故障复盘
type PostMortem struct {
    IncidentID   string
    Timeline     []TimelineEvent
    RootCause    string
    Impact       Impact
    ActionItems  []ActionItem
    Preventions  []Prevention
}

type Impact struct {
    AffectedUsers int
    Revenue       float64
    Duration      time.Duration
}

type ActionItem struct {
    Description string
    Assignee    string
    DueDate     time.Time
    Status      string
}

func (m *IncidentManager) GeneratePostMortem(incidentID string) *PostMortem {
    incident := m.incidents[incidentID]

    return &PostMortem{
        IncidentID: incidentID,
        Timeline:   incident.Timeline,
        RootCause:  incident.RootCause,
        Impact: Impact{
            AffectedUsers: m.calculateAffectedUsers(incident),
            Duration:      incident.ResolvedAt.Sub(incident.CreatedAt),
        },
        ActionItems: m.generateActionItems(incident),
    }
}

func (m *IncidentManager) generateActionItems(incident *Incident) []ActionItem {
    var items []ActionItem

    // 根据根因自动生成改进项
    if strings.Contains(incident.RootCause, "deploy") {
        items = append(items, ActionItem{
            Description: "完善发布流程,增加灰度验证",
            Assignee:    "devops-team",
            DueDate:     time.Now().Add(7 * 24 * time.Hour),
        })
    }

    if strings.Contains(incident.RootCause, "capacity") {
        items = append(items, ActionItem{
            Description: "优化容量评估,增加自动扩容",
            Assignee:    "sre-team",
            DueDate:     time.Now().Add(14 * 24 * time.Hour),
        })
    }

    return items
}
```

---

## 五、监控最佳实践

### 5.1 监控指标体系

**层次化指标:**

```yaml
# 基础设施层
infrastructure:
  - node_cpu_usage
  - node_memory_usage
  - node_disk_io
  - node_network_traffic

# 中间件层
middleware:
  - mysql_qps
  - mysql_slow_queries
  - redis_hit_rate
  - kafka_lag

# 应用层
application:
  - http_request_rate
  - http_error_rate
  - http_latency_p99
  - active_connections

# 业务层
business:
  - orders_per_minute
  - gmv_per_hour
  - conversion_rate
  - cart_abandonment_rate
```

### 5.2 Grafana Dashboard设计

**订单服务监控大盘:**

```json
{
  "dashboard": {
    "title": "Order Service Dashboard",
    "rows": [
      {
        "title": "核心指标",
        "panels": [
          {
            "title": "QPS",
            "targets": [
              {
                "expr": "rate(http_requests_total{service='order-service'}[5m])"
              }
            ]
          },
          {
            "title": "错误率",
            "targets": [
              {
                "expr": "rate(http_errors_total[5m]) / rate(http_requests_total[5m])"
              }
            ],
            "alert": {
              "conditions": [
                {
                  "evaluator": {
                    "params": [0.05],
                    "type": "gt"
                  }
                }
              ]
            }
          },
          {
            "title": "P99延迟",
            "targets": [
              {
                "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))"
              }
            ]
          }
        ]
      },
      {
        "title": "业务指标",
        "panels": [
          {
            "title": "订单量趋势",
            "targets": [
              {
                "expr": "increase(orders_created_total[1h])"
              }
            ]
          },
          {
            "title": "GMV",
            "targets": [
              {
                "expr": "sum(increase(order_amount_yuan_sum[1h]))"
              }
            ]
          }
        ]
      }
    ]
  }
}
```

### 5.3 监控成本优化

**采样策略:**

```go
// 动态采样: 根据流量自动调整采样率
type DynamicSampler struct {
    baseRate float64  // 基准采样率10%
}

func (s *DynamicSampler) ShouldSample(ctx context.Context) bool {
    // 1. 错误请求100%采样
    if isError(ctx) {
        return true
    }

    // 2. 慢请求100%采样
    if isSlow(ctx) {
        return true
    }

    // 3. 正常请求按基准采样
    return rand.Float64() < s.baseRate
}

// 指标聚合: 降低存储成本
// 原始数据: 15s间隔,保留7天
// 聚合数据: 5m间隔,保留30天
// 长期数据: 1h间隔,保留1年
```

**存储优化:**

```yaml
# Prometheus配置
storage:
  tsdb:
    retention.time: 15d  # 原始数据保留15天
    retention.size: 500GB

  # 远程写入(长期存储)
  remote_write:
    - url: "http://thanos-receive:19291/api/v1/receive"
      queue_config:
        capacity: 10000
        max_shards: 10
      write_relabel_configs:
        # 只存储关键指标到长期存储
        - source_labels: [__name__]
          regex: '(http_requests_total|orders_created_total|.*_p99)'
          action: keep

# Thanos降采样
thanos:
  downsample:
    - resolution: 5m
      retention: 30d
    - resolution: 1h
      retention: 365d
```

---

## 六、总结

完整的分布式系统监控方案需要:

**技术选型:**
- **指标**: Prometheus + Grafana (实时监控+可视化)
- **日志**: ELK/Loki (日志聚合+检索)
- **链路**: Jaeger + OpenTelemetry (分布式追踪)
- **告警**: AlertManager + PagerDuty (分级告警+值班)

**监控覆盖:**
- **全链路**: 从用户请求到数据库,每个环节可观测
- **多维度**: 基础设施+应用+业务指标
- **实时性**: 秒级采集,分钟级告警

**告警质量:**
- **低误报**: 合理阈值+聚合降噪
- **可执行**: 告警必须包含处理建议
- **闭环管理**: 从触发到复盘完整流程

**成本控制:**
- **采样**: 错误/慢请求100%,正常请求10%
- **聚合**: 原始数据7天,聚合数据1年
- **分级存储**: 热数据SSD,冷数据对象存储
