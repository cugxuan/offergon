---
title: 如何保证消息队列的消息不丢失、不重复、有序？
tags:
  - 分布式系统
status: robot
class: 分布式系统
slug: message-queue-reliability-no-loss-no-duplication-ordering
ref:
---

## 核心要点

- **消息不丢失**：生产者确认（acks=all）+ Broker 持久化 + 消费者手动提交 offset
- **消息不重复**：幂等性生产者 + 事务消息 + 消费者幂等处理（唯一键去重）
- **消息有序**：单 Partition 顺序 + 有序消费者 + 限制并发度为 1
- **CAP 权衡**：强一致性会牺牲性能，需根据业务场景权衡可靠性和吞吐量

## 消息不丢失（No Loss）

消息丢失可能发生在三个环节：生产者、Broker、消费者。

### 1. 生产者侧防丢失

#### 问题场景

```go
// 错误示例：fire-and-forget，网络异常导致消息丢失
producer.SendMessage(&sarama.ProducerMessage{
    Topic: "orders",
    Value: sarama.StringEncoder("order-123"),
})
// 消息可能在网络传输中丢失，生产者不知道
```

#### 解决方案：同步发送 + 重试

```go
import "github.com/IBM/sarama"

func ReliableProducer() sarama.SyncProducer {
    config := sarama.NewConfig()

    // 1. 等待所有副本确认（最高可靠性）
    config.Producer.RequiredAcks = sarama.WaitForAll // acks=all

    // 2. 开启幂等性（防止重试导致重复）
    config.Producer.Idempotent = true

    // 3. 重试配置
    config.Producer.Retry.Max = 3
    config.Producer.Retry.Backoff = 100 * time.Millisecond

    // 4. 确保成功回调
    config.Producer.Return.Successes = true
    config.Producer.Return.Errors = true

    producer, _ := sarama.NewSyncProducer([]string{"localhost:9092"}, config)
    return producer
}

func SendMessageWithRetry(producer sarama.SyncProducer, msg *sarama.ProducerMessage) error {
    partition, offset, err := producer.SendMessage(msg)
    if err != nil {
        // 日志记录失败消息，人工介入或重试队列
        log.Errorf("Failed to send message: %v", err)
        return err
    }

    log.Infof("Message sent to partition %d at offset %d", partition, offset)
    return nil
}
```

#### acks 参数对比

| acks 值 | 说明 | 可靠性 | 性能 | 使用场景 |
|---------|------|--------|------|----------|
| **0** | 不等待确认 | 低（可能丢失） | 最高 | 日志收集、指标上报 |
| **1** | Leader 确认即返回 | 中（Leader 宕机前未复制会丢失） | 高 | 一般业务消息 |
| **all/-1** | 所有 ISR 副本确认 | 高（几乎不丢失） | 低 | 金融交易、订单 |

```properties
# Broker 配置：最少同步副本数
min.insync.replicas=2

# acks=all 时，至少 2 个副本写入成功才返回
# 如果可用副本 < 2，生产者会收到 NotEnoughReplicas 异常
```

### 2. Broker 侧防丢失

#### 问题场景

- **单副本**：Broker 宕机导致数据丢失
- **异步刷盘**：Page Cache 中的数据未刷盘前宕机
- **副本不同步**：Leader 宕机，Follower 数据不完整

#### 解决方案：副本 + ISR 机制

```properties
# server.properties

# 副本配置
default.replication.factor=3  # 默认 3 副本
min.insync.replicas=2         # 最少 2 个副本同步

# 副本同步配置
replica.lag.time.max.ms=10000  # Follower 落后超过 10s 踢出 ISR
replica.lag.max.messages=4000  # 已弃用，使用时间判断

# 禁用 unclean leader 选举（防止数据丢失）
unclean.leader.election.enable=false  # 不允许非 ISR 副本成为 Leader
```

**ISR（In-Sync Replicas）机制**：

```
Leader: Partition-0 [offset: 1000]
  ├─ Follower-1 [offset: 1000] ✓ 在 ISR 中
  ├─ Follower-2 [offset: 995]  ✓ 在 ISR 中（延迟 < 10s）
  └─ Follower-3 [offset: 800]  ✗ 被踢出 ISR（延迟过大）

acks=all 时，只需 ISR 中的副本确认（Follower-1 和 Follower-2）
```

#### Go 生产者配置示例

```go
func BrokerReliableConfig() *sarama.Config {
    config := sarama.NewConfig()

    // 等待 ISR 副本确认
    config.Producer.RequiredAcks = sarama.WaitForAll

    // 网络超时配置
    config.Net.ReadTimeout = 30 * time.Second
    config.Net.WriteTimeout = 30 * time.Second

    // 元数据刷新（检测 Broker 变化）
    config.Metadata.Retry.Max = 3
    config.Metadata.Retry.Backoff = 250 * time.Millisecond

    return config
}
```

### 3. 消费者侧防丢失

#### 问题场景

```go
// 错误示例：自动提交 offset，消息处理失败但 offset 已提交
config.Consumer.Offsets.AutoCommit.Enable = true
config.Consumer.Offsets.AutoCommit.Interval = 1 * time.Second

for msg := range consumer.Messages() {
    err := processMessage(msg) // 处理失败
    // offset 已在后台自动提交，消息丢失
}
```

#### 解决方案：手动提交 offset

```go
func ReliableConsumer() {
    config := sarama.NewConfig()

    // 禁用自动提交
    config.Consumer.Offsets.AutoCommit.Enable = false

    // 从最早的 offset 开始消费（防止漏消息）
    config.Consumer.Offsets.Initial = sarama.OffsetOldest

    consumer, _ := sarama.NewConsumerGroup([]string{"localhost:9092"}, "group1", config)

    handler := &ReliableHandler{}
    consumer.Consume(context.Background(), []string{"orders"}, handler)
}

type ReliableHandler struct{}

func (h *ReliableHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
    for msg := range claim.Messages() {
        // 1. 处理消息
        err := processMessage(msg)
        if err != nil {
            log.Errorf("Failed to process message: %v", err)
            // 可选：写入死信队列或重试队列
            continue
        }

        // 2. 处理成功后才提交 offset
        session.MarkMessage(msg, "")

        // 3. 定期同步提交（提高可靠性）
        session.Commit()
    }
    return nil
}
```

#### At-Least-Once 保证

通过手动提交实现至少一次消费：

```
1. 消费消息（offset 100）
2. 处理消息
3. 提交 offset 101
4. 如果步骤 2 失败，offset 仍为 100，下次重新消费
```

## 消息不重复（No Duplication）

消息重复的根源：网络重传、重试机制、分区重平衡。

### 1. 生产者侧去重

#### 问题场景

```go
// 生产者重试导致消息重复
producer.SendMessage(msg) // 网络超时，实际已写入 Kafka
// 重试逻辑触发
producer.SendMessage(msg) // 再次写入，消息重复
```

#### 解决方案：幂等性生产者

```go
func IdempotentProducer() sarama.SyncProducer {
    config := sarama.NewConfig()

    // 开启幂等性（Kafka 0.11+）
    config.Producer.Idempotent = true

    // 幂等性要求以下配置
    config.Producer.RequiredAcks = sarama.WaitForAll
    config.Producer.Retry.Max = 5
    config.Net.MaxOpenRequests = 1 // 限制单连接并发请求为 1

    producer, _ := sarama.NewSyncProducer([]string{"localhost:9092"}, config)
    return producer
}
```

**幂等性原理**：

```
生产者：
  ├─ Producer ID (PID): 12345
  └─ Sequence Number: 0, 1, 2, 3...

Broker：
  ├─ 缓存 PID + Partition → 最大 Sequence Number
  └─ 重复消息（相同 PID + Seq）直接返回成功，不写入

示例：
  消息 A (PID=12345, Seq=0) → 写入成功
  消息 A (PID=12345, Seq=0) → 检测到重复，返回成功但不写入
```

#### 事务消息（Exactly-Once）

```go
func TransactionalProducer() {
    config := sarama.NewConfig()
    config.Producer.Idempotent = true
    config.Producer.Transaction.ID = "txn-producer-1" // 事务 ID

    producer, _ := sarama.NewSyncProducer([]string{"localhost:9092"}, config)

    // 开启事务
    producer.BeginTxn()

    // 发送多条消息（原子性）
    producer.SendMessage(&sarama.ProducerMessage{Topic: "orders", Value: sarama.StringEncoder("order-1")})
    producer.SendMessage(&sarama.ProducerMessage{Topic: "payments", Value: sarama.StringEncoder("pay-1")})

    // 提交事务（要么全部成功，要么全部回滚）
    producer.CommitTxn()
}
```

### 2. 消费者侧去重

#### 业务幂等性

即使消息重复，处理结果仍然一致。

```go
// 方案 1：唯一键去重（数据库唯一索引）
func ProcessOrder(order *Order) error {
    // 订单号作为唯一键，重复插入会失败
    err := db.Create(&Order{
        OrderID: order.OrderID, // UNIQUE INDEX
        Amount:  order.Amount,
    }).Error

    if errors.Is(err, gorm.ErrDuplicatedKey) {
        log.Warnf("Duplicate order %s, skipping", order.OrderID)
        return nil // 重复消息，幂等处理
    }
    return err
}

// 方案 2：分布式锁
func ProcessOrderWithLock(order *Order) error {
    lockKey := fmt.Sprintf("order:lock:%s", order.OrderID)

    // Redis SET NX（仅在不存在时设置）
    acquired := rdb.SetNX(ctx, lockKey, 1, 10*time.Minute).Val()
    if !acquired {
        log.Warnf("Order %s already processing", order.OrderID)
        return nil
    }
    defer rdb.Del(ctx, lockKey)

    // 处理订单
    return db.Create(order).Error
}

// 方案 3：消息去重表
type ProcessedMessage struct {
    MessageID string `gorm:"primaryKey"` // Kafka offset 或业务 ID
    ProcessedAt time.Time
}

func ProcessWithDeduplication(msg *sarama.ConsumerMessage) error {
    msgID := fmt.Sprintf("%s-%d-%d", msg.Topic, msg.Partition, msg.Offset)

    // 检查是否已处理
    var count int64
    db.Model(&ProcessedMessage{}).Where("message_id = ?", msgID).Count(&count)
    if count > 0 {
        log.Warnf("Message %s already processed", msgID)
        return nil
    }

    // 开启事务
    tx := db.Begin()

    // 处理业务逻辑
    tx.Create(&Order{...})

    // 记录消息已处理
    tx.Create(&ProcessedMessage{MessageID: msgID, ProcessedAt: time.Now()})

    tx.Commit()
    return nil
}
```

#### 消费者事务

```go
// Kafka 事务消费（读取-处理-提交原子性）
func TransactionalConsume() {
    config := sarama.NewConfig()
    config.Consumer.IsolationLevel = sarama.ReadCommitted // 只读已提交消息

    consumer, _ := sarama.NewConsumerGroup([]string{"localhost:9092"}, "group1", config)

    // 事务性处理
    for msg := range consumer.Messages() {
        tx := db.Begin()

        // 处理消息
        tx.Create(&Order{...})

        // 提交 offset 到数据库（而非 Kafka）
        tx.Create(&ConsumerOffset{
            Topic:     msg.Topic,
            Partition: msg.Partition,
            Offset:    msg.Offset,
        })

        tx.Commit()
    }
}
```

## 消息有序（Ordering）

### 1. 全局有序

#### 方案：单 Partition

```go
func CreateOrderedTopic() {
    admin, _ := sarama.NewClusterAdmin([]string{"localhost:9092"}, nil)

    // 创建单分区 Topic
    admin.CreateTopic("strict-order", &sarama.TopicDetail{
        NumPartitions:     1, // 单分区保证全局有序
        ReplicationFactor: 3,
    }, false)
}

func ProduceOrdered(producer sarama.SyncProducer) {
    for i := 0; i < 100; i++ {
        msg := &sarama.ProducerMessage{
            Topic:     "strict-order",
            Value:     sarama.StringEncoder(fmt.Sprintf("msg-%d", i)),
            Partition: 0, // 显式指定 Partition 0
        }
        producer.SendMessage(msg)
    }
}

func ConsumeOrdered() {
    config := sarama.NewConfig()
    consumer, _ := sarama.NewConsumer([]string{"localhost:9092"}, config)

    partitionConsumer, _ := consumer.ConsumePartition("strict-order", 0, sarama.OffsetNewest)

    for msg := range partitionConsumer.Messages() {
        // 消息严格按顺序到达
        fmt.Printf("Received: %s (offset: %d)\n", msg.Value, msg.Offset)
    }
}
```

**缺点**：吞吐量受限于单 Partition，无法并行消费。

### 2. 局部有序（推荐）

#### 方案：相同 Key 路由到同一 Partition

```go
// 订单消息按用户 ID 分区，保证单用户订单有序
func ProducePartialOrdered(producer sarama.SyncProducer, userID string, order *Order) {
    msg := &sarama.ProducerMessage{
        Topic: "orders",
        Key:   sarama.StringEncoder(userID), // 相同 userID 进入同一 Partition
        Value: sarama.ByteEncoder(toJSON(order)),
    }

    // Kafka 默认使用 murmur2 哈希：partition = hash(key) % numPartitions
    producer.SendMessage(msg)
}

// 用户 A 的订单：order-1 → order-2 → order-3（顺序保证）
// 用户 B 的订单：order-10 → order-11（顺序保证）
// 用户 A 和 B 之间无序（但不影响业务）
```

#### 自定义分区器

```go
type CustomPartitioner struct{}

func (p *CustomPartitioner) Partition(msg *sarama.ProducerMessage, numPartitions int32) (int32, error) {
    key := string(msg.Key.(sarama.StringEncoder))

    // 根据业务规则分区
    if strings.HasPrefix(key, "VIP-") {
        return 0, nil // VIP 用户固定在 Partition 0
    }

    // 普通用户哈希分区
    hash := fnv.New32a()
    hash.Write([]byte(key))
    return int32(hash.Sum32() % uint32(numPartitions)), nil
}

func (p *CustomPartitioner) RequiresConsistency() bool {
    return true
}

// 使用自定义分区器
config := sarama.NewConfig()
config.Producer.Partitioner = sarama.NewCustomPartitioner(func(topic string) sarama.Partitioner {
    return &CustomPartitioner{}
})
```

### 3. 消费者有序处理

#### 单线程消费

```go
func OrderedConsumer() {
    config := sarama.NewConfig()
    consumer, _ := sarama.NewConsumerGroup([]string{"localhost:9092"}, "group1", config)

    handler := &OrderedHandler{}
    consumer.Consume(context.Background(), []string{"orders"}, handler)
}

type OrderedHandler struct{}

func (h *OrderedHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
    for msg := range claim.Messages() {
        // 单线程顺序处理
        processMessage(msg)
        session.MarkMessage(msg, "")
    }
    return nil
}
```

#### 多线程乱序问题

```go
// 错误示例：goroutine 并发处理破坏顺序
for msg := range claim.Messages() {
    go func(m *sarama.ConsumerMessage) { // 并发处理
        processMessage(m)
        session.MarkMessage(m, "")
    }(msg)
}
// 消息 1、2、3 可能按 3、1、2 的顺序完成
```

#### 解决方案：按 Key 分发到不同 goroutine

```go
func PartitionedParallelConsumer() {
    // 为每个 Partition 启动独立 goroutine
    partitionWorkers := make(map[int32]chan *sarama.ConsumerMessage)

    for msg := range claim.Messages() {
        partition := msg.Partition

        // 懒加载 worker
        if _, exists := partitionWorkers[partition]; !exists {
            partitionWorkers[partition] = make(chan *sarama.ConsumerMessage, 100)
            go partitionWorker(partitionWorkers[partition], session)
        }

        // 同一 Partition 消息发送到同一 worker（保证有序）
        partitionWorkers[partition] <- msg
    }
}

func partitionWorker(msgChan chan *sarama.ConsumerMessage, session sarama.ConsumerGroupSession) {
    for msg := range msgChan {
        processMessage(msg) // 单线程顺序处理
        session.MarkMessage(msg, "")
    }
}
```

## 综合实践：可靠消息系统

### 完整示例：订单系统

```go
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "github.com/IBM/sarama"
    "gorm.io/gorm"
    "log"
    "time"
)

type Order struct {
    OrderID   string    `gorm:"primaryKey"`
    UserID    string    `gorm:"index"`
    Amount    float64
    CreatedAt time.Time
}

type ProcessedMessage struct {
    MessageID   string `gorm:"primaryKey"`
    ProcessedAt time.Time
}

// 生产者：可靠发送
func ReliableOrderProducer() sarama.SyncProducer {
    config := sarama.NewConfig()
    config.Producer.RequiredAcks = sarama.WaitForAll // 不丢失
    config.Producer.Idempotent = true                // 不重复
    config.Producer.Retry.Max = 3
    config.Producer.Return.Successes = true

    producer, _ := sarama.NewSyncProducer([]string{"localhost:9092"}, config)
    return producer
}

func PublishOrder(producer sarama.SyncProducer, order *Order) error {
    data, _ := json.Marshal(order)

    msg := &sarama.ProducerMessage{
        Topic: "orders",
        Key:   sarama.StringEncoder(order.UserID), // 同用户订单有序
        Value: sarama.ByteEncoder(data),
    }

    partition, offset, err := producer.SendMessage(msg)
    if err != nil {
        log.Printf("Failed to send order %s: %v", order.OrderID, err)
        return err
    }

    log.Printf("Order %s sent to partition %d at offset %d", order.OrderID, partition, offset)
    return nil
}

// 消费者：可靠消费
func ReliableOrderConsumer(db *gorm.DB) {
    config := sarama.NewConfig()
    config.Consumer.Offsets.AutoCommit.Enable = false // 手动提交
    config.Consumer.Offsets.Initial = sarama.OffsetOldest

    consumer, _ := sarama.NewConsumerGroup([]string{"localhost:9092"}, "order-service", config)

    handler := &OrderHandler{db: db}
    consumer.Consume(context.Background(), []string{"orders"}, handler)
}

type OrderHandler struct {
    db *gorm.DB
}

func (h *OrderHandler) Setup(sarama.ConsumerGroupSession) error   { return nil }
func (h *OrderHandler) Cleanup(sarama.ConsumerGroupSession) error { return nil }

func (h *OrderHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
    for msg := range claim.Messages() {
        // 幂等处理
        if err := h.processOrderIdempotent(msg); err != nil {
            log.Printf("Failed to process message: %v", err)
            continue // 不提交 offset，重新消费
        }

        // 处理成功后提交
        session.MarkMessage(msg, "")
        session.Commit()
    }
    return nil
}

func (h *OrderHandler) processOrderIdempotent(msg *sarama.ConsumerMessage) error {
    // 1. 消息去重
    msgID := fmt.Sprintf("%s-%d-%d", msg.Topic, msg.Partition, msg.Offset)

    var count int64
    h.db.Model(&ProcessedMessage{}).Where("message_id = ?", msgID).Count(&count)
    if count > 0 {
        log.Printf("Message %s already processed, skipping", msgID)
        return nil
    }

    // 2. 解析订单
    var order Order
    if err := json.Unmarshal(msg.Value, &order); err != nil {
        return err
    }

    // 3. 事务处理（原子性）
    return h.db.Transaction(func(tx *gorm.DB) error {
        // 3.1 创建订单（唯一索引防重）
        if err := tx.Create(&order).Error; err != nil {
            return err
        }

        // 3.2 记录消息已处理
        if err := tx.Create(&ProcessedMessage{
            MessageID:   msgID,
            ProcessedAt: time.Now(),
        }).Error; err != nil {
            return err
        }

        log.Printf("Order %s processed successfully", order.OrderID)
        return nil
    })
}

func main() {
    // 生产者
    producer := ReliableOrderProducer()
    defer producer.Close()

    PublishOrder(producer, &Order{
        OrderID: "ORDER-001",
        UserID:  "USER-123",
        Amount:  99.99,
    })

    // 消费者
    db, _ := gorm.Open(...)
    ReliableOrderConsumer(db)
}
```

## 性能与可靠性权衡

| 需求 | 配置 | 性能 | 可靠性 |
|------|------|------|--------|
| **高吞吐** | acks=0, async, 批量压缩 | 极高 | 低（可能丢失） |
| **平衡** | acks=1, 幂等生产者, 手动提交 | 高 | 中 |
| **强一致** | acks=all, 事务消息, 单分区 | 低 | 极高 |

### 场景选型

```go
// 场景 1：日志采集（允许少量丢失）
config.Producer.RequiredAcks = sarama.NoResponse // acks=0
config.Producer.Compression = sarama.CompressionLZ4

// 场景 2：用户行为追踪（性能优先）
config.Producer.RequiredAcks = sarama.WaitForLocal // acks=1
config.Producer.Idempotent = true

// 场景 3：订单支付（可靠性优先）
config.Producer.RequiredAcks = sarama.WaitForAll // acks=all
config.Producer.Idempotent = true
config.Producer.Transaction.ID = "payment-producer"
```

## 监控指标

关键指标和告警：

```go
// 生产者监控
metrics := producer.Metrics()
fmt.Printf("Records sent: %d\n", metrics["record-send-total"])
fmt.Printf("Records failed: %d\n", metrics["record-error-total"])

// 消费者监控
lag := getConsumerLag() // 消费延迟
if lag > 10000 {
    alert("消费堆积超过 1 万条")
}

// Broker 监控
// - UnderReplicatedPartitions（副本不足）
// - OfflinePartitionsCount（离线分区）
// - RequestHandlerAvgIdlePercent（请求处理空闲率）
```

## 总结

### 消息不丢失

- **生产者**：acks=all + 重试 + 同步发送
- **Broker**：3 副本 + min.insync.replicas=2 + 禁用 unclean leader 选举
- **消费者**：手动提交 offset + 先处理后提交

### 消息不重复

- **生产者**：幂等性生产者 + 事务消息
- **消费者**：业务幂等（唯一键/分布式锁/去重表）

### 消息有序

- **全局有序**：单 Partition（性能受限）
- **局部有序**：相同 Key 路由到同一 Partition（推荐）
- **消费有序**：单线程消费或按 Partition 分配 goroutine

### 实践建议

1. **根据业务场景选择配置**，不要盲目追求最高可靠性
2. **监控消费延迟和消息堆积**，及时发现问题
3. **定期演练故障场景**（Broker 宕机、网络分区）
4. **设计幂等消费逻辑**，即使消息重复也不影响业务
5. **灰度验证配置变更**，避免影响线上服务
