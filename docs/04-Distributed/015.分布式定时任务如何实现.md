---
title: 分布式定时任务如何实现？
tags:
  - 分布式系统
  - 分布式
status: robot
class: 分布式系统
slug: distributed-scheduled-task-implementation
ref:
---

## 核心要点

- 分布式定时任务需要解决单点故障、任务重复执行、高可用性等问题
- 常见实现方案包括：分布式锁、主从选举、任务分片、专业调度系统（如Kubernetes CronJob、XXL-JOB）
- 核心挑战在于保证同一时刻只有一个节点执行任务，同时实现故障自动转移
- Go语言生态中常用 robfig/cron 库 + Redis分布式锁实现轻量级分布式定时任务

## 详细解答

### 一、为什么需要分布式定时任务

在单机环境中，使用 Linux Crontab 或应用内定时器即可满足需求。但在分布式系统中面临以下问题：

1. **单点故障**：定时任务所在服务器宕机导致任务无法执行
2. **重复执行**：多个服务实例同时执行相同任务，造成数据重复处理
3. **负载均衡**：大批量任务需要分散到多个节点并行处理
4. **任务监控**：需要统一的任务执行状态监控和告警

**典型应用场景**：
- 订单超时自动取消
- 用户会员到期提醒
- 数据报表定时生成
- 数据库定时备份
- 缓存定时更新

### 二、分布式定时任务实现方案

#### 方案1：分布式锁 + Cron（轻量级方案）

**原理**：每个服务节点都运行定时任务，但执行前先尝试获取分布式锁，只有抢到锁的节点才执行。

**优点**：实现简单，无需额外组件
**缺点**：无法任务分片，所有任务集中在一个节点执行

**完整实现代码**：

```go
package scheduler

import (
    "context"
    "fmt"
    "time"

    "github.com/go-redis/redis/v8"
    "github.com/robfig/cron/v3"
)

// RedisLock 基于Redis的分布式锁
type RedisLock struct {
    rdb        *redis.Client
    lockKey    string
    lockValue  string
    expiration time.Duration
}

func NewRedisLock(rdb *redis.Client, lockKey string, expiration time.Duration) *RedisLock {
    return &RedisLock{
        rdb:        rdb,
        lockKey:    lockKey,
        lockValue:  fmt.Sprintf("%d", time.Now().UnixNano()),
        expiration: expiration,
    }
}

// TryLock 尝试获取锁
func (l *RedisLock) TryLock(ctx context.Context) (bool, error) {
    success, err := l.rdb.SetNX(ctx, l.lockKey, l.lockValue, l.expiration).Result()
    if err != nil {
        return false, fmt.Errorf("redis setnx failed: %w", err)
    }
    return success, nil
}

// Unlock 释放锁（使用Lua脚本保证原子性）
func (l *RedisLock) Unlock(ctx context.Context) error {
    script := `
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
    `
    _, err := l.rdb.Eval(ctx, script, []string{l.lockKey}, l.lockValue).Result()
    return err
}

// DistributedScheduler 分布式定时任务调度器
type DistributedScheduler struct {
    rdb  *redis.Client
    cron *cron.Cron
}

func NewDistributedScheduler(rdb *redis.Client) *DistributedScheduler {
    return &DistributedScheduler{
        rdb:  rdb,
        cron: cron.New(cron.WithSeconds()), // 支持秒级定时
    }
}

// AddJob 添加分布式定时任务
func (s *DistributedScheduler) AddJob(spec string, jobName string, job func(context.Context) error) error {
    _, err := s.cron.AddFunc(spec, func() {
        ctx := context.Background()
        lockKey := fmt.Sprintf("cron:lock:%s", jobName)

        // 创建分布式锁（锁过期时间设置为任务预期执行时间的2倍）
        lock := NewRedisLock(s.rdb, lockKey, 5*time.Minute)

        // 尝试获取锁
        acquired, err := lock.TryLock(ctx)
        if err != nil {
            fmt.Printf("[%s] Failed to acquire lock: %v\n", jobName, err)
            return
        }

        if !acquired {
            fmt.Printf("[%s] Another instance is running, skip\n", jobName)
            return
        }

        // 确保任务结束后释放锁
        defer func() {
            if err := lock.Unlock(ctx); err != nil {
                fmt.Printf("[%s] Failed to release lock: %v\n", jobName, err)
            }
        }()

        // 执行任务
        fmt.Printf("[%s] Starting job at %s\n", jobName, time.Now().Format("2006-01-02 15:04:05"))

        if err := job(ctx); err != nil {
            fmt.Printf("[%s] Job failed: %v\n", jobName, err)
            return
        }

        fmt.Printf("[%s] Job completed successfully\n", jobName)
    })

    return err
}

// Start 启动调度器
func (s *DistributedScheduler) Start() {
    s.cron.Start()
}

// Stop 停止调度器
func (s *DistributedScheduler) Stop() {
    s.cron.Stop()
}

// 使用示例
func Example() {
    rdb := redis.NewClient(&redis.Options{
        Addr: "localhost:6379",
    })

    scheduler := NewDistributedScheduler(rdb)

    // 添加定时任务：每分钟执行一次
    scheduler.AddJob("0 * * * * *", "cancel-timeout-orders", func(ctx context.Context) error {
        fmt.Println("Cancelling timeout orders...")
        // 业务逻辑：查询超时订单并取消
        return nil
    })

    // 添加定时任务：每天凌晨2点执行
    scheduler.AddJob("0 0 2 * * *", "generate-daily-report", func(ctx context.Context) error {
        fmt.Println("Generating daily report...")
        // 业务逻辑：生成数据报表
        return nil
    })

    scheduler.Start()

    // 优雅关闭
    // defer scheduler.Stop()
}
```

#### 方案2：主从选举 + Cron

**原理**：通过选举算法（如Raft、Zookeeper）选出主节点，只有主节点执行定时任务，从节点待命。

**优点**：高可用，主节点故障时自动切换
**缺点**：需要引入选举组件，架构复杂

**基于etcd实现主从选举**：

```go
package election

import (
    "context"
    "fmt"
    "time"

    "go.etcd.io/etcd/client/v3"
    "go.etcd.io/etcd/client/v3/concurrency"
)

type LeaderElection struct {
    client   *clientv3.Client
    session  *concurrency.Session
    election *concurrency.Election
}

func NewLeaderElection(endpoints []string, electionKey string) (*LeaderElection, error) {
    client, err := clientv3.New(clientv3.Config{
        Endpoints:   endpoints,
        DialTimeout: 5 * time.Second,
    })
    if err != nil {
        return nil, err
    }

    session, err := concurrency.NewSession(client, concurrency.WithTTL(10))
    if err != nil {
        return nil, err
    }

    election := concurrency.NewElection(session, electionKey)

    return &LeaderElection{
        client:   client,
        session:  session,
        election: election,
    }, nil
}

// Campaign 参与选举
func (e *LeaderElection) Campaign(ctx context.Context, value string) error {
    return e.election.Campaign(ctx, value)
}

// IsLeader 判断是否为Leader
func (e *LeaderElection) IsLeader(ctx context.Context) bool {
    leader, err := e.election.Leader(ctx)
    if err != nil {
        return false
    }

    // 检查当前节点是否是Leader
    return string(leader.Kvs[0].Value) == e.session.Lease().String()
}

// Resign 主动放弃Leader身份
func (e *LeaderElection) Resign(ctx context.Context) error {
    return e.election.Resign(ctx)
}

// Close 关闭选举
func (e *LeaderElection) Close() error {
    e.session.Close()
    return e.client.Close()
}

// SchedulerWithElection 基于选举的定时任务调度器
type SchedulerWithElection struct {
    election *LeaderElection
    scheduler *DistributedScheduler
}

func (s *SchedulerWithElection) RunAsLeader(ctx context.Context) {
    nodeID := fmt.Sprintf("node-%d", time.Now().Unix())

    // 参与选举
    go func() {
        if err := s.election.Campaign(ctx, nodeID); err != nil {
            fmt.Printf("Campaign failed: %v\n", err)
        }
    }()

    // 定期检查是否为Leader
    ticker := time.NewTicker(5 * time.Second)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            if s.election.IsLeader(ctx) {
                fmt.Printf("[%s] I am the leader, running tasks\n", nodeID)
                // 只有Leader执行定时任务
                // s.scheduler.Start()
            } else {
                fmt.Printf("[%s] I am a follower, waiting\n", nodeID)
            }
        case <-ctx.Done():
            return
        }
    }
}
```

#### 方案3：任务分片（高性能方案）

**原理**：将大任务拆分成多个分片，分配给不同节点并行执行。

**优点**：支持海量数据处理，充分利用集群资源
**缺点**：实现复杂，需要任务分片逻辑

**任务分片实现**：

```go
package sharding

import (
    "context"
    "fmt"
    "hash/crc32"
)

// ShardingStrategy 分片策略
type ShardingStrategy interface {
    GetShard(key string, totalShards int) int
}

// HashSharding 哈希分片
type HashSharding struct{}

func (h *HashSharding) GetShard(key string, totalShards int) int {
    hash := crc32.ChecksumIEEE([]byte(key))
    return int(hash % uint32(totalShards))
}

// ShardedTask 分片任务
type ShardedTask struct {
    NodeID      int    // 当前节点ID
    TotalNodes  int    // 集群总节点数
    Strategy    ShardingStrategy
}

func NewShardedTask(nodeID, totalNodes int) *ShardedTask {
    return &ShardedTask{
        NodeID:     nodeID,
        TotalNodes: totalNodes,
        Strategy:   &HashSharding{},
    }
}

// ShouldProcess 判断当前节点是否应该处理该数据
func (s *ShardedTask) ShouldProcess(dataKey string) bool {
    shard := s.Strategy.GetShard(dataKey, s.TotalNodes)
    return shard == s.NodeID
}

// ProcessOrders 示例：处理订单任务（分片）
func (s *ShardedTask) ProcessOrders(ctx context.Context, orderIDs []string) {
    for _, orderID := range orderIDs {
        if s.ShouldProcess(orderID) {
            fmt.Printf("[Node %d] Processing order: %s\n", s.NodeID, orderID)
            // 执行业务逻辑
        } else {
            fmt.Printf("[Node %d] Skipping order: %s (belongs to another shard)\n",
                s.NodeID, orderID)
        }
    }
}

// 使用示例
func ExampleSharding() {
    // 假设有3个节点的集群
    node1 := NewShardedTask(0, 3)
    node2 := NewShardedTask(1, 3)
    node3 := NewShardedTask(2, 3)

    orderIDs := []string{"order-1", "order-2", "order-3", "order-4", "order-5"}

    ctx := context.Background()

    // 每个节点执行相同的任务，但只处理属于自己分片的数据
    node1.ProcessOrders(ctx, orderIDs)
    node2.ProcessOrders(ctx, orderIDs)
    node3.ProcessOrders(ctx, orderIDs)
}
```

#### 方案4：专业调度系统

**开源方案对比**：

| 方案 | 特点 | 适用场景 |
|------|------|----------|
| **XXL-JOB** | Java生态，Web管理界面，任务分片 | Java项目，中大型团队 |
| **Azkaban** | LinkedIn开源，工作流调度 | 复杂依赖的任务链 |
| **Airflow** | Python生态，DAG任务编排 | 数据处理、ETL |
| **Kubernetes CronJob** | K8s原生，容器化任务 | 云原生应用 |
| **TemporalIO** | Go编写，分布式工作流引擎 | 复杂业务流程 |

**Kubernetes CronJob 示例**：

```yaml
# k8s-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: order-timeout-checker
spec:
  schedule: "*/5 * * * *"  # 每5分钟执行一次
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: checker
            image: myapp:latest
            command:
            - /app/bin/order-checker
            env:
            - name: TASK_NAME
              value: "cancel-timeout-orders"
          restartPolicy: OnFailure
```

**Go应用中使用Temporal实现分布式任务**：

```go
package temporal

import (
    "context"
    "time"

    "go.temporal.io/sdk/client"
    "go.temporal.io/sdk/worker"
    "go.temporal.io/sdk/workflow"
)

// OrderTimeoutWorkflow 订单超时检查工作流
func OrderTimeoutWorkflow(ctx workflow.Context) error {
    ao := workflow.ActivityOptions{
        StartToCloseTimeout: 10 * time.Minute,
    }
    ctx = workflow.WithActivityOptions(ctx, ao)

    var result string
    err := workflow.ExecuteActivity(ctx, CancelTimeoutOrdersActivity).Get(ctx, &result)
    if err != nil {
        return err
    }

    workflow.GetLogger(ctx).Info("Workflow completed", "result", result)
    return nil
}

// CancelTimeoutOrdersActivity 活动函数
func CancelTimeoutOrdersActivity(ctx context.Context) (string, error) {
    // 查询并取消超时订单
    return "Processed 10 timeout orders", nil
}

// StartTemporalWorker 启动Temporal Worker
func StartTemporalWorker() error {
    c, err := client.Dial(client.Options{
        HostPort: "localhost:7233",
    })
    if err != nil {
        return err
    }
    defer c.Close()

    w := worker.New(c, "order-task-queue", worker.Options{})

    w.RegisterWorkflow(OrderTimeoutWorkflow)
    w.RegisterActivity(CancelTimeoutOrdersActivity)

    return w.Run(worker.InterruptCh())
}
```

### 三、实际案例：订单超时自动取消

**需求**：电商系统中，未支付订单30分钟后自动取消。

**实现方案**：延迟队列 + 定时扫描兜底

```go
package order

import (
    "context"
    "fmt"
    "time"

    "github.com/go-redis/redis/v8"
)

// DelayQueue 基于Redis的延迟队列
type DelayQueue struct {
    rdb *redis.Client
    key string
}

func NewDelayQueue(rdb *redis.Client, key string) *DelayQueue {
    return &DelayQueue{rdb: rdb, key: key}
}

// Enqueue 添加延迟任务
func (q *DelayQueue) Enqueue(ctx context.Context, orderID string, delaySeconds int) error {
    executeAt := time.Now().Add(time.Duration(delaySeconds) * time.Second).Unix()
    return q.rdb.ZAdd(ctx, q.key, &redis.Z{
        Score:  float64(executeAt),
        Member: orderID,
    }).Err()
}

// Dequeue 获取到期任务
func (q *DelayQueue) Dequeue(ctx context.Context, limit int64) ([]string, error) {
    now := time.Now().Unix()

    // 查询score <= now 的任务
    result, err := q.rdb.ZRangeByScore(ctx, q.key, &redis.ZRangeBy{
        Min:   "-inf",
        Max:   fmt.Sprintf("%d", now),
        Count: limit,
    }).Result()

    if err != nil {
        return nil, err
    }

    if len(result) > 0 {
        // 删除已取出的任务
        q.rdb.ZRem(ctx, q.key, result)
    }

    return result, nil
}

// OrderService 订单服务
type OrderService struct {
    delayQueue *DelayQueue
    scheduler  *DistributedScheduler
}

// CreateOrder 创建订单时加入延迟队列
func (s *OrderService) CreateOrder(ctx context.Context, orderID string) error {
    // 创建订单逻辑
    fmt.Printf("Order created: %s\n", orderID)

    // 添加到延迟队列（30分钟后执行）
    return s.delayQueue.Enqueue(ctx, orderID, 1800)
}

// StartTimeoutChecker 启动超时检查定时任务
func (s *OrderService) StartTimeoutChecker() {
    // 每分钟检查一次延迟队列
    s.scheduler.AddJob("0 * * * * *", "check-timeout-orders", func(ctx context.Context) error {
        orderIDs, err := s.delayQueue.Dequeue(ctx, 100)
        if err != nil {
            return err
        }

        for _, orderID := range orderIDs {
            if err := s.cancelOrder(ctx, orderID); err != nil {
                fmt.Printf("Failed to cancel order %s: %v\n", orderID, err)
            }
        }

        return nil
    })

    // 兜底任务：每10分钟扫描数据库（防止延迟队列丢失）
    s.scheduler.AddJob("0 */10 * * * *", "scan-timeout-orders", func(ctx context.Context) error {
        // 扫描数据库中创建时间超过30分钟且状态为"待支付"的订单
        fmt.Println("Scanning database for timeout orders...")
        return nil
    })

    s.scheduler.Start()
}

func (s *OrderService) cancelOrder(ctx context.Context, orderID string) error {
    fmt.Printf("Cancelling timeout order: %s\n", orderID)
    // 更新订单状态为"已取消"
    // 释放库存
    return nil
}
```

### 四、方案选择建议

| 场景 | 推荐方案 | 理由 |
|------|----------|------|
| 小规模应用（QPS < 1000） | 分布式锁 + Cron | 简单可靠，无额外组件 |
| 中等规模（需要高可用） | 主从选举 + Cron | 自动故障转移 |
| 大规模（百万级任务） | 任务分片 | 并行处理，性能高 |
| 复杂工作流 | Temporal/Airflow | 任务编排、重试、监控 |
| Kubernetes环境 | CronJob | 云原生，运维简单 |

### 五、最佳实践

1. **幂等性设计**：定时任务必须支持重复执行
2. **超时控制**：设置任务执行超时时间，防止死锁
3. **监控告警**：记录任务执行日志，失败时及时告警
4. **优雅关闭**：应用停机时等待任务执行完成
5. **错误重试**：失败任务自动重试，设置最大重试次数
6. **资源隔离**：定时任务与在线服务分离部署
7. **时区处理**：统一使用UTC时间，避免时区混乱

```go
// 完整的生产级定时任务示例
package production

import (
    "context"
    "fmt"
    "os"
    "os/signal"
    "syscall"
    "time"
)

func ProductionExample() {
    ctx, cancel := context.WithCancel(context.Background())
    defer cancel()

    // 初始化Redis
    rdb := redis.NewClient(&redis.Options{Addr: "localhost:6379"})

    scheduler := NewDistributedScheduler(rdb)

    // 添加任务
    scheduler.AddJob("0 */5 * * * *", "health-check", func(ctx context.Context) error {
        timeout, cancel := context.WithTimeout(ctx, 4*time.Minute)
        defer cancel()

        // 任务逻辑（带超时控制）
        return doHealthCheck(timeout)
    })

    scheduler.Start()

    // 优雅关闭
    sigCh := make(chan os.Signal, 1)
    signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)

    <-sigCh
    fmt.Println("Shutting down gracefully...")

    scheduler.Stop()
    fmt.Println("Scheduler stopped")
}

func doHealthCheck(ctx context.Context) error {
    // 模拟耗时任务
    select {
    case <-time.After(2 * time.Second):
        return nil
    case <-ctx.Done():
        return ctx.Err()
    }
}
```

## 总结

分布式定时任务的核心是解决**并发安全**和**高可用性**问题。轻量级应用推荐使用 **分布式锁 + Cron** 方案，中大型应用可选择 **主从选举** 或 **专业调度系统**。设计时需重点关注幂等性、监控告警、优雅关闭等细节，确保系统稳定可靠。
