---
title: 如何优化批量数据导入的性能？
tags:
  - 性能优化
status: robot
class: 性能优化
slug: optimize-bulk-data-import-performance
ref:
---

## 核心要点

**关闭约束与索引**:导入前禁用外键、唯一索引、全文索引,导入后重建
**批量操作**:使用 LOAD DATA INFILE、批量 INSERT(1000行/批),避免逐行插入
**并行导入**:分表分区并行写入,充分利用多核CPU和磁盘I/O
**数据库参数调优**:调大 innodb_buffer_pool_size、关闭 binlog/fsync
**数据预处理**:清洗脏数据、排序后导入(避免页分裂)、压缩传输

---

## 详细回答

### 一、批量数据导入的性能瓶颈

#### 1. 常见性能问题

| 瓶颈类型 | 表现 | 原因 | 影响 |
|---------|------|------|------|
| **磁盘 I/O** | 导入速度 < 10MB/s | 随机写入、频繁 fsync | 大量时间浪费在磁盘等待 |
| **索引维护** | 有索引的表导入慢 10 倍 | 每次插入都要更新 B+树 | CPU 和 I/O 双重开销 |
| **事务日志** | binlog/redo log 写入慢 | 同步刷盘(fsync) | 每次提交都要等待磁盘 |
| **锁竞争** | 单表并发导入阻塞 | 表锁或行锁争抢 | 并发度下降 |
| **网络传输** | 大文件上传慢 | 未压缩、单线程 | 网络带宽成为瓶颈 |

**实际案例:**
- 导入 1 亿条数据(10GB),逐行 INSERT 需要 **50 小时**
- 优化后使用 LOAD DATA INFILE,**仅需 30 分钟**(提速 100 倍)

---

### 二、MySQL 批量导入优化方案

#### 1. 使用 LOAD DATA INFILE(推荐)

这是 MySQL 最快的批量导入方式,比 INSERT 快 **20-100 倍**。

##### 基本用法

```sql
-- 1. 准备 CSV 文件 (data.csv)
-- user_id,username,email,created_at
-- 1,alice,alice@example.com,2024-01-01 10:00:00
-- 2,bob,bob@example.com,2024-01-01 11:00:00

-- 2. 导入数据
LOAD DATA INFILE '/path/to/data.csv'
INTO TABLE users
FIELDS TERMINATED BY ','      -- 字段分隔符
ENCLOSED BY '"'                -- 字段包围符
LINES TERMINATED BY '\n'       -- 行分隔符
IGNORE 1 ROWS                  -- 跳过表头
(user_id, username, email, created_at);
```

##### 本地文件导入(安全性更高)

```sql
-- 从客户端本地文件导入(不需要服务器文件系统权限)
LOAD DATA LOCAL INFILE '/local/path/data.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
IGNORE 1 ROWS
(user_id, username, email, created_at);
```

##### 性能对比

| 方式 | 1 亿条数据导入时间 | 性能比 |
|-----|-----------------|--------|
| 逐行 INSERT | 50 小时 | 1× |
| 批量 INSERT(1000行/批) | 5 小时 | 10× |
| **LOAD DATA INFILE** | **30 分钟** | **100×** |

**原理:**
- 直接写入数据文件,跳过 SQL 解析
- 批量更新索引,减少 B+树维护开销
- 减少事务提交次数

---

#### 2. 批量 INSERT 优化

如果无法使用 LOAD DATA INFILE,可以使用批量 INSERT。

##### ❌ 错误做法:逐行插入

```go
// 极慢:每次插入都要提交事务
for _, user := range users {
    db.Exec("INSERT INTO users (id, name) VALUES (?, ?)", user.ID, user.Name)
}
// 插入 100 万条数据需要 1 小时
```

##### ✅ 正确做法:批量插入

```go
// 方案 A:拼接多个 VALUES(推荐)
func batchInsert(users []User, batchSize int) {
    for i := 0; i < len(users); i += batchSize {
        end := i + batchSize
        if end > len(users) {
            end = len(users)
        }
        batch := users[i:end]

        // 构造 SQL:INSERT INTO users (...) VALUES (...),(...),(...)
        placeholders := make([]string, len(batch))
        values := make([]interface{}, 0, len(batch)*2)
        for j, user := range batch {
            placeholders[j] = "(?, ?)"
            values = append(values, user.ID, user.Name)
        }
        sql := fmt.Sprintf("INSERT INTO users (id, name) VALUES %s",
            strings.Join(placeholders, ","))

        db.Exec(sql, values...)
    }
}
// 插入 100 万条数据仅需 5 分钟
```

**批量大小选择:**
- **推荐 1000 行/批**(平衡性能和内存)
- 太小(< 100):性能提升不明显
- 太大(> 5000):单个事务过大,占用内存多

##### 方案 B:使用事务 + PreparedStatement

```go
func batchInsertWithTx(users []User, batchSize int) error {
    tx, _ := db.Begin()
    stmt, _ := tx.Prepare("INSERT INTO users (id, name) VALUES (?, ?)")
    defer stmt.Close()

    for i, user := range users {
        stmt.Exec(user.ID, user.Name)

        // 每 1000 条提交一次事务
        if (i+1)%batchSize == 0 {
            tx.Commit()
            tx, _ = db.Begin()
            stmt, _ = tx.Prepare("INSERT INTO users (id, name) VALUES (?, ?)")
        }
    }
    tx.Commit()
    return nil
}
```

**优点:**
- 减少事务提交次数(1000 行提交 1 次)
- 使用 PreparedStatement 避免重复解析 SQL

---

#### 3. 索引与约束管理

##### 策略:导入前禁用索引,导入后重建

```sql
-- 1. 导入前:禁用索引和约束
ALTER TABLE users DISABLE KEYS;              -- 禁用非主键索引(MyISAM)
SET foreign_key_checks = 0;                  -- 禁用外键检查
SET unique_checks = 0;                       -- 禁用唯一性检查

-- 2. 导入数据(使用 LOAD DATA INFILE 或批量 INSERT)
LOAD DATA INFILE '/path/to/data.csv' INTO TABLE users ...;

-- 3. 导入后:重新启用索引和约束
ALTER TABLE users ENABLE KEYS;               -- 启用索引
SET foreign_key_checks = 1;
SET unique_checks = 1;
```

##### InnoDB 表的索引优化

```sql
-- 方案 A:删除二级索引,导入后重建(适合新表)
-- 1. 删除除主键外的所有索引
ALTER TABLE users DROP INDEX idx_email;
ALTER TABLE users DROP INDEX idx_created_at;

-- 2. 导入数据
LOAD DATA INFILE '/path/to/data.csv' INTO TABLE users ...;

-- 3. 重建索引(利用 Fast Index Creation)
ALTER TABLE users ADD INDEX idx_email(email);
ALTER TABLE users ADD INDEX idx_created_at(created_at);
```

**性能提升:**
- 有 3 个二级索引的表,禁用索引后导入速度提升 **5-10 倍**
- 导入后批量重建索引比逐行维护快 **3-5 倍**

---

#### 4. 数据库参数调优

##### 关键参数配置(仅导入期间临时修改)

```sql
-- 1. 增大 InnoDB 缓冲池(加速索引构建)
SET GLOBAL innodb_buffer_pool_size = 8G;    -- 设为物理内存的 50-70%

-- 2. 调整事务日志大小
SET GLOBAL innodb_log_file_size = 512M;     -- 减少日志切换
SET GLOBAL innodb_log_buffer_size = 64M;    -- 增大日志缓冲

-- 3. 禁用 binlog(非生产环境)
SET sql_log_bin = 0;                         -- 临时关闭 binlog

-- 4. 调整刷盘策略(风险:断电丢数据)
SET GLOBAL innodb_flush_log_at_trx_commit = 2;  -- 每秒刷盘一次,而非每次提交
SET GLOBAL sync_binlog = 0;                     -- 禁用 binlog 同步刷盘

-- 5. 禁用双写缓冲(SSD 环境可考虑)
SET GLOBAL innodb_doublewrite = 0;

-- 6. 增大批量插入缓冲
SET GLOBAL bulk_insert_buffer_size = 256M;
```

**风险提示:**
- `innodb_flush_log_at_trx_commit=2` 会降低持久性,**仅在导入期间使用**
- 导入完成后务必恢复默认值(=1)

**性能提升:**
- 调整参数后,大批量导入性能提升 **2-3 倍**

---

#### 5. 数据预处理优化

##### 5.1 按主键排序后导入(重要!)

```bash
# 1. 对 CSV 文件按主键排序
sort -t',' -k1 -n data.csv > data_sorted.csv

# 2. 导入排序后的数据
mysql -e "LOAD DATA INFILE '/path/to/data_sorted.csv' INTO TABLE users ..."
```

**原理:**
- InnoDB 聚簇索引按主键排序存储
- 顺序插入避免页分裂(Page Split),减少磁盘 I/O
- 性能提升 **30-50%**

**对比:**
| 导入方式 | B+树页分裂次数 | 导入时间 |
|---------|--------------|---------|
| 乱序导入 | 50 万次 | 10 分钟 |
| **排序后导入** | **5000 次** | **6 分钟** |

##### 5.2 数据清洗与校验

```python
import pandas as pd

# 1. 读取原始数据
df = pd.read_csv('raw_data.csv')

# 2. 数据清洗
df = df.drop_duplicates(subset=['user_id'])    # 去重
df = df.dropna(subset=['email'])               # 删除空值
df['created_at'] = pd.to_datetime(df['created_at'])  # 格式转换

# 3. 数据校验
assert df['user_id'].is_unique, "主键重复"
assert df['email'].str.contains('@').all(), "邮箱格式错误"

# 4. 按主键排序
df = df.sort_values('user_id')

# 5. 导出为 CSV
df.to_csv('clean_data.csv', index=False)
```

**好处:**
- 避免导入失败后回滚(回滚成本高)
- 减少脏数据导致的性能问题

##### 5.3 数据压缩传输

```bash
# 方案 A:压缩后传输
gzip data.csv                              # 压缩(10GB → 2GB)
scp data.csv.gz user@server:/tmp/          # 传输
gunzip /tmp/data.csv.gz                    # 解压

# 方案 B:边压缩边传输
gzip -c data.csv | ssh user@server 'gunzip > /tmp/data.csv'
```

**效果:**
- CSV 文件压缩比通常达到 **5-10 倍**
- 10GB 文件传输时间从 10 分钟降到 2 分钟

---

### 三、并行导入策略

#### 1. 分表并行导入

**场景:** 导入数据到分库分表环境。

```go
// 将 1 亿条数据分到 16 个表并行导入
func parallelImport(data []Record, tableCount int) {
    var wg sync.WaitGroup
    chunkSize := len(data) / tableCount

    for i := 0; i < tableCount; i++ {
        wg.Add(1)
        go func(tableIndex int) {
            defer wg.Done()

            start := tableIndex * chunkSize
            end := start + chunkSize
            if tableIndex == tableCount-1 {
                end = len(data)  // 最后一个 goroutine 处理剩余数据
            }
            chunk := data[start:end]

            // 导入到对应分表
            tableName := fmt.Sprintf("users_%02d", tableIndex)
            importToTable(tableName, chunk)
        }(i)
    }
    wg.Wait()
}
```

**性能提升:**
- 16 表并行导入,时间从 30 分钟降到 **2-3 分钟**(接近线性提升)

#### 2. 分区并行导入

**场景:** 单表分区(按时间或 Hash 分区)。

```sql
-- 创建分区表
CREATE TABLE orders (
    order_id BIGINT PRIMARY KEY,
    user_id BIGINT,
    created_at DATETIME
) PARTITION BY RANGE (YEAR(created_at)) (
    PARTITION p2021 VALUES LESS THAN (2022),
    PARTITION p2022 VALUES LESS THAN (2023),
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025)
);
```

```bash
# 并行导入不同分区的数据
mysqlimport --local --use-threads=4 \
  mydb /data/orders_2021.csv \
       /data/orders_2022.csv \
       /data/orders_2023.csv \
       /data/orders_2024.csv
```

**好处:**
- 不同分区的数据物理隔离,并行写入无锁竞争
- MySQL 8.0+ 支持分区表的并行导入

#### 3. 使用多个数据库连接

```go
// 使用连接池并行导入
func parallelInsert(data []Record, workerCount int) {
    jobs := make(chan []Record, workerCount)
    var wg sync.WaitGroup

    // 启动多个 worker
    for i := 0; i < workerCount; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            db := getDBConnection()  // 每个 worker 独立连接
            for batch := range jobs {
                batchInsert(db, batch)
            }
        }()
    }

    // 分发任务
    batchSize := 1000
    for i := 0; i < len(data); i += batchSize {
        end := i + batchSize
        if end > len(data) {
            end = len(data)
        }
        jobs <- data[i:end]
    }
    close(jobs)
    wg.Wait()
}
```

**注意事项:**
- 单表并发写入可能遇到锁竞争,建议 **worker 数 ≤ CPU 核心数**
- 分表/分区环境可以更激进(worker 数 = 表数量)

---

### 四、其他数据库的批量导入优化

#### 1. PostgreSQL

##### COPY 命令(最快)

```sql
-- 从文件导入
COPY users (user_id, username, email)
FROM '/path/to/data.csv'
WITH (FORMAT csv, HEADER true);

-- 从程序导入(使用 STDIN)
COPY users (user_id, username, email)
FROM STDIN
WITH (FORMAT csv);
-- 然后通过程序发送数据流
```

##### 性能调优

```sql
-- 1. 禁用 WAL(风险:断电丢数据)
ALTER TABLE users SET UNLOGGED;

-- 2. 增大 maintenance_work_mem(加速索引创建)
SET maintenance_work_mem = '2GB';

-- 3. 调整检查点参数
SET checkpoint_segments = 128;
SET checkpoint_completion_target = 0.9;

-- 4. 导入后重建索引
REINDEX TABLE users;
```

#### 2. MongoDB

```javascript
// 批量插入(使用 insertMany)
const bulk = db.users.initializeUnorderedBulkOp();
for (let i = 0; i < users.length; i++) {
    bulk.insert(users[i]);
    if (i % 1000 === 0) {
        bulk.execute();
        bulk = db.users.initializeUnorderedBulkOp();
    }
}
bulk.execute();
```

```bash
# 使用 mongoimport 工具
mongoimport --db mydb --collection users \
  --type csv --headerline \
  --numInsertionWorkers 4 \
  --file data.csv
```

#### 3. ClickHouse(OLAP 数据库)

```sql
-- ClickHouse 最快的导入方式
cat data.csv | clickhouse-client --query="
  INSERT INTO users FORMAT CSV
"

-- 或从文件导入
clickhouse-client --query="
  INSERT INTO users FORMAT CSV
" < data.csv
```

**性能:**
- ClickHouse 导入速度可达 **100-200 MB/s**(单核)
- 并行导入可达 **1-2 GB/s**

---

### 五、完整优化方案示例

#### 场景:导入 1 亿条用户数据(10GB CSV)

**原始方案(极慢):**
```python
# 逐行插入
for row in csv.reader(open('data.csv')):
    cursor.execute("INSERT INTO users VALUES (%s, %s, %s)", row)
# 耗时:50 小时
```

**优化后方案(快 100 倍):**

```bash
#!/bin/bash

# 步骤 1:数据预处理
echo "1. 数据清洗与排序..."
python clean_data.py        # 去重、校验、格式化
sort -t',' -k1 -n clean_data.csv > sorted_data.csv

# 步骤 2:拆分文件(准备并行导入)
echo "2. 拆分文件到 16 个分片..."
split -n l/16 sorted_data.csv part_

# 步骤 3:调整数据库参数
echo "3. 调整 MySQL 参数..."
mysql -e "
  SET GLOBAL innodb_buffer_pool_size = 8G;
  SET GLOBAL innodb_flush_log_at_trx_commit = 2;
  SET GLOBAL sync_binlog = 0;
"

# 步骤 4:禁用索引
echo "4. 禁用二级索引..."
mysql -e "
  ALTER TABLE users DROP INDEX idx_email;
  ALTER TABLE users DROP INDEX idx_created_at;
  SET foreign_key_checks = 0;
  SET unique_checks = 0;
"

# 步骤 5:并行导入(16 个进程)
echo "5. 并行导入数据..."
for i in {0..15}; do
  (
    mysql -e "
      SET sql_log_bin = 0;
      LOAD DATA LOCAL INFILE 'part_$i'
      INTO TABLE users_$i
      FIELDS TERMINATED BY ','
      LINES TERMINATED BY '\n'
      (user_id, username, email, created_at);
    "
  ) &
done
wait

# 步骤 6:重建索引
echo "6. 重建索引..."
mysql -e "
  ALTER TABLE users ADD INDEX idx_email(email);
  ALTER TABLE users ADD INDEX idx_created_at(created_at);
"

# 步骤 7:恢复参数
echo "7. 恢复数据库参数..."
mysql -e "
  SET GLOBAL innodb_flush_log_at_trx_commit = 1;
  SET GLOBAL sync_binlog = 1;
  SET foreign_key_checks = 1;
  SET unique_checks = 1;
"

echo "导入完成!"
```

**性能对比:**

| 步骤 | 耗时 | 累计提升 |
|------|------|---------|
| 原始逐行插入 | 50 小时 | 基准 |
| + 批量 INSERT(1000行/批) | 5 小时 | 10× |
| + LOAD DATA INFILE | 30 分钟 | 100× |
| + 禁用索引 | 10 分钟 | 300× |
| + 参数调优 | 6 分钟 | 500× |
| + 数据排序 | 4 分钟 | 750× |
| + **并行导入(16表)** | **15 秒** | **12000×** |

---

### 六、监控与故障处理

#### 1. 导入进度监控

```sql
-- 查看当前导入进度(InnoDB)
SELECT
  TABLE_NAME,
  TABLE_ROWS,
  DATA_LENGTH / 1024 / 1024 AS data_mb,
  INDEX_LENGTH / 1024 / 1024 AS index_mb
FROM information_schema.TABLES
WHERE TABLE_NAME = 'users';

-- 查看当前正在执行的 LOAD DATA
SHOW PROCESSLIST;
```

```bash
# 监控文件导入进度(Linux)
watch -n 5 'ls -lh /var/lib/mysql/mydb/users.ibd'
```

#### 2. 常见错误处理

**错误 1:Packet too large**
```sql
-- 增大数据包大小
SET GLOBAL max_allowed_packet = 1G;
```

**错误 2:Lock wait timeout**
```sql
-- 增大锁等待超时时间
SET GLOBAL innodb_lock_wait_timeout = 600;
```

**错误 3:Out of memory**
```bash
# 减小批量大小
batch_size = 500  # 从 1000 降到 500
```

**错误 4:主键冲突**
```sql
-- 使用 REPLACE 或 INSERT IGNORE
LOAD DATA INFILE '/path/to/data.csv'
REPLACE INTO TABLE users ...;  -- 冲突时替换
-- 或
INSERT IGNORE INTO users ...;  -- 冲突时跳过
```

#### 3. 回滚与恢复

```bash
# 导入前备份
mysqldump mydb users > users_backup.sql

# 导入失败后恢复
mysql mydb < users_backup.sql
```

**建议:**
- 大批量导入前做好备份(mysqldump 或快照)
- 分批导入,每批成功后做检查点
- 使用事务(小批量)或分表(大批量)保证可回滚

---

### 七、最佳实践总结

#### 设计阶段

✅ **DO:**
- 提前设计好表结构,避免导入后频繁 DDL
- 使用分区表或分库分表,便于并行导入
- 主键选择自增 ID 或有序 UUID,避免页分裂

❌ **DON'T:**
- 创建过多索引(导入前删除,导入后重建)
- 使用复杂的外键约束(导入期间禁用)
- 使用随机主键(UUID v4),导致严重页分裂

#### 实施阶段

✅ **DO:**
- 优先使用 LOAD DATA INFILE(性能最佳)
- 数据按主键排序后导入
- 禁用索引、binlog、外键检查
- 并行导入(分表/分区)

❌ **DON'T:**
- 逐行插入(性能差 100 倍)
- 在生产环境直接导入(影响业务)
- 导入未清洗的脏数据

#### 运维阶段

✅ **DO:**
- 监控磁盘空间(数据+索引+临时文件)
- 导入后执行 ANALYZE TABLE 更新统计信息
- 恢复数据库参数到默认值

❌ **DON'T:**
- 导入期间执行其他重负载操作
- 忘记重建索引
- 长期使用 `innodb_flush_log_at_trx_commit=2`

---

### 八、不同规模的导入方案选择

| 数据量 | 推荐方案 | 预计耗时 | 注意事项 |
|--------|---------|---------|---------|
| < 10 万 | 批量 INSERT(1000行/批) | < 1 分钟 | 无需特殊优化 |
| 10万-100万 | LOAD DATA INFILE | 1-5 分钟 | 禁用索引 |
| 100万-1000万 | LOAD DATA + 禁用索引 + 参数调优 | 5-30 分钟 | 数据排序 |
| 1000万-1亿 | LOAD DATA + 分表并行导入 | 10-60 分钟 | 监控磁盘空间 |
| **> 1 亿** | **分表并行 + 分批导入** | **1-10 小时** | **分多天导入,避免影响业务** |

---

## 总结

批量数据导入性能优化的核心是 **减少磁盘 I/O 和索引维护开销**。实践中需要把握以下要点:

1. **使用最快的导入方式**:LOAD DATA INFILE > 批量 INSERT > 逐行 INSERT
2. **管理索引与约束**:导入前禁用,导入后重建(提速 5-10 倍)
3. **数据库参数调优**:调大缓冲池,调整刷盘策略(提速 2-3 倍)
4. **数据预处理**:排序、清洗、压缩(提速 30-50%)
5. **并行导入**:分表/分区并行写入(线性提速)

通过综合运用这些技术,可以将 **数十小时的导入任务优化到数分钟**,性能提升高达 **100-1000 倍**。关键是要根据数据规模、硬件资源、业务要求选择合适的方案,并做好监控和故障恢复预案。
