---
title: 如何解决 Redis 的热 key 和大 key 问题?
tags:
  - 性能优化
  - 缓存
status: robot
class: 性能优化
slug: redis-hot-key-big-key-solution
ref:
---

## 核心要点

**热 key 问题:** 少数 key 被高频访问,导致单机负载过高、流量倾斜、服务雪崩

**大 key 问题:** 单个 key 占用内存过大,导致阻塞操作、网络拥堵、主从同步延迟

**解决思路:** 发现问题 → 分析影响 → 针对性优化(拆分、缓存、限流)

---

## 详细回答

### 一、热 Key 问题

#### 1. 什么是热 Key?

**定义:** 在一段时间内,某个或某些 key 被极高频率地访问,远超其他 key。

**典型场景:**
- **突发热点:** 明星微博、热门新闻、秒杀商品
- **业务热点:** 热门商品详情、热门直播间信息
- **系统配置:** 全局配置项被所有请求访问

**示例:**
```
正常情况:
key1: 100 QPS
key2: 150 QPS
key3: 120 QPS

热 key 出现:
hot_key: 100,000 QPS  ← 热 key
key1: 100 QPS
key2: 150 QPS
```

---

#### 2. 热 Key 带来的问题

**性能问题:**
- **单机负载过高:** Redis 单线程处理请求,热 key 导致 CPU 打满
- **流量倾斜:** 集群模式下,热 key 所在节点负载极高,其他节点空闲
- **网络拥堵:** 热 key 数据频繁传输,占满网卡带宽

**稳定性问题:**
- **缓存击穿:** 热 key 过期瞬间,大量请求直接打到数据库
- **服务雪崩:** 热 key 所在 Redis 节点挂掉,请求全部失败
- **响应延迟:** 热 key 处理慢,阻塞其他请求

**实际案例:**
某电商平台秒杀活动,商品详情 key 被 50 万 QPS 访问,导致:
- Redis 节点 CPU 100%
- 响应时间从 1ms 飙升到 500ms
- 服务大面积超时

---

#### 3. 如何发现热 Key?

##### 方法 1: 客户端统计

在应用层统计 key 的访问频率。

```go
// Go 示例: 使用本地计数器
var keyCounter sync.Map

func GetFromRedis(key string) (string, error) {
    // 统计访问次数
    count, _ := keyCounter.LoadOrStore(key, int64(0))
    atomic.AddInt64(count.(*int64), 1)

    // 定期上报热点 key
    if *count.(*int64) > 10000 {
        log.Printf("Hot key detected: %s, count: %d", key, *count.(*int64))
    }

    return redisClient.Get(ctx, key).Result()
}
```

**优势:** 最准确,可以统计应用层的访问模式
**劣势:** 需要改造代码,有性能开销

---

##### 方法 2: Redis Monitor 命令

实时监控 Redis 执行的所有命令。

```bash
# 连接 Redis
redis-cli

# 开启监控
MONITOR

# 输出示例
1610000000.123456 [0 127.0.0.1:12345] "GET" "user:1001"
1610000000.234567 [0 127.0.0.1:12345] "GET" "product:999"
1610000000.345678 [0 127.0.0.1:12345] "GET" "product:999"
```

然后使用脚本统计高频 key:

```bash
# 监控 1 分钟,统计 TOP 10 热 key
redis-cli MONITOR | head -n 100000 | awk '{print $4}' | sort | uniq -c | sort -rn | head -10
```

**优势:** 无需改代码,实时监控
**劣势:** 性能开销大(会降低 50% 吞吐量),不适合生产环境长时间使用

---

##### 方法 3: hotkeys 参数(推荐)

Redis 4.0.3+ 提供 `--hotkeys` 参数,基于 LFU(Least Frequently Used)算法统计热点 key。

```bash
# 扫描热点 key
redis-cli --hotkeys

# 输出示例
# Scanning the entire keyspace to find hot keys...
# Hot key 'product:999' found so far with counter 125342
# Hot key 'user:1001' found so far with counter 98234
```

**前提条件:** 需要启用 `maxmemory-policy` 为 `allkeys-lfu` 或 `volatile-lfu`。

```bash
# 修改配置
CONFIG SET maxmemory-policy allkeys-lfu
```

**优势:** 官方支持,性能开销小
**劣势:** 需要 Redis 4.0.3+,依赖 LFU 算法

---

##### 方法 4: 代理层统计

使用 Redis 代理(如 Codis、Twemproxy)统计热点 key。

**Codis 示例:**
Codis Dashboard 可以实时展示每个 key 的访问频率。

**优势:** 集中管理,适合大规模集群
**劣势:** 需要额外部署代理

---

##### 方法 5: 使用 Redis 的 Object Freq 命令

如果启用了 LFU 算法,可以查询 key 的访问频率。

```bash
# 查看 key 的访问频率(0-255)
OBJECT FREQ product:999
# 输出: (integer) 215
```

**用法:** 定期扫描所有 key,找出频率最高的。

```python
import redis

r = redis.Redis()
hot_keys = []

for key in r.scan_iter():
    freq = r.execute_command('OBJECT', 'FREQ', key)
    if freq > 200:
        hot_keys.append((key, freq))

# 排序
hot_keys.sort(key=lambda x: x[1], reverse=True)
print("Top 10 hot keys:", hot_keys[:10])
```

---

#### 4. 热 Key 解决方案

##### 方案 1: 本地缓存(推荐)

将热点数据缓存到应用服务器的本地内存中,减少 Redis 访问。

**实现方式:**

```go
// Go 示例: 使用 golang-lru
import (
    lru "github.com/hashicorp/golang-lru"
)

var localCache, _ = lru.New(1000)  // 缓存 1000 个 key

func GetProduct(productID string) (*Product, error) {
    // 1. 先查本地缓存
    if val, ok := localCache.Get(productID); ok {
        return val.(*Product), nil
    }

    // 2. 查 Redis
    val, err := redisClient.Get(ctx, "product:"+productID).Result()
    if err == redis.Nil {
        // 3. 查数据库
        product, err := db.GetProduct(productID)
        if err != nil {
            return nil, err
        }

        // 4. 写入 Redis
        redisClient.Set(ctx, "product:"+productID, product, 10*time.Minute)

        // 5. 写入本地缓存(短过期时间)
        localCache.AddEx(productID, product, 10*time.Second)

        return product, nil
    }

    // 解析并写入本地缓存
    var product Product
    json.Unmarshal([]byte(val), &product)
    localCache.AddEx(productID, &product, 10*time.Second)

    return &product, nil
}
```

**Java 示例(Caffeine):**

```java
// 创建本地缓存
Cache<String, Object> localCache = Caffeine.newBuilder()
    .maximumSize(1000)
    .expireAfterWrite(10, TimeUnit.SECONDS)
    .build();

public Product getProduct(String productId) {
    // 1. 查本地缓存
    Product product = (Product) localCache.getIfPresent(productId);
    if (product != null) {
        return product;
    }

    // 2. 查 Redis
    String key = "product:" + productId;
    String json = redisTemplate.opsForValue().get(key);

    if (json != null) {
        product = JSON.parseObject(json, Product.class);
        localCache.put(productId, product);  // 写入本地缓存
        return product;
    }

    // 3. 查数据库并回写缓存
    product = productMapper.selectById(productId);
    redisTemplate.opsForValue().set(key, JSON.toJSONString(product), 10, TimeUnit.MINUTES);
    localCache.put(productId, product);

    return product;
}
```

**优势:**
- 极大降低 Redis 压力(QPS 降低 90%+)
- 响应速度快(纳秒级)
- 简单易实现

**注意事项:**
- 本地缓存过期时间要短(5-30 秒),避免数据不一致
- 内存占用要控制,避免 OOM
- 适合读多写少的场景

---

##### 方案 2: 热 Key 拆分(多副本)

将热 key 拆分为多个副本,分散请求。

**实现方式:**

```go
// 原始方式
key := "product:999"

// 拆分为 10 个副本
replica := rand.Intn(10)  // 0-9
key := fmt.Sprintf("product:999:replica:%d", replica)
```

**完整示例:**

```go
func GetProductWithReplica(productID string) (*Product, error) {
    // 随机选择一个副本
    replica := rand.Intn(10)
    key := fmt.Sprintf("product:%s:replica:%d", productID, replica)

    val, err := redisClient.Get(ctx, key).Result()
    if err == redis.Nil {
        // 缓存未命中,查数据库
        product, err := db.GetProduct(productID)
        if err != nil {
            return nil, err
        }

        // 写入所有副本
        for i := 0; i < 10; i++ {
            replicaKey := fmt.Sprintf("product:%s:replica:%d", productID, i)
            redisClient.Set(ctx, replicaKey, product, 10*time.Minute)
        }

        return product, nil
    }

    var product Product
    json.Unmarshal([]byte(val), &product)
    return &product, nil
}
```

**优势:**
- 将单点压力分散到多个 key
- 集群模式下,多个副本可能分布在不同节点

**劣势:**
- 占用更多内存
- 更新时需要更新所有副本
- 短时间内可能有数据不一致

---

##### 方案 3: 使用 Redis 集群 + 本地缓存

结合 Redis Cluster 和本地缓存,双重保障。

**架构:**
```
客户端请求
   ↓
本地缓存(10s TTL)
   ↓ (miss)
Redis Cluster(多节点)
   ↓ (miss)
数据库
```

**优势:**
- Redis 层面已经分布式
- 本地缓存进一步降低压力

---

##### 方案 4: 限流降级

对热 key 访问进行限流,超过阈值直接降级。

```go
import "golang.org/x/time/rate"

// 为热 key 创建限流器
var rateLimiters sync.Map

func GetWithRateLimit(key string, maxQPS int) (string, error) {
    // 获取或创建限流器
    limiter, _ := rateLimiters.LoadOrStore(key, rate.NewLimiter(rate.Limit(maxQPS), maxQPS))

    // 尝试获取令牌
    if !limiter.(*rate.Limiter).Allow() {
        // 限流,返回降级数据
        return getDefaultValue(key), nil
    }

    // 正常访问 Redis
    return redisClient.Get(ctx, key).Result()
}
```

**优势:**
- 保护 Redis 不被打挂
- 提供降级方案

**劣势:**
- 部分用户体验下降

---

##### 方案 5: 使用 Proxy 做二级缓存

在 Redis 前加一层代理,代理层做缓存。

**架构:**
```
应用 → Proxy(缓存热 key) → Redis → DB
```

**开源方案:**
- **Twemproxy**: Twitter 开源的 Redis 代理
- **Codis**: 豌豆荚开源的 Redis 集群方案(自带热 key 检测)

---

### 二、大 Key 问题

#### 1. 什么是大 Key?

**定义:** 单个 key 占用内存过大或元素过多。

**判断标准:**
- **String 类型:** value 大于 10KB
- **List/Set/ZSet/Hash:** 元素数量超过 5000

**典型场景:**
- 存储大 JSON 对象(几十 KB)
- List 存储大量消息(几万条)
- Hash 存储大量字段(几千个)
- Set 存储大量用户 ID(几十万个)

---

#### 2. 大 Key 带来的问题

**性能问题:**
- **阻塞操作:** 读写大 key 耗时长,阻塞其他请求(Redis 单线程)
- **网络拥堵:** 大 key 传输占用大量带宽
- **内存碎片:** 频繁创建删除大 key 导致内存碎片

**稳定性问题:**
- **主从同步延迟:** 大 key 同步慢,主从延迟增加
- **过期删除阻塞:** 大 key 过期时,DEL 操作耗时长(几百毫秒)
- **内存占用过高:** 少数大 key 占用大量内存,影响其他数据

**实际案例:**
某社交平台,用户关注列表用 Set 存储,热门用户关注数达 50 万:
- DEL 操作耗时 2 秒,阻塞 Redis
- 主从同步延迟 10 秒+
- 集群 failover 时,大 key 迁移耗时过长

---

#### 3. 如何发现大 Key?

##### 方法 1: redis-cli --bigkeys(推荐)

Redis 自带工具,扫描每种数据类型中最大的 key。

```bash
# 扫描大 key
redis-cli --bigkeys

# 输出示例
# Biggest string found so far 'user:profile:1001' with 52341 bytes
# Biggest list   found so far 'message:list:999' with 23451 items
# Biggest hash   found so far 'order:detail:888' with 8934 fields
```

**优势:** 官方工具,简单易用
**劣势:** 只能找出最大的,无法自定义阈值

---

##### 方法 2: redis-cli --memkeys

扫描所有 key 的内存使用情况(需要 Redis 4.0+)。

```bash
# 扫描内存使用 TOP key
redis-cli --memkeys --memkeys-samples 10000

# 输出示例
# Key: user:profile:1001, Memory: 52341 bytes
# Key: message:list:999, Memory: 234567 bytes
```

---

##### 方法 3: MEMORY USAGE 命令

查询单个 key 的内存占用(Redis 4.0+)。

```bash
# 查询 key 的内存占用
MEMORY USAGE user:profile:1001
# 输出: (integer) 52341  (单位: 字节)
```

**批量扫描:**
```bash
redis-cli SCAN 0 COUNT 1000 | while read key; do
    size=$(redis-cli MEMORY USAGE "$key")
    if [ "$size" -gt 10240 ]; then  # 大于 10KB
        echo "$key: $size bytes"
    fi
done
```

---

##### 方法 4: DEBUG OBJECT 命令

查看 key 的详细信息。

```bash
DEBUG OBJECT user:profile:1001
# 输出:
# Value at:0x7f8e3c0a1234 refcount:1 encoding:raw serializedlength:52341 lru:1234567
```

**serializedlength** 表示序列化后的大小。

---

##### 方法 5: 监控工具

**RedisInsight:** Redis 官方 GUI 工具,可视化展示大 key。

**云厂商监控:**
- 阿里云 Redis
- AWS ElastiCache
- 腾讯云 Redis

都提供大 key 分析功能。

---

#### 4. 大 Key 解决方案

##### 方案 1: 拆分大 Key(推荐)

将大 key 拆分为多个小 key。

**示例 1: 拆分大 String**

```go
// 原始方式: 单个大 JSON(50KB)
key := "user:profile:1001"
value := `{"name":"...", "orders":[...], "addresses":[...]}` // 50KB

// 优化: 拆分为多个小 key
redisClient.Set(ctx, "user:1001:basic", `{"name":"..."}`, 0)     // 1KB
redisClient.Set(ctx, "user:1001:orders", `[...]`, 0)            // 30KB
redisClient.Set(ctx, "user:1001:addresses", `[...]`, 0)         // 10KB

// 按需查询
basic := redisClient.Get(ctx, "user:1001:basic").Val()
```

---

**示例 2: 拆分大 Hash**

```bash
# 原始方式: 单个 Hash 有 10 万个字段
HSET user:1001:followers uid_1 1
HSET user:1001:followers uid_2 1
# ... 10 万个字段

# 优化: 按 hash 分桶(100 个桶,每个桶 1000 个字段)
# 桶号 = uid % 100
HSET user:1001:followers:0 uid_100 1
HSET user:1001:followers:1 uid_101 1
HSET user:1001:followers:2 uid_102 1
```

**Go 实现:**
```go
func AddFollower(userID, followerID int64) error {
    bucket := followerID % 100
    key := fmt.Sprintf("user:%d:followers:%d", userID, bucket)
    return redisClient.HSet(ctx, key, strconv.FormatInt(followerID, 10), 1).Err()
}

func IsFollower(userID, followerID int64) (bool, error) {
    bucket := followerID % 100
    key := fmt.Sprintf("user:%d:followers:%d", userID, bucket)
    return redisClient.HExists(ctx, key, strconv.FormatInt(followerID, 10)).Result()
}
```

---

**示例 3: 拆分大 List**

```go
// 原始方式: 单个 List 存储 10 万条消息
key := "message:list:room_999"

// 优化: 按时间分片,每小时一个 List
func PushMessage(roomID string, msg string) error {
    hour := time.Now().Format("2006010215")  // 2024011512
    key := fmt.Sprintf("message:list:%s:%s", roomID, hour)
    return redisClient.LPush(ctx, key, msg).Err()
}

func GetMessages(roomID string, startTime, endTime time.Time) ([]string, error) {
    var messages []string

    // 遍历时间范围内的所有分片
    for t := startTime; t.Before(endTime); t = t.Add(time.Hour) {
        hour := t.Format("2006010215")
        key := fmt.Sprintf("message:list:%s:%s", roomID, hour)
        msgs, _ := redisClient.LRange(ctx, key, 0, -1).Result()
        messages = append(messages, msgs...)
    }

    return messages, nil
}
```

---

##### 方案 2: 压缩 Value

对大 value 进行压缩,减少内存占用。

```go
import (
    "bytes"
    "compress/gzip"
    "io/ioutil"
)

// 压缩
func CompressValue(data string) ([]byte, error) {
    var buf bytes.Buffer
    gz := gzip.NewWriter(&buf)
    _, err := gz.Write([]byte(data))
    if err != nil {
        return nil, err
    }
    gz.Close()
    return buf.Bytes(), nil
}

// 解压
func DecompressValue(data []byte) (string, error) {
    reader, err := gzip.NewReader(bytes.NewReader(data))
    if err != nil {
        return "", err
    }
    defer reader.Close()

    result, err := ioutil.ReadAll(reader)
    return string(result), err
}

// 使用
func SetCompressed(key string, value string) error {
    compressed, err := CompressValue(value)
    if err != nil {
        return err
    }
    return redisClient.Set(ctx, key, compressed, 0).Err()
}
```

**压缩效果:**
- 文本数据: 压缩率 60-80%
- JSON 数据: 压缩率 70-90%

**注意:** 压缩解压有 CPU 开销,适合大 value(>10KB)。

---

##### 方案 3: 使用合适的数据结构

选择更高效的数据结构。

**示例: 存储用户标签**

```bash
# 不推荐: 使用 String 存储 JSON 数组
SET user:1001:tags '["tag1","tag2","tag3",...,"tag1000"]'  # 10KB

# 推荐: 使用 Set
SADD user:1001:tags tag1 tag2 tag3 ... tag1000  # 5KB

# 查询
SISMEMBER user:1001:tags tag1
```

**优势:**
- Set 存储效率更高
- 支持集合操作(交集、并集)

---

##### 方案 4: 设置过期时间

为大 key 设置合理的过期时间,自动清理。

```go
// 设置 1 小时过期
redisClient.Set(ctx, "large:key", value, 1*time.Hour)

// 为已存在的 key 设置过期时间
redisClient.Expire(ctx, "large:key", 1*time.Hour)
```

**注意:** 大 key 过期删除时会阻塞,见下一方案。

---

##### 方案 5: 异步删除(UNLINK)

使用 `UNLINK` 代替 `DEL`,异步删除大 key。

```bash
# 同步删除(阻塞)
DEL large:key  # 可能耗时几百毫秒

# 异步删除(非阻塞)
UNLINK large:key  # 立即返回,后台线程删除
```

**Go 示例:**
```go
// 使用 UNLINK 删除
redisClient.Unlink(ctx, "large:key")
```

**原理:**
- `UNLINK` 会立即解除 key 的引用
- 实际内存回收由后台线程(Bio)完成
- 不会阻塞主线程

**建议:** Redis 4.0+ 全部使用 `UNLINK` 代替 `DEL`。

---

##### 方案 6: 懒删除(Lazy Free)

开启 Redis 的懒删除机制。

```bash
# 配置文件或运行时配置
CONFIG SET lazyfree-lazy-eviction yes      # 内存淘汰时懒删除
CONFIG SET lazyfree-lazy-expire yes        # 过期删除时懒删除
CONFIG SET lazyfree-lazy-server-del yes    # 内部删除时懒删除
```

**效果:** 大 key 过期时不再阻塞主线程。

---

##### 方案 7: 渐进式删除(HSCAN + HDEL)

对于大 Hash/Set/ZSet,使用渐进式删除。

**删除大 Hash:**
```go
func DeleteLargeHash(key string) error {
    cursor := uint64(0)
    count := int64(100)  // 每次删除 100 个字段

    for {
        // 扫描部分字段
        keys, newCursor, err := redisClient.HScan(ctx, key, cursor, "*", count).Result()
        if err != nil {
            return err
        }

        // 删除这部分字段
        if len(keys) > 0 {
            redisClient.HDel(ctx, key, keys...)
        }

        cursor = newCursor
        if cursor == 0 {
            break
        }

        // 间隔 10ms,避免阻塞
        time.Sleep(10 * time.Millisecond)
    }

    // 最后删除 key
    return redisClient.Del(ctx, key).Err()
}
```

**删除大 Set:**
```go
func DeleteLargeSet(key string) error {
    for {
        // 每次弹出 100 个元素
        members, err := redisClient.SPopN(ctx, key, 100).Result()
        if err == redis.Nil || len(members) == 0 {
            break
        }
        time.Sleep(10 * time.Millisecond)
    }

    return redisClient.Del(ctx, key).Err()
}
```

---

##### 方案 8: 迁移到其他存储

对于超大数据,考虑迁移到更合适的存储。

**场景 1: 大文件存储**
- 原方案: Redis 存储图片(几 MB)
- 优化: 迁移到对象存储(OSS/S3)

**场景 2: 海量消息列表**
- 原方案: Redis List 存储百万级消息
- 优化: 迁移到 Kafka/RocketMQ

**场景 3: 大数据集分析**
- 原方案: Redis Set 存储千万级用户 ID
- 优化: 迁移到 ClickHouse/ES

---

### 三、监控和预防

#### 1. 日常监控

**监控指标:**
- **慢查询:** `SLOWLOG GET 10`
- **内存使用:** `INFO MEMORY`
- **QPS:** `INFO STATS`
- **网络流量:** 监控网卡流量

**告警规则:**
- 慢查询 > 10ms
- 单 key 内存 > 10MB
- 单节点 QPS > 10 万
- 网络流量 > 80%

---

#### 2. 最佳实践

**设计阶段:**
1. 评估 key 大小,避免超过 10KB
2. 评估访问频率,预判热 key
3. 设计合理的 key 命名规范
4. 为 key 设置合理的过期时间

**开发阶段:**
1. 避免使用 `KEYS *`、`HGETALL`、`SMEMBERS` 等全量操作
2. 使用 `SCAN`、`HSCAN`、`SSCAN` 等渐进式命令
3. 批量操作使用 Pipeline
4. 大 key 删除使用 `UNLINK`

**运维阶段:**
1. 定期检查大 key 和热 key
2. 开启懒删除机制
3. 合理配置 maxmemory 和淘汰策略
4. 做好容量规划,预留 30% buffer

---

### 四、实战案例

#### 案例 1: 秒杀活动热 key 优化

**背景:**
- 秒杀商品详情 key 被 100 万 QPS 访问
- Redis 单节点 CPU 100%
- 大量请求超时

**优化方案:**

```go
// 1. 本地缓存 + 热 key 拆分
var localCache, _ = lru.New(1000)

func GetSeckillProduct(productID string) (*Product, error) {
    // 本地缓存(5 秒)
    if val, ok := localCache.Get(productID); ok {
        return val.(*Product), nil
    }

    // 热 key 拆分(10 个副本)
    replica := rand.Intn(10)
    key := fmt.Sprintf("seckill:product:%s:replica:%d", productID, replica)

    val, err := redisClient.Get(ctx, key).Result()
    if err == redis.Nil {
        // 查数据库
        product, _ := db.GetProduct(productID)

        // 写入所有副本
        for i := 0; i < 10; i++ {
            replicaKey := fmt.Sprintf("seckill:product:%s:replica:%d", productID, i)
            redisClient.Set(ctx, replicaKey, product, 10*time.Minute)
        }

        localCache.AddEx(productID, product, 5*time.Second)
        return product, nil
    }

    var product Product
    json.Unmarshal([]byte(val), &product)
    localCache.AddEx(productID, &product, 5*time.Second)

    return &product, nil
}
```

**效果:**
- Redis QPS: 100 万 → 5 万(降低 95%)
- 响应时间: 50ms → 2ms
- CPU 使用率: 100% → 20%

---

#### 案例 2: 用户关注列表大 key 优化

**背景:**
- 热门用户关注列表用 Set 存储,达到 50 万
- DEL 操作耗时 2 秒,阻塞 Redis

**优化方案:**

```go
// 1. 拆分为 100 个桶
func AddFollower(userID, followerID int64) error {
    bucket := followerID % 100
    key := fmt.Sprintf("user:%d:followers:%d", userID, bucket)
    return redisClient.SAdd(ctx, key, followerID).Err()
}

// 2. 判断是否关注
func IsFollower(userID, followerID int64) (bool, error) {
    bucket := followerID % 100
    key := fmt.Sprintf("user:%d:followers:%d", userID, bucket)
    return redisClient.SIsMember(ctx, key, followerID).Result()
}

// 3. 获取关注总数
func GetFollowerCount(userID int64) (int64, error) {
    var total int64
    for i := 0; i < 100; i++ {
        key := fmt.Sprintf("user:%d:followers:%d", userID, i)
        count, _ := redisClient.SCard(ctx, key).Result()
        total += count
    }
    return total, nil
}

// 4. 删除时使用 UNLINK
func DeleteFollowers(userID int64) error {
    keys := make([]string, 100)
    for i := 0; i < 100; i++ {
        keys[i] = fmt.Sprintf("user:%d:followers:%d", userID, i)
    }
    return redisClient.Unlink(ctx, keys...).Err()
}
```

**效果:**
- 单个 key 大小: 50 万 → 5000(降低 99%)
- 删除耗时: 2000ms → 5ms
- 主从同步延迟: 10s → 0.5s

---

### 五、总结对比

| 问题类型 | 核心影响 | 检测方法 | 解决方案 |
|---------|---------|---------|---------|
| **热 Key** | 单点压力大、流量倾斜 | `--hotkeys`、客户端统计 | 本地缓存、多副本、限流 |
| **大 Key** | 操作阻塞、内存占用高 | `--bigkeys`、`MEMORY USAGE` | 拆分 key、压缩、异步删除 |

---

## 面试回答模板

**回答框架:**

"Redis 的热 key 和大 key 问题,我分别从定义、影响、检测和解决方案来回答:

**一、热 Key 问题:**

**定义:** 少数 key 被极高频率访问,远超其他 key。比如秒杀商品、热门微博。

**影响:**
- 单机负载过高,CPU 打满
- 集群流量倾斜,热 key 所在节点压力大
- 可能导致缓存击穿和服务雪崩

**检测方法:**
- 使用 `redis-cli --hotkeys` 命令(需要开启 LFU)
- 客户端统计访问频率
- 使用代理层(如 Codis)监控

**解决方案:**

1. **本地缓存(推荐):** 在应用服务器内存缓存热点数据,TTL 设置 5-30 秒,可降低 90%+ 的 Redis 访问

2. **热 key 拆分:** 将热 key 复制多个副本(如 key_0 到 key_9),随机访问,分散压力

3. **限流降级:** 对热 key 访问进行限流保护,超过阈值返回降级数据

---

**二、大 Key 问题:**

**定义:** 单个 key 占用内存过大,比如 String 超过 10KB,或 Hash/Set 元素超过 5000 个。

**影响:**
- 读写操作耗时长,阻塞其他请求(Redis 单线程)
- 删除/过期时阻塞严重,可能耗时几秒
- 主从同步延迟增加
- 内存碎片和网络拥堵

**检测方法:**
- `redis-cli --bigkeys` 扫描最大的 key
- `MEMORY USAGE key` 查看单个 key 内存占用
- 使用监控工具(RedisInsight、云厂商监控)

**解决方案:**

1. **拆分 key(推荐):**
   - String: 将大 JSON 拆分为多个小 key
   - Hash: 按字段 hash 值分桶,如 100 个桶,每桶 1000 个字段
   - List: 按时间分片,如每小时一个 List

2. **压缩 value:** 使用 gzip 压缩大 value,可减少 60-90% 内存

3. **异步删除:** 使用 `UNLINK` 代替 `DEL`,后台异步删除,不阻塞主线程

4. **开启懒删除:** 配置 `lazyfree-lazy-expire yes`,过期删除时不阻塞

在我之前的项目中,遇到过秒杀热 key 问题,通过本地缓存 + 多副本方案,将 Redis QPS 从 100 万降到 5 万;还优化过用户关注列表大 key,通过分桶方案将删除耗时从 2 秒降到 5 毫秒。"
