---
title: 如何进行数据库的分库分表设计?
tags:
  - 性能优化
status: robot
class: 性能优化
slug: database-sharding-partitioning-design
ref:
---

## 核心要点

**分库分表时机**:单表数据量>1000万或单库QPS>1000时考虑拆分
**分片策略**:范围分片(range)、哈希分片(hash)、地理分片(geo)各有适用场景
**分片键选择**:需兼顾数据均衡、查询效率、扩容难度三者平衡
**跨分片问题**:分布式事务、全局ID、跨分片查询、数据迁移是四大核心挑战
**渐进式演进**:垂直拆分→读写分离→分库→分表→分库分表,循序渐进

---

## 详细回答

### 一、为什么需要分库分表?

#### 1. 单库单表的性能瓶颈

| 维度 | 瓶颈表现 | 临界值 | 影响 |
|------|---------|--------|------|
| **存储容量** | 单表数据量过大 | 1000万-3000万行 | B+树层级增加,索引效率下降 |
| **并发性能** | 单库连接数不足 | QPS > 1000 | 大量请求排队等待 |
| **锁竞争** | 行锁、表锁争抢 | 高并发写入 | 事务等待时间增加 |
| **缓冲池** | InnoDB Buffer Pool 不足 | 热数据无法全部缓存 | 频繁磁盘 I/O |
| **主从延迟** | Binlog 同步滞后 | 写入TPS过高 | 从库数据不一致 |

**实际案例:**
- 订单表 3000 万行,单次 `SELECT * WHERE user_id=?` 从 50ms 劣化到 500ms
- 用户表单库 QPS 达到 1500,连接池耗尽导致服务不可用

#### 2. 分库分表的收益

- **容量扩展**:从单表 3000 万→256 个分表,每表 12 万行,查询性能恢复到 10ms
- **并发提升**:从单库 1000 QPS→16 个分库,总 QPS 可达 16000
- **故障隔离**:单个分片故障只影响 1/N 的数据,不会全站挂掉

---

### 二、分库分表的四种拆分策略

#### 1. 垂直拆分(Vertical Sharding)—— 按业务模块拆分

**核心思想:** 将不同业务的表拆分到不同的数据库。

**示例:**
```
原单库:
┌─────────────────────┐
│  monolith_db        │
│  - users            │
│  - orders           │
│  - products         │
│  - payments         │
│  - logistics        │
└─────────────────────┘

垂直拆分后:
┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│  user_db     │  │  order_db    │  │  product_db  │
│  - users     │  │  - orders    │  │  - products  │
│  - profiles  │  │  - payments  │  │  - inventory │
└──────────────┘  └──────────────┘  └──────────────┘
```

**优点:**
- **实施简单**,符合微服务架构理念
- **资源隔离**,核心业务独享数据库资源
- **故障隔离**,订单库挂了不影响用户登录

**缺点:**
- 无法解决单表数据量过大的问题
- 跨库 JOIN 困难,需要应用层聚合

**适用场景:** 微服务改造的第一步,业务解耦

---

#### 2. 水平拆分(Horizontal Sharding)—— 按数据行拆分

**核心思想:** 同一张表的数据按照某个规则拆分到多个库/表。

##### 2.1 哈希分片(Hash Sharding)

**原理:** 对分片键计算哈希值,取模确定分片位置。

```go
// 用户表按 user_id 哈希分库分表
func getShardIndex(userID int64, shardCount int) int {
    return int(userID % int64(shardCount))
}

// 示例:user_id=12345,分 256 个库
shardIndex := getShardIndex(12345, 256)  // 结果:105
dbName := fmt.Sprintf("user_db_%03d", shardIndex)  // user_db_105
tableName := fmt.Sprintf("users_%03d", shardIndex) // users_105
```

**优点:**
- **数据分布均匀**,避免热点
- **查询简单**,根据分片键直接定位
- **扩容相对容易**(一致性哈希可减少迁移量)

**缺点:**
- **范围查询困难**:查询 `user_id BETWEEN 1000 AND 2000` 需要扫描所有分片
- **扩容需要数据迁移**:从 256 个分片扩容到 512 个,需要重新计算哈希

**适用场景:**
- 高并发写入场景(如用户注册、订单创建)
- 以单条记录查询为主(如根据 user_id 查用户信息)

##### 2.2 范围分片(Range Sharding)

**原理:** 按数据范围划分分片。

```sql
-- 订单表按时间范围分表
orders_202401  -- 2024年1月订单
orders_202402  -- 2024年2月订单
orders_202403  -- 2024年3月订单

-- 或按 ID 范围分表
orders_0000  -- order_id: 0-9999999
orders_0001  -- order_id: 10000000-19999999
orders_0002  -- order_id: 20000000-29999999
```

**优点:**
- **范围查询高效**:查询某月订单直接访问对应分表
- **扩容无需迁移**:新增分片即可(如新增 2024年4月分表)
- **冷热数据分离**:历史订单归档到冷存储

**缺点:**
- **数据分布不均**:最新月份分表写入压力大
- **需要预估数据增长**:分片边界设计不当会导致单分片过大

**适用场景:**
- 时序数据(日志、订单、消息记录)
- 冷热数据分离(近期数据热,历史数据冷)

##### 2.3 地理分片(Geo Sharding)

**原理:** 按地理位置划分分片。

```
user_db_bj  -- 北京用户
user_db_sh  -- 上海用户
user_db_gz  -- 广州用户
```

**优点:**
- **就近访问**,降低网络延迟
- **符合数据合规要求**(如中国用户数据存储在中国境内)

**缺点:**
- 数据分布极不均匀(北京用户可能是广州的 10 倍)

**适用场景:** 跨国/跨地区业务,数据本地化要求

---

### 三、分片键(Sharding Key)的选择

分片键的选择直接决定分库分表的效果,需要综合考虑以下因素:

#### 1. 分片键选择的三大原则

| 原则 | 说明 | 示例 |
|------|------|------|
| **数据均衡** | 避免数据倾斜,防止单分片过大 | ✅ user_id(均匀) vs ❌ 城市(北京用户占50%) |
| **查询效率** | 90%以上的查询能通过分片键定位 | ✅ 订单表用 order_id vs ❌ 用户表用 created_at |
| **扩容成本** | 扩容时需要迁移的数据量 | ✅ 一致性哈希 vs ❌ 简单取模 |

#### 2. 常见业务场景的分片键选择

**用户表(users):**
- **最佳选择:** `user_id`(哈希分片)
- **原因:** 用户 ID 天然均匀,且 90% 查询都是 `WHERE user_id=?`

**订单表(orders):**
- **方案 A:** 按 `order_id` 哈希分片 → 适合查询单个订单
- **方案 B:** 按 `user_id` 哈希分片 → 适合查询用户的所有订单
- **方案 C:** 按 `created_at` 范围分片 → 适合统计分析、数据归档

**建议:** 多维度分片(双写)
```
order_db_by_order_id_128  -- 按 order_id 哈希,快速查单个订单
order_db_by_user_id_128   -- 按 user_id 哈希,快速查用户订单列表
```

**日志表(logs):**
- **最佳选择:** `created_at`(范围分片,按天/周/月)
- **原因:** 日志查询都是时间范围查询,且历史日志可归档删除

#### 3. 避免的分片键选择

| 错误选择 | 问题 | 后果 |
|---------|------|------|
| **状态字段**(status) | 数据极度不均匀 | 已完成订单占 99%,待支付订单占 1% |
| **城市/地区** | 一线城市用户远多于小城市 | 北京分片 1000 万用户,拉萨分片 1 万用户 |
| **业务类型** | 主营业务占 90% | 某个分片超载,其他分片空闲 |
| **UUID前缀** | 随机性不足 | UUID v1 时间戳导致热点 |

---

### 四、跨分片问题的解决方案

#### 1. 全局唯一 ID 生成

**问题:** 分库分表后,数据库自增 ID 会重复。

**解决方案:**

##### 方案 A:Snowflake 算法(Twitter 雪花算法)

```go
// 64位 ID 组成:1位符号 + 41位时间戳 + 10位机器ID + 12位序列号
// 0 - 00000000000000000000000000000000000000000 - 0000000000 - 000000000000
// 符号  时间戳(毫秒,69年)                           机器ID      序列号(4096/ms)

type Snowflake struct {
    mu        sync.Mutex
    timestamp int64
    machineID int64
    sequence  int64
}

func (s *Snowflake) NextID() int64 {
    s.mu.Lock()
    defer s.mu.Unlock()

    now := time.Now().UnixMilli()
    if now == s.timestamp {
        s.sequence = (s.sequence + 1) & 4095  // 12位序列号,最大4095
        if s.sequence == 0 {
            // 序列号溢出,等待下一毫秒
            for now <= s.timestamp {
                now = time.Now().UnixMilli()
            }
        }
    } else {
        s.sequence = 0
    }
    s.timestamp = now

    // 组装 ID:时间戳(41位) + 机器ID(10位) + 序列号(12位)
    id := ((now - 1640995200000) << 22) | (s.machineID << 12) | s.sequence
    return id
}
```

**优点:**
- 性能高(单机每毫秒 4096 个 ID)
- 趋势递增,有利于数据库索引
- 无需依赖外部服务

**缺点:**
- 依赖系统时钟(时钟回拨会导致 ID 重复)
- 机器 ID 需要分配和管理

##### 方案 B:数据库号段模式

```sql
-- ID 分配表
CREATE TABLE id_generator (
    biz_type VARCHAR(32) PRIMARY KEY,
    max_id BIGINT NOT NULL,
    step INT NOT NULL DEFAULT 1000,
    updated_at TIMESTAMP
);

-- 应用启动时批量获取 ID 号段
UPDATE id_generator
SET max_id = max_id + step
WHERE biz_type = 'order'
RETURNING max_id, step;
-- 假设返回 max_id=5001000, step=1000
-- 应用可使用 ID: 5000001 ~ 5001000
```

**优点:**
- 实现简单,无需时钟同步
- 性能高(批量获取,减少数据库交互)

**缺点:**
- 依赖中心数据库(单点故障)
- ID 不是严格递增(多个应用同时分配号段)

##### 方案 C:Redis INCR

```bash
# 使用 Redis 自增
INCR order_id_generator
# 返回:12345678
```

**优点:** 实现简单,性能极高

**缺点:** 依赖 Redis,需要考虑持久化和高可用

**推荐方案:** **Snowflake(本地生成) + 号段模式(兜底)**

---

#### 2. 分布式事务

**问题:** 一个业务操作涉及多个分片,如何保证原子性?

**示例场景:** 用户下单,需要:
1. 扣减库存(product_db)
2. 创建订单(order_db)
3. 扣减余额(user_db)

##### 方案 A:两阶段提交(2PC)

```
协调者                参与者1(product_db)    参与者2(order_db)    参与者3(user_db)
  |                           |                    |                    |
  |------- Prepare --------->|                    |                    |
  |------- Prepare ------------------------>|                    |
  |------- Prepare ------------------------------------>|
  |                           |                    |                    |
  |<------ Yes/No ----------|                    |                    |
  |<------ Yes/No -------------------------|                    |
  |<------ Yes/No --------------------------------------|
  |                           |                    |                    |
  |------- Commit ---------->|                    |                    |
  |------- Commit ----------------------->|                    |
  |------- Commit ----------------------------------->|
```

**优点:** 强一致性

**缺点:**
- 性能差(两次网络交互)
- 单点故障(协调者挂掉导致参与者阻塞)
- **不推荐在高并发场景使用**

##### 方案 B:TCC(Try-Confirm-Cancel)

```go
// Try 阶段:预留资源
func TryDeductInventory(productID int64, count int) (reservationID string, err error) {
    // 冻结库存,不实际扣减
    db.Exec("UPDATE inventory SET frozen = frozen + ? WHERE product_id = ?", count, productID)
    return uuid.New().String(), nil
}

// Confirm 阶段:确认扣减
func ConfirmDeductInventory(reservationID string) error {
    db.Exec("UPDATE inventory SET stock = stock - frozen, frozen = 0 WHERE reservation_id = ?", reservationID)
    return nil
}

// Cancel 阶段:回滚
func CancelDeductInventory(reservationID string) error {
    db.Exec("UPDATE inventory SET frozen = 0 WHERE reservation_id = ?", reservationID)
    return nil
}
```

**优点:**
- 无阻塞,性能好
- 业务可控性强

**缺点:**
- 实现复杂,需要为每个操作实现 Try/Confirm/Cancel
- 需要考虑幂等性和超时补偿

##### 方案 C:本地消息表 + 最终一致性(推荐)

```go
// 1. 在订单库的本地事务中:创建订单 + 插入消息表
tx.Exec("INSERT INTO orders (...) VALUES (...)")
tx.Exec("INSERT INTO outbox_messages (event_type, payload, status) VALUES ('order_created', ?, 'pending')", orderJSON)
tx.Commit()

// 2. 后台任务扫描消息表,发送消息到 MQ
for msg := range pendingMessages {
    mq.Publish("order.created", msg.Payload)
    db.Exec("UPDATE outbox_messages SET status = 'sent' WHERE id = ?", msg.ID)
}

// 3. 其他服务消费消息,执行后续操作
// - 库存服务监听 order.created,扣减库存
// - 支付服务监听 order.created,扣减余额
```

**优点:**
- **性能好**,无需同步等待
- **高可用**,利用 MQ 的重试和持久化
- **业务解耦**,易于扩展

**缺点:**
- 最终一致性,非实时一致
- 需要处理消息重复和乱序

**推荐方案:** **本地消息表 + MQ**(适合 99% 场景)

---

#### 3. 跨分片查询

**问题:** 如何查询分布在多个分片的数据?

**场景 1:查询用户的所有订单(订单表按 order_id 分片)**

```sql
-- 原SQL
SELECT * FROM orders WHERE user_id = 12345 ORDER BY created_at DESC LIMIT 10;

-- 分片后需要:
-- 1. 查询所有256个分片(并行)
-- 2. 应用层归并排序
-- 3. 取 TOP 10
```

**优化方案:**

**方案 A:冗余分片(空间换时间)**
```
order_db_by_order_id_128  -- 按 order_id 哈希,主表
order_db_by_user_id_128   -- 按 user_id 哈希,从表(冗余)
```

写入时双写,查询时按需选择分片维度。

**方案 B:搜索引擎(Elasticsearch)**

将订单数据同步到 ES,复杂查询走搜索引擎:
```json
GET /orders/_search
{
  "query": {
    "bool": {
      "must": [
        {"term": {"user_id": 12345}},
        {"range": {"created_at": {"gte": "2024-01-01"}}}
      ]
    }
  },
  "sort": [{"created_at": "desc"}],
  "size": 10
}
```

**方案 C:数据仓库(OLAP)**

实时性要求不高的统计查询,通过 ETL 同步到 ClickHouse/Doris。

**推荐方案:**
- **高频精确查询:** 冗余分片(双写)
- **复杂条件查询:** Elasticsearch
- **大数据分析:** ClickHouse/Doris

---

#### 4. 数据迁移与扩容

**问题:** 业务增长,需要从 128 个分片扩容到 256 个分片。

##### 扩容步骤(双写方案)

```
阶段 1:双写新旧分片(1-2周)
┌────────┐
│  应用   │
└───┬────┘
    ├─────> 旧分片(128个)  [读写]
    └─────> 新分片(256个)  [只写]

阶段 2:数据迁移 + 数据校验(1-2周)
后台任务将旧分片数据迁移到新分片,并校验一致性

阶段 3:切换读流量(1天)
┌────────┐
│  应用   │
└───┬────┘
    ├─────> 旧分片(128个)  [只写]
    └─────> 新分片(256个)  [读写]

阶段 4:停止双写(1天)
┌────────┐
│  应用   │
└───┬────┘
    └─────> 新分片(256个)  [读写]
```

##### 迁移工具

```go
// 数据迁移脚本
func migrateData(oldShardCount, newShardCount int) {
    for oldShard := 0; oldShard < oldShardCount; oldShard++ {
        oldDB := getDB(oldShard)
        rows := oldDB.Query("SELECT * FROM users")

        for rows.Next() {
            var user User
            rows.Scan(&user.ID, &user.Name, ...)

            // 重新计算新分片位置
            newShard := user.ID % newShardCount
            newDB := getDB(newShard)
            newDB.Exec("INSERT INTO users (...) VALUES (...)", user)
        }
    }
}
```

**注意事项:**
- **灰度迁移**:先迁移 10% 流量验证,再全量迁移
- **数据校验**:迁移后对比新旧分片数据一致性
- **回滚方案**:保留旧分片数据,发现问题立即回滚

---

### 五、分库分表中间件

手动管理分片逻辑复杂,推荐使用成熟的中间件:

#### 1. ShardingSphere(Apache 开源)

```yaml
# sharding-jdbc 配置示例
spring:
  shardingsphere:
    datasource:
      names: ds0,ds1,ds2,ds3  # 4 个数据库
      ds0:
        url: jdbc:mysql://localhost:3306/order_db_0
      ds1:
        url: jdbc:mysql://localhost:3306/order_db_1
      # ...

    rules:
      sharding:
        tables:
          orders:
            actual-data-nodes: ds${0..3}.orders_${0..15}  # 4库 x 16表
            database-strategy:
              standard:
                sharding-column: user_id
                sharding-algorithm-name: db-hash-mod
            table-strategy:
              standard:
                sharding-column: order_id
                sharding-algorithm-name: table-hash-mod

        sharding-algorithms:
          db-hash-mod:
            type: HASH_MOD
            props:
              sharding-count: 4
          table-hash-mod:
            type: HASH_MOD
            props:
              sharding-count: 16
```

**优点:**
- 应用无需修改 SQL,透明分片
- 支持读写分离、分布式事务
- 社区活跃,文档完善

#### 2. Vitess(YouTube 开源)

```sql
-- Vitess VSchema 配置
{
  "sharded": true,
  "vindexes": {
    "hash": {
      "type": "hash"
    }
  },
  "tables": {
    "orders": {
      "column_vindexes": [
        {
          "column": "user_id",
          "name": "hash"
        }
      ]
    }
  }
}
```

**优点:**
- 久经考验(YouTube 数十亿用户)
- 支持在线扩容(无需停机)
- 兼容 MySQL 协议

#### 3. 对比选择

| 特性 | ShardingSphere | Vitess | 自研 |
|------|---------------|--------|------|
| **实施成本** | 低(JDBC集成) | 中(独立代理) | 高 |
| **性能损耗** | 极小(<5%) | 小(10-15%) | 无 |
| **功能完整性** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 按需定制 |
| **社区支持** | 中文社区活跃 | 英文社区 | 无 |
| **推荐场景** | 中小型项目 | 大型/超大型项目 | 特殊需求 |

**推荐:** 优先选择 **ShardingSphere**,简单易用且满足大多数需求。

---

### 六、分库分表实施路线图

```
阶段 0:单库单表(初创期)
数据量 < 100万,QPS < 100
├─ 无需分库分表
└─ 做好索引优化、SQL优化

阶段 1:垂直拆分(成长期)
数据量 100万-1000万,QPS 100-500
├─ 按业务模块拆分数据库
├─ 用户库、订单库、商品库独立
└─ 引入读写分离

阶段 2:水平分表(扩张期)
单表数据量 > 1000万,QPS 500-1000
├─ 对大表进行分表(如 orders_0 ~ orders_127)
├─ 单库多表,降低单表压力
└─ 引入分布式 ID 生成器

阶段 3:水平分库分表(成熟期)
单库 QPS > 1000,总数据量 > 1亿
├─ 分库分表(如 8 个库 x 16 个表)
├─ 引入 ShardingSphere 等中间件
└─ 解决分布式事务、跨分片查询

阶段 4:异构存储(规模化期)
数据量 > 10亿,QPS > 10000
├─ 冷热数据分离(历史订单归档到对象存储)
├─ 引入 ES、ClickHouse 等异构存储
└─ 读写分离 + 缓存 + 搜索引擎 + 数据仓库
```

**关键原则:** 能不分就不分,能晚分就晚分,分了就要分对。

---

### 七、最佳实践与踩坑指南

#### 1. 设计阶段

✅ **DO:**
- 在业务初期就设计好分片键(即使暂时不分片)
- 使用全局唯一 ID(Snowflake)替代自增 ID
- 预留分片字段(如 `shard_id` 字段)

❌ **DON'T:**
- 等到数据库已经卡死才考虑分库分表
- 频繁修改分片策略(每次修改都需要数据迁移)
- 分片键选择不当(如用城市、状态等不均匀字段)

#### 2. 实施阶段

✅ **DO:**
- 灰度发布(先 10% 流量,再 50%,最后 100%)
- 保留回滚方案(双写期间保留旧分片)
- 数据校验(迁移后对比新旧数据一致性)

❌ **DON'T:**
- 一次性全量迁移(风险巨大)
- 迁移期间不校验数据
- 删除旧分片数据(至少保留 1 个月)

#### 3. 运维阶段

✅ **DO:**
- 监控各分片的数据量、QPS(避免数据倾斜)
- 定期归档冷数据(如 3 个月前的订单)
- 建立数据迁移 SOP(标准操作流程)

❌ **DON'T:**
- 手动执行 DDL(用工具如 gh-ost、pt-online-schema-change)
- 跨分片 JOIN(改为应用层聚合)
- 分布式事务滥用(优先使用最终一致性)

---

### 八、经典案例分析

#### 案例 1:淘宝订单表分库分表

**规模:** 日订单量 5000 万,总订单量 1000 亿+

**分片策略:**
- **主表:** 按 `order_id` 哈希分 1024 个库,每库 1024 个表(共 100 万分片)
- **买家维度表:** 按 `buyer_id` 哈希分片,方便查询"我的订单"
- **卖家维度表:** 按 `seller_id` 哈希分片,方便查询"店铺订单"

**关键技术:**
- 三份数据冗余(空间换时间)
- 使用分布式 ID(TDDL 序列号)
- 历史订单归档到 ODPS(离线存储)

#### 案例 2:微信红包分库分表

**规模:** 春节红包峰值 40 万笔/秒

**分片策略:**
- 按 `red_packet_id` 哈希分 256 个库
- 使用 Redis 预扣库存,异步写入 MySQL
- 红包详情表(大)和红包列表表(小)分开存储

**关键技术:**
- 削峰填谷(Redis 队列缓冲)
- 最终一致性(先返回成功,后台异步入库)
- 热点数据全缓存(当日红包全在 Redis)

---

## 总结

分库分表是解决数据库性能瓶颈的终极手段,但也是**最复杂的优化方式**。实施时需要把握以下核心要点:

1. **分片时机:** 单表 > 1000 万或单库 QPS > 1000 时考虑
2. **分片策略:** 哈希分片(均匀)vs 范围分片(范围查询),按业务选择
3. **分片键:** 兼顾数据均衡、查询效率、扩容成本三者平衡
4. **跨分片问题:** 全局 ID(Snowflake)、分布式事务(本地消息表)、跨分片查询(冗余/ES)
5. **渐进演进:** 垂直拆分→读写分离→分库→分表→分库分表,循序渐进

**核心理念:** 能不分就不分,能晚分就晚分,分了就要分对。分库分表不是银弹,需要结合缓存、读写分离、索引优化、SQL 优化等多种手段,形成完整的数据库性能优化体系。
