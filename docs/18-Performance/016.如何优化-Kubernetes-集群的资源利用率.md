---
title: 如何优化 Kubernetes 集群的资源利用率?
tags:
  - 性能优化
status: robot
class: 性能优化
slug: optimize-kubernetes-cluster-resource-utilization
ref:
---

## 核心要点

**资源配置优化**:合理设置 requests/limits,避免过度预留或资源饥饿
**弹性伸缩**:HPA/VPA/CA 三层自动扩缩容,按需分配资源
**调度优化**:亲和性、污点容忍、优先级抢占,提升装箱率
**资源回收**:清理僵尸资源、使用 LimitRange/ResourceQuota 限额管控
**监控分析**:基于 Prometheus+Grafana 持续监控,数据驱动优化决策

---

## 详细回答

### 一、资源请求与限制的合理配置

Kubernetes 的资源利用率优化首先要从最基础的 **资源请求(requests)和限制(limits)** 配置入手。这是 K8s 调度器决定 Pod 分配的核心依据。

#### 1. Requests vs Limits 的区别

```yaml
resources:
  requests:    # 保证资源:调度依据,QoS 分类基础
    cpu: "500m"
    memory: "512Mi"
  limits:      # 限制上限:防止资源超用,触发 OOMKilled/CPU 限流
    cpu: "1000m"
    memory: "1Gi"
```

**关键原则:**
- **requests 应基于实际使用的 P50-P90 值设置**,过高会浪费资源(资源预留但未使用),过低会导致节点过载
- **limits 应设置为峰值的 1.2-1.5 倍**,给予一定缓冲,但避免无限制增长
- **CPU 可超卖(limits > requests),内存不可超卖**(内存不足会触发 OOMKill)

#### 2. QoS 等级与优驾逐策略

K8s 根据 requests/limits 配置将 Pod 分为三个 QoS 等级:

| QoS 等级 | 条件 | 驱逐优先级 | 适用场景 |
|---------|------|-----------|---------|
| **Guaranteed** | requests = limits(所有容器) | 最低 | 核心业务、数据库 |
| **Burstable** | 设置了 requests,但 requests < limits | 中等 | 一般应用服务 |
| **BestEffort** | 未设置 requests 和 limits | 最高 | 批处理任务、测试环境 |

**实践建议:**
- 核心服务使用 Guaranteed,确保资源保障
- 普通服务使用 Burstable,允许弹性使用
- 批处理任务使用 BestEffort,充分利用闲置资源

---

### 二、多维度自动弹性伸缩

#### 1. HPA(Horizontal Pod Autoscaler)—— Pod 水平扩缩容

基于 CPU、内存或自定义指标(如 QPS、延迟)自动调整 Pod 副本数:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # CPU 使用率超过 70% 时扩容
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"    # 每秒请求数超过 1000 时扩容
  behavior:  # 扩缩容行为控制
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 分钟稳定期,避免抖动
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60  # 每分钟最多缩容 50%
```

**关键配置:**
- **目标利用率设置 60-80%**,既保证性能又留有缓冲
- **设置 stabilizationWindow**,避免因流量抖动频繁扩缩容
- **结合自定义指标**(需要 Prometheus Adapter),比 CPU/内存更准确

#### 2. VPA(Vertical Pod Autoscaler)—— Pod 垂直扩缩容

自动调整 Pod 的 requests/limits 值:

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  updatePolicy:
    updateMode: "Auto"  # 自动更新(需重启 Pod),也可选 Recreate/Initial/Off
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2000m
        memory: 2Gi
      controlledResources: ["cpu", "memory"]
```

**适用场景:**
- 单体应用或有状态服务(不适合频繁水平扩容)
- 资源需求变化较大但可预测的应用
- **注意: VPA 会重启 Pod,不适合无状态高可用服务**

#### 3. CA(Cluster Autoscaler)—— 集群节点自动扩缩容

当 Pod 因资源不足无法调度时,自动添加节点;当节点利用率低时,自动删除节点:

```yaml
# 部署 Cluster Autoscaler
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  template:
    spec:
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.27.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --cloud-provider=aws  # 或 gcp/azure/alicloud
        - --nodes=2:10:node-group-name  # 最小 2 个,最大 10 个节点
        - --scale-down-enabled=true
        - --scale-down-delay-after-add=10m  # 扩容后 10 分钟才允许缩容
        - --scale-down-unneeded-time=10m    # 节点空闲 10 分钟后删除
```

**配置建议:**
- 设置合理的 **scale-down-delay**,避免扩容后立即缩容
- 使用 **PodDisruptionBudget(PDB)** 保护关键 Pod 不被驱逐
- 配合云厂商的**抢占式实例**(Spot Instance),降低成本

**三层伸缩协同工作:**
```
流量增加 → HPA 增加 Pod → 资源不足 → CA 增加节点
流量减少 → HPA 减少 Pod → 节点空闲 → CA 删除节点
        ↓
    VPA 调整单个 Pod 的资源配置
```

---

### 三、调度策略优化

#### 1. 节点亲和性(Node Affinity)—— 精准调度

将 Pod 调度到特定类型的节点:

```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:  # 硬性要求
      nodeSelectorTerms:
      - matchExpressions:
        - key: node-type
          operator: In
          values:
          - high-cpu  # 只调度到高 CPU 节点
    preferredDuringSchedulingIgnoredDuringExecution:  # 软性偏好
    - weight: 80
      preference:
        matchExpressions:
        - key: zone
          operator: In
          values:
          - cn-beijing-a  # 优先调度到北京 A 区
```

**使用场景:**
- 计算密集型任务调度到高 CPU 节点
- GPU 任务调度到 GPU 节点
- 就近调度减少跨可用区网络延迟

#### 2. Pod 亲和性与反亲和性 —— 拓扑分布

```yaml
affinity:
  podAntiAffinity:  # Pod 反亲和性:将副本分散到不同节点
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values:
          - my-app
      topologyKey: kubernetes.io/hostname  # 同一主机名的节点不调度

  podAffinity:  # Pod 亲和性:将相关 Pod 调度到一起
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - cache-service  # 优先调度到有 cache-service 的节点
        topologyKey: kubernetes.io/hostname
```

**实践建议:**
- 使用 **反亲和性** 提升高可用,避免单点故障
- 使用 **亲和性** 减少跨节点通信延迟(如应用和缓存部署在一起)

#### 3. Taints(污点)与 Tolerations(容忍)—— 资源隔离

为节点添加污点,只有容忍该污点的 Pod 才能调度:

```bash
# 为节点添加污点(专用于生产环境)
kubectl taint nodes prod-node env=production:NoSchedule

# 为节点添加污点(即将下线)
kubectl taint nodes old-node status=draining:NoExecute
```

```yaml
# Pod 容忍污点
tolerations:
- key: "env"
  operator: "Equal"
  value: "production"
  effect: "NoSchedule"  # 允许调度到 production 节点
- key: "status"
  operator: "Equal"
  value: "draining"
  effect: "NoExecute"
  tolerationSeconds: 3600  # 节点被标记为 draining 后,Pod 可存活 1 小时
```

**使用场景:**
- 隔离生产/测试环境
- 预留专用节点(如 GPU 节点只给 AI 任务)
- 节点维护时优雅驱逐 Pod

#### 4. 优先级与抢占(Priority & Preemption)

```yaml
# 定义优先级类
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000  # 数值越大优先级越高
globalDefault: false
description: "高优先级,用于核心业务"

---
# Pod 使用优先级
apiVersion: v1
kind: Pod
metadata:
  name: critical-app
spec:
  priorityClassName: high-priority  # 资源不足时,驱逐低优先级 Pod
  containers:
  - name: app
    image: my-app:latest
```

**抢占机制:**
当高优先级 Pod 无法调度时,会驱逐低优先级 Pod 腾出资源。这可以确保**核心业务优先保障**。

---

### 四、资源回收与成本控制

#### 1. 清理僵尸资源

```bash
# 查找未使用的 ConfigMap/Secret
kubectl get configmap --all-namespaces -o json | \
  jq '.items[] | select(.metadata.creationTimestamp < (now - 30*86400 | todate)) | .metadata.name'

# 清理已完成的 Job(保留最近 3 个)
kubectl delete job $(kubectl get job -o jsonpath='{.items[?(@.status.succeeded==1)].metadata.name}' | awk '{for(i=4;i<=NF;i++) print $i}')

# 清理 Evicted 状态的 Pod
kubectl get pods --all-namespaces --field-selector=status.phase==Failed -o json | \
  jq -r '.items[] | select(.status.reason=="Evicted") | "\(.metadata.namespace) \(.metadata.name)"' | \
  xargs -n2 bash -c 'kubectl delete pod $1 -n $0'
```

#### 2. ResourceQuota 与 LimitRange —— 命名空间级别限额

```yaml
# ResourceQuota:命名空间总资源限制
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-quota
  namespace: dev-team
spec:
  hard:
    requests.cpu: "20"
    requests.memory: "40Gi"
    limits.cpu: "40"
    limits.memory: "80Gi"
    pods: "50"  # 最多 50 个 Pod

---
# LimitRange:单个 Pod/容器的默认值和限制
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: dev-team
spec:
  limits:
  - max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    default:  # 未指定 limits 时的默认值
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:  # 未指定 requests 时的默认值
      cpu: "200m"
      memory: "256Mi"
    type: Container
```

**作用:**
- 防止单个团队/应用占用过多资源
- 强制设置默认值,避免遗漏配置

#### 3. 使用 Spot/Preemptible 实例降低成本

云厂商的抢占式实例价格通常是按需实例的 **20-30%**:

```yaml
# AWS EKS 使用 Spot 实例
apiVersion: v1
kind: Node
metadata:
  labels:
    node.kubernetes.io/instance-type: t3.large
    capacity-type: SPOT  # 标记为 Spot 实例
spec:
  taints:
  - key: "spot"
    value: "true"
    effect: "NoSchedule"

---
# 容忍 Spot 实例的 Pod(适合无状态服务)
tolerations:
- key: "spot"
  operator: "Equal"
  value: "true"
  effect: "NoSchedule"
```

**使用建议:**
- **无状态服务、批处理任务使用 Spot 实例**,可节省 70-80% 成本
- **核心数据库、有状态服务使用按需实例**,保证稳定性
- 配合 **Pod Disruption Budget** 限制同时被驱逐的 Pod 数量

---

### 五、监控与持续优化

#### 1. 核心监控指标

| 指标 | 计算方式 | 目标值 | 说明 |
|-----|---------|--------|------|
| **节点 CPU 利用率** | (已使用 CPU / 总 CPU) * 100% | 60-75% | 过低浪费,过高影响性能 |
| **节点内存利用率** | (已使用内存 / 总内存) * 100% | 70-85% | 内存不可超卖,需留缓冲 |
| **资源预留率** | (requests 总和 / 节点容量) * 100% | 70-80% | 过高导致资源碎片 |
| **Pod 密度** | Pod 数量 / 节点数 | 30-50/节点 | 取决于 Pod 大小 |
| **资源碎片率** | 无法调度的 Pod 比例 | <5% | 高碎片需调整 requests |

#### 2. Prometheus + Grafana 监控

```yaml
# 部署 kube-state-metrics(暴露 K8s 资源状态指标)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: monitoring
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: kube-state-metrics
        image: quay.io/coreos/kube-state-metrics:v2.10.0
        ports:
        - containerPort: 8080
```

**关键 PromQL 查询:**

```promql
# 节点 CPU 利用率
100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Pod 内存使用率
container_memory_working_set_bytes / container_spec_memory_limit_bytes * 100

# 资源预留 vs 实际使用
sum(kube_pod_container_resource_requests{resource="cpu"}) / sum(kube_node_status_capacity{resource="cpu"}) * 100

# 无法调度的 Pod 数量
sum(kube_pod_status_phase{phase="Pending"})
```

#### 3. 成本分析工具

- **Kubecost**:分析每个命名空间/应用的资源成本
- **KRR(Kubernetes Resource Recommendations)**:基于历史数据推荐 requests/limits
- **Goldilocks**:结合 VPA 推荐最优资源配置

```bash
# 安装 Kubecost
helm install kubecost kubecost/cost-analyzer \
  --namespace kubecost --create-namespace

# 访问 UI 查看成本分解
kubectl port-forward -n kubecost svc/kubecost-cost-analyzer 9090:9090
```

---

### 六、优化实践案例

#### 案例 1:降低资源预留率,提升装箱率

**问题:** 集群节点 CPU 实际使用率 40%,但无法调度新 Pod(资源碎片)

**原因:** 应用设置 `requests.cpu=2`,但实际只用 0.5 核

**解决方案:**
1. 通过 Prometheus 查询过去 30 天的 P90 CPU 使用量
2. 将 `requests.cpu` 从 2 核降低到 0.8 核
3. 设置 `limits.cpu=1.5` 核,允许 burst

**结果:** 节点装箱率提升 60%,节省 5 台服务器

#### 案例 2:HPA + CA 自动应对流量洪峰

**场景:** 电商大促,流量瞬间 10 倍增长

**配置:**
```yaml
# HPA 配置
minReplicas: 10  # 预热副本数
maxReplicas: 200
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      averageUtilization: 70
behavior:
  scaleUp:
    stabilizationWindowSeconds: 60  # 快速扩容
    policies:
    - type: Percent
      value: 100
      periodSeconds: 60  # 每分钟翻倍
```

**结果:** 流量高峰时自动扩容到 180 副本,峰值过后 30 分钟缩容到 15 副本,全程无人工干预

#### 案例 3:使用 Spot 实例降低 70% 成本

**场景:** 大数据批处理任务,每天凌晨运行 4 小时

**方案:**
1. 创建专用 Spot 节点池(50 个节点)
2. 批处理任务容忍 Spot 污点
3. 使用 **Job** + **CronJob** 执行任务,允许失败重试

**结果:**
- 按需实例成本:50 节点 × 4 小时 × $0.1/小时 = $20/天
- Spot 实例成本:50 节点 × 4 小时 × $0.03/小时 = $6/天
- **节省 70% 成本,年省约 $5000**

---

### 七、优化检查清单

**资源配置:**
- [ ] 所有 Pod 是否设置了 requests 和 limits?
- [ ] requests 是否基于实际使用量(P90)设置?
- [ ] 核心服务是否使用 Guaranteed QoS?

**弹性伸缩:**
- [ ] 是否启用了 HPA,目标利用率是否合理(60-80%)?
- [ ] 是否配置了 Cluster Autoscaler?
- [ ] 是否设置了 stabilizationWindow 避免抖动?

**调度优化:**
- [ ] 是否使用反亲和性提升高可用?
- [ ] 是否使用污点隔离生产/测试环境?
- [ ] 是否为核心业务设置了优先级?

**成本控制:**
- [ ] 是否定期清理僵尸资源(Evicted Pod、旧 Job)?
- [ ] 是否设置了 ResourceQuota 限制命名空间资源?
- [ ] 是否使用 Spot 实例降低成本?

**监控分析:**
- [ ] 是否部署了 Prometheus + Grafana?
- [ ] 是否监控节点利用率、资源预留率、Pod 密度?
- [ ] 是否使用 Kubecost 等工具分析成本?

---

## 总结

优化 Kubernetes 集群资源利用率是一个 **持续迭代** 的过程,需要从以下五个方面协同推进:

1. **精准配置 requests/limits**:基于监控数据而非经验值
2. **三层自动伸缩**:HPA(Pod 水平)+ VPA(Pod 垂直)+ CA(节点)
3. **智能调度策略**:亲和性、污点、优先级,提升装箱率
4. **资源回收与限额**:清理僵尸资源,使用 Quota/LimitRange 防止滥用
5. **数据驱动优化**:基于 Prometheus 监控持续分析,结合 Kubecost 降低成本

通过这些措施,可以将集群 **CPU 利用率从 30-40% 提升到 60-75%**,内存利用率从 40-50% 提升到 70-85%,同时降低 **30-50% 的基础设施成本**。关键是要建立 **监控→分析→优化→验证** 的闭环机制,避免一次性优化后再度退化。
