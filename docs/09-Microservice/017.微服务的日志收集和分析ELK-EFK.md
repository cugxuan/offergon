---
title: 微服务的日志收集和分析（ELK/EFK）
tags:
  - 微服务
status: robot
class: 微服务
slug: microservice-log-collection-analysis-elk-efk
ref:
---

## 核心要点

**ELK架构**:Elasticsearch(存储检索) + Logstash(采集处理) + Kibana(可视化),经典的集中式日志方案
**EFK架构**:用Fluentd替代Logstash,更轻量、云原生,适合Kubernetes环境
**关键技术**:结构化日志、TraceID关联、日志分级采集、索引生命周期管理

---

## 详细回答

### 一、微服务日志的挑战

在单体应用时代,日志文件存储在单一服务器上,通过`tail -f`或`grep`即可查看分析。但在微服务架构下面临新的挑战:

#### 1. 分布式环境的痛点

```
❌ 日志分散在几十甚至上百个服务实例
❌ 一次请求链路跨越多个服务,日志散落各处
❌ 容器化后,Pod重启日志丢失
❌ 手工登录每台机器查日志效率极低
❌ 缺乏统一的检索和分析能力
```

#### 2. 日志收集的需求

```
✅ 集中存储: 所有服务日志汇聚到统一平台
✅ 实时检索: 秒级查询任意时间段的日志
✅ 链路追踪: 通过TraceID串联分布式请求链路
✅ 可视化分析: 图表展示错误率、QPS等指标
✅ 告警能力: 异常日志自动触发告警
```

---

### 二、ELK架构详解

ELK是Elastic公司推出的日志解决方案,由三个组件组成:

#### 1. 架构图

```
┌─────────────────────────────────────────────────────┐
│  微服务实例                                          │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐          │
│  │ Service A│  │ Service B│  │ Service C│          │
│  │  (Pod)   │  │  (Pod)   │  │  (Pod)   │          │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘          │
│       │ JSON log    │ JSON log    │ JSON log       │
└───────┼─────────────┼─────────────┼─────────────────┘
        │             │             │
        ▼             ▼             ▼
   ┌────────────────────────────────────┐
   │  Logstash / Fluentd               │  ← 日志采集
   │  - 收集日志                        │
   │  - 解析格式(JSON/多行日志)         │
   │  - 过滤和转换                      │
   │  - 添加标签(service、env)          │
   └──────────────┬─────────────────────┘
                  │ Bulk API
                  ▼
   ┌─────────────────────────────────────┐
   │  Elasticsearch Cluster              │  ← 日志存储
   │  ┌──────┐  ┌──────┐  ┌──────┐      │
   │  │ Node1│  │ Node2│  │ Node3│      │
   │  └──────┘  └──────┘  └──────┘      │
   │  - 分布式存储                        │
   │  - 全文检索                          │
   │  - 聚合分析                          │
   └──────────────┬──────────────────────┘
                  │ REST API
                  ▼
   ┌─────────────────────────────────────┐
   │  Kibana                             │  ← 可视化
   │  - 日志检索界面                      │
   │  - 仪表盘(Dashboard)                 │
   │  - 告警规则配置                      │
   └─────────────────────────────────────┘
```

#### 2. 各组件职责

**Elasticsearch**:
- 分布式搜索和分析引擎
- 基于Lucene,支持全文检索
- 通过倒排索引实现秒级查询
- 水平扩展,支持PB级数据

**Logstash**:
- 数据收集和处理管道
- 支持200+输入插件(file, syslog, kafka等)
- 强大的过滤器(grok, mutate, date等)
- 输出到Elasticsearch、Kafka等

**Kibana**:
- ES的可视化界面
- Discover: 日志检索
- Dashboard: 自定义仪表盘
- Alerting: 告警规则

#### 3. Logstash配置示例

```ruby
# /etc/logstash/conf.d/microservice.conf
input {
  # 从Filebeat接收日志
  beats {
    port => 5044
  }

  # 或直接读取日志文件
  file {
    path => "/var/log/app/*.log"
    start_position => "beginning"
    codec => "json"  # 如果日志是JSON格式
  }
}

filter {
  # 解析JSON日志
  json {
    source => "message"
  }

  # 提取TraceID到独立字段
  if [trace_id] {
    mutate {
      add_field => { "[@metadata][trace_id]" => "%{trace_id}" }
    }
  }

  # 添加服务名标签
  mutate {
    add_field => { "service" => "%{[kubernetes][labels][app]}" }
    add_field => { "env" => "${ENV:prod}" }
  }

  # 过滤敏感信息
  mutate {
    remove_field => ["password", "credit_card"]
  }

  # 解析时间戳
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }
}

output {
  # 输出到Elasticsearch
  elasticsearch {
    hosts => ["es-node1:9200", "es-node2:9200"]
    index => "microservice-logs-%{+YYYY.MM.dd}"  # 按日期分索引
    user => "elastic"
    password => "${ES_PASSWORD}"
  }

  # 同时输出到控制台(调试用)
  # stdout { codec => rubydebug }
}
```

---

### 三、EFK架构(Kubernetes推荐)

EFK用**Fluentd**替代Logstash,更适合云原生环境:

#### 1. Fluentd vs Logstash

| 特性 | Fluentd | Logstash |
|------|---------|----------|
| **内存占用** | ~40MB | ~200MB |
| **语言** | C + Ruby | JRuby (JVM) |
| **配置** | 简洁 | 功能丰富但复杂 |
| **插件生态** | 500+ | 200+ |
| **Kubernetes集成** | 原生支持(DaemonSet) | 需额外配置 |
| **性能** | 高吞吐 | 功能强大但重 |

#### 2. Fluentd DaemonSet部署

```yaml
# fluentd-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_SCHEME
          value: "http"
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: "k8s-logs"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          subPath: fluent.conf
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config
        configMap:
          name: fluentd-config
```

#### 3. Fluentd配置

```ruby
# fluent.conf
<source>
  @type tail
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag kubernetes.*
  read_from_head true
  <parse>
    @type json
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# 添加Kubernetes元数据
<filter kubernetes.**>
  @type kubernetes_metadata
  @id filter_kube_metadata
</filter>

# 只收集特定服务的日志
<filter kubernetes.var.log.containers.**user-service**.log>
  @type record_transformer
  <record>
    service "user-service"
    env "#{ENV['CLUSTER_ENV']}"
  </record>
</filter>

# 输出到Elasticsearch
<match kubernetes.**>
  @type elasticsearch
  @id out_es
  @log_level info
  include_tag_key true
  host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
  port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
  scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
  logstash_format true
  logstash_prefix k8s-logs
  <buffer>
    @type file
    path /var/log/fluentd-buffers/kubernetes.system.buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_interval 5s
    retry_max_interval 30s
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>
```

---

### 四、Go微服务日志最佳实践

#### 1. 结构化日志输出

```go
package main

import (
    "context"
    "github.com/sirupsen/logrus"
    "github.com/google/uuid"
)

// 初始化JSON格式日志
func init() {
    logrus.SetFormatter(&logrus.JSONFormatter{
        TimestampFormat: "2006-01-02T15:04:05.000Z07:00",
        FieldMap: logrus.FieldMap{
            logrus.FieldKeyTime:  "timestamp",
            logrus.FieldKeyLevel: "level",
            logrus.FieldKeyMsg:   "message",
        },
    })
    logrus.SetLevel(logrus.InfoLevel)
}

// 日志上下文,包含TraceID等关键信息
type LogContext struct {
    TraceID   string
    UserID    int64
    Service   string
    RequestID string
}

func NewLogContext(ctx context.Context) *logrus.Entry {
    traceID := getTraceID(ctx)
    userID := getUserID(ctx)

    return logrus.WithFields(logrus.Fields{
        "trace_id":   traceID,
        "user_id":    userID,
        "service":    "user-service",
        "request_id": uuid.New().String(),
    })
}

// 使用示例
func HandleRequest(ctx context.Context, req *Request) error {
    logger := NewLogContext(ctx)

    logger.WithFields(logrus.Fields{
        "method": "HandleRequest",
        "input":  req,
    }).Info("处理用户请求")

    // 业务逻辑
    user, err := getUserFromDB(ctx, req.UserID)
    if err != nil {
        logger.WithError(err).Error("查询用户失败")
        return err
    }

    logger.WithFields(logrus.Fields{
        "user_id":   user.ID,
        "user_name": user.Name,
        "duration":  "45ms",
    }).Info("请求处理完成")

    return nil
}
```

**输出示例**:
```json
{
  "timestamp": "2025-10-10T14:23:45.123Z",
  "level": "info",
  "message": "处理用户请求",
  "trace_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "user_id": 12345,
  "service": "user-service",
  "request_id": "f1e2d3c4-b5a6-7890-1234-567890abcdef",
  "method": "HandleRequest"
}
```

#### 2. TraceID传递机制

```go
package tracing

import (
    "context"
    "github.com/google/uuid"
)

type contextKey string

const TraceIDKey contextKey = "trace_id"

// 生成或传递TraceID
func WithTraceID(ctx context.Context, traceID string) context.Context {
    if traceID == "" {
        traceID = uuid.New().String()
    }
    return context.WithValue(ctx, TraceIDKey, traceID)
}

func GetTraceID(ctx context.Context) string {
    if traceID, ok := ctx.Value(TraceIDKey).(string); ok {
        return traceID
    }
    return ""
}

// HTTP中间件:从Header获取或生成TraceID
func TraceIDMiddleware(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        traceID := r.Header.Get("X-Trace-ID")
        if traceID == "" {
            traceID = uuid.New().String()
        }

        ctx := WithTraceID(r.Context(), traceID)

        // 将TraceID返回给客户端
        w.Header().Set("X-Trace-ID", traceID)

        next.ServeHTTP(w, r.WithContext(ctx))
    })
}

// gRPC拦截器:传递TraceID
func UnaryServerInterceptor() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
        // 从metadata获取TraceID
        md, _ := metadata.FromIncomingContext(ctx)
        traceIDs := md.Get("x-trace-id")

        var traceID string
        if len(traceIDs) > 0 {
            traceID = traceIDs[0]
        } else {
            traceID = uuid.New().String()
        }

        ctx = WithTraceID(ctx, traceID)

        // 将TraceID传递到下游服务
        ctx = metadata.AppendToOutgoingContext(ctx, "x-trace-id", traceID)

        return handler(ctx, req)
    }
}
```

---

### 五、Elasticsearch索引优化

#### 1. 索引生命周期管理(ILM)

日志数据有时效性,采用**滚动索引**策略:

```json
// 创建索引模板
PUT _index_template/microservice-logs
{
  "index_patterns": ["microservice-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 3,
      "number_of_replicas": 1,
      "index.lifecycle.name": "microservice-policy",
      "index.lifecycle.rollover_alias": "microservice-logs"
    },
    "mappings": {
      "properties": {
        "@timestamp": { "type": "date" },
        "level": { "type": "keyword" },
        "message": { "type": "text" },
        "trace_id": { "type": "keyword" },
        "service": { "type": "keyword" },
        "user_id": { "type": "long" },
        "duration": { "type": "float" }
      }
    }
  }
}

// 定义生命周期策略
PUT _ilm/policy/microservice-policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",      // 索引大小超过50GB滚动
            "max_age": "1d"          // 或超过1天滚动
          }
        }
      },
      "warm": {
        "min_age": "7d",             // 7天后进入warm阶段
        "actions": {
          "shrink": {
            "number_of_shards": 1    // 减少分片数
          },
          "forcemerge": {
            "max_num_segments": 1    // 合并段,优化查询
          }
        }
      },
      "delete": {
        "min_age": "30d",            // 30天后删除
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

#### 2. 查询优化

```json
// 使用TraceID查询完整链路日志
GET microservice-logs-*/_search
{
  "query": {
    "bool": {
      "must": [
        { "term": { "trace_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890" } }
      ]
    }
  },
  "sort": [
    { "@timestamp": "asc" }
  ],
  "size": 1000
}

// 查询错误率趋势
GET microservice-logs-*/_search
{
  "size": 0,
  "query": {
    "bool": {
      "must": [
        { "term": { "service": "user-service" } },
        { "range": { "@timestamp": { "gte": "now-1h" } } }
      ]
    }
  },
  "aggs": {
    "errors_over_time": {
      "date_histogram": {
        "field": "@timestamp",
        "fixed_interval": "1m"
      },
      "aggs": {
        "error_count": {
          "filter": { "term": { "level": "error" } }
        }
      }
    }
  }
}
```

---

### 六、Kibana可视化实践

#### 1. 创建索引模式

```
Kibana → Stack Management → Index Patterns
→ Create index pattern: "microservice-logs-*"
→ Time field: @timestamp
```

#### 2. Discover查询技巧

```
# 查询特定服务的错误日志
service:"user-service" AND level:"error"

# 查询慢请求(耗时>1000ms)
duration:>1000

# 查询特定用户的所有请求
user_id:12345

# 组合查询
service:"order-service" AND (level:"error" OR level:"warn") AND @timestamp:[now-1h TO now]
```

#### 3. Dashboard仪表盘配置

创建关键指标仪表盘:

```
📊 日志量趋势图 (Line Chart)
   - Y轴: Count
   - X轴: @timestamp (按分钟聚合)
   - 拆分: service

📊 错误率饼图 (Pie Chart)
   - Slice: level (error/warn/info)
   - Metric: Count

📊 Top10慢请求 (Data Table)
   - Metric: Max(duration)
   - Bucket: Top 10 trace_id
   - 排序: duration降序

📊 服务错误热力图 (Heat Map)
   - Y轴: service
   - X轴: @timestamp
   - 颜色强度: error count
```

---

### 七、告警配置

#### 1. ElastAlert告警规则

```yaml
# /etc/elastalert/rules/error_rate_alert.yaml
name: "微服务错误率告警"
type: frequency
index: microservice-logs-*

# 触发条件: 5分钟内错误数>50
num_events: 50
timeframe:
  minutes: 5

filter:
- query:
    bool:
      must:
        - term:
            level: "error"
        - term:
            service: "user-service"

# 告警输出
alert:
  - "slack"
  - "email"

slack_webhook_url: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
slack_channel_override: "#alerts"

email:
  - "oncall@example.com"
```

#### 2. Kibana Watcher告警

```json
PUT _watcher/watch/error_spike
{
  "trigger": {
    "schedule": {
      "interval": "5m"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["microservice-logs-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                { "term": { "level": "error" } },
                { "range": { "@timestamp": { "gte": "now-5m" } } }
              ]
            }
          },
          "aggs": {
            "error_count": { "value_count": { "field": "_id" } }
          }
        }
      }
    }
  },
  "condition": {
    "compare": {
      "ctx.payload.aggregations.error_count.value": { "gt": 50 }
    }
  },
  "actions": {
    "send_slack": {
      "webhook": {
        "method": "POST",
        "url": "https://hooks.slack.com/services/YOUR/WEBHOOK/URL",
        "body": "{\"text\": \"🚨错误数超过阈值: {{ctx.payload.aggregations.error_count.value}}条\"}"
      }
    }
  }
}
```

---

### 八、最佳实践总结

#### 1. 日志规范

```
✅ 使用结构化日志(JSON格式)
✅ 必须包含: timestamp、level、message、service、trace_id
✅ 敏感信息脱敏(密码、Token、身份证号等)
✅ 日志分级: DEBUG/INFO/WARN/ERROR/FATAL
✅ 避免打印大对象,超过1KB应截断
```

#### 2. 性能优化

```
✅ 异步写日志,避免阻塞主流程
✅ 使用Filebeat而非Logstash直采(减少资源消耗)
✅ ES设置合理的分片数(建议单分片30-50GB)
✅ 开启索引生命周期管理,定期清理旧数据
✅ 高频日志采样(如DEBUG日志采样1%)
```

#### 3. 成本控制

```
💰 按日期滚动索引,过期数据自动删除
💰 冷热数据分离(hot-warm-cold架构)
💰 压缩历史索引
💰 只索引必要字段,message字段可选择不索引
💰 使用ES的快照功能备份到对象存储(S3/OSS)
```

#### 4. 工具对比

| 方案 | 适用场景 | 优势 | 劣势 |
|------|---------|------|------|
| **ELK** | 传统虚拟机部署 | 功能强大,生态完善 | 资源消耗大 |
| **EFK** | Kubernetes环境 | 轻量级,云原生 | 配置相对复杂 |
| **Loki** | 成本敏感场景 | 成本低,与Grafana集成好 | 查询能力弱于ES |
| **ClickHouse** | 海量日志分析 | 性能极高,成本低 | 运维复杂度高 |
| **云厂商方案** | 快速上线 | 开箱即用,无需运维 | 成本高,厂商锁定 |
