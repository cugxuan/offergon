---
title: 分库分表的策略，如何设计分片键？
tags:
  - 数据库
status: robot
class: 数据库
slug: sharding-strategy-shard-key-design
ref:
---

## 核心要点

分库分表通过水平拆分将数据分散到多个数据库/表，缓解单库单表性能瓶颈；分片键选择需遵循：高频查询字段、数据分布均匀、避免跨库事务、支持业务扩展性；常见策略包括哈希取模、范围分片、一致性哈希、地理位置分片等。

## 详细回答

### 一、分库分表概念与场景

**垂直拆分 vs 水平拆分：**

| 拆分方式 | 定义 | 适用场景 | 示例 |
|---------|------|---------|------|
| **垂直分库** | 按业务模块拆分数据库 | 不同业务模块数据分离 | 用户库、订单库、商品库 |
| **垂直分表** | 将宽表拆分为多个窄表 | 表字段过多，冷热数据分离 | 用户基础表、用户扩展表 |
| **水平分库** | 将同一表的数据分散到多个库 | 单库容量/连接数瓶颈 | 订单库 0-3，每库存储部分订单 |
| **水平分表** | 将同一表的数据分散到多个表 | 单表数据量过大（千万级） | orders_0 到 orders_255 |

**何时需要分库分表：**
```
单表数据量：
- < 100 万行：无需分表，优化索引即可
- 100 万 - 1000 万：考虑分表，或使用分区表
- > 1000 万行：强烈建议分库分表

单库连接数/QPS：
- > 3000 QPS：考虑读写分离
- > 5000 QPS：考虑分库
```

### 二、分片策略对比

#### 1. 哈希取模（Hash Mod）

根据分片键的哈希值对分片数取模，决定数据存储位置。

**示例：订单表按用户 ID 分片（8 个库）**
```go
package sharding

import (
    "hash/crc32"
)

// 计算分片索引（8 个库）
func GetShardIndex(userID int64, shardCount int) int {
    hash := crc32.ChecksumIEEE([]byte(fmt.Sprintf("%d", userID)))
    return int(hash % uint32(shardCount))
}

// 使用示例
func InsertOrder(order *Order) error {
    // 用户 ID: 12345, 分片数: 8
    shardIndex := GetShardIndex(order.UserID, 8)  // 结果: 5

    // 路由到 db_5 库
    dbName := fmt.Sprintf("order_db_%d", shardIndex)
    db := GetDB(dbName)

    return db.Create(order).Error
}

// 查询示例
func GetOrdersByUserID(userID int64) ([]*Order, error) {
    shardIndex := GetShardIndex(userID, 8)
    dbName := fmt.Sprintf("order_db_%d", shardIndex)
    db := GetDB(dbName)

    var orders []*Order
    err := db.Where("user_id = ?", userID).Find(&orders).Error
    return orders, err
}
```

**优点：**
- 数据分布均匀，避免热点问题
- 实现简单，计算开销小

**缺点：**
- 扩容困难：增加分片需要重新哈希，导致大量数据迁移
- 不支持范围查询：查询 user_id IN (1,2,3) 需要扫描所有分片

**适用场景：** 分片数固定、按单一 ID 查询为主的场景（如用户订单、账户余额）

#### 2. 范围分片（Range）

按照分片键的数值范围划分数据。

**示例：订单表按时间分片**
```go
// 按月份分片（每月一个表）
func GetTableName(orderTime time.Time) string {
    return fmt.Sprintf("orders_%s", orderTime.Format("200601"))  // orders_202510
}

// 插入订单
func InsertOrder(order *Order) error {
    tableName := GetTableName(order.CreateTime)
    db := GetDB("order_db")

    return db.Table(tableName).Create(order).Error
}

// 查询指定时间范围的订单
func GetOrdersByDateRange(startDate, endDate time.Time) ([]*Order, error) {
    var orders []*Order

    // 计算需要查询的所有分表
    currentDate := startDate
    for currentDate.Before(endDate) || currentDate.Equal(endDate) {
        tableName := GetTableName(currentDate)

        var monthOrders []*Order
        err := db.Table(tableName).
            Where("create_time BETWEEN ? AND ?", startDate, endDate).
            Find(&monthOrders).Error

        if err != nil {
            return nil, err
        }

        orders = append(orders, monthOrders...)
        currentDate = currentDate.AddDate(0, 1, 0)  // 下一个月
    }

    return orders, nil
}
```

**优点：**
- 支持范围查询，查询效率高
- 扩容简单：新增分片不影响旧数据
- 数据归档方便：可以直接删除旧分表

**缺点：**
- 数据分布可能不均匀（热点问题）
- 最新数据的分片压力大（如当月订单）

**适用场景：** 时间序列数据、日志数据、历史数据归档

#### 3. 一致性哈希（Consistent Hashing）

使用哈希环解决传统哈希取模扩容时数据迁移量大的问题。

```go
package sharding

import (
    "hash/crc32"
    "sort"
    "sync"
)

type ConsistentHash struct {
    circle       map[uint32]string  // 哈希环
    sortedHashes []uint32            // 排序的哈希值
    virtualNodes int                 // 虚拟节点数
    mu           sync.RWMutex
}

func NewConsistentHash(virtualNodes int) *ConsistentHash {
    return &ConsistentHash{
        circle:       make(map[uint32]string),
        virtualNodes: virtualNodes,
    }
}

// 添加分片节点
func (c *ConsistentHash) AddNode(node string) {
    c.mu.Lock()
    defer c.mu.Unlock()

    // 为每个物理节点创建多个虚拟节点（提高均匀性）
    for i := 0; i < c.virtualNodes; i++ {
        virtualKey := fmt.Sprintf("%s#%d", node, i)
        hash := crc32.ChecksumIEEE([]byte(virtualKey))
        c.circle[hash] = node
        c.sortedHashes = append(c.sortedHashes, hash)
    }

    sort.Slice(c.sortedHashes, func(i, j int) bool {
        return c.sortedHashes[i] < c.sortedHashes[j]
    })
}

// 获取分片节点
func (c *ConsistentHash) GetNode(key string) string {
    c.mu.RLock()
    defer c.mu.RUnlock()

    if len(c.circle) == 0 {
        return ""
    }

    hash := crc32.ChecksumIEEE([]byte(key))

    // 在哈希环上顺时针找到第一个节点
    idx := sort.Search(len(c.sortedHashes), func(i int) bool {
        return c.sortedHashes[i] >= hash
    })

    if idx == len(c.sortedHashes) {
        idx = 0  // 环形结构，回到起点
    }

    return c.circle[c.sortedHashes[idx]]
}

// 使用示例
func main() {
    ch := NewConsistentHash(150)  // 每个节点 150 个虚拟节点

    // 初始 4 个分片
    ch.AddNode("db_0")
    ch.AddNode("db_1")
    ch.AddNode("db_2")
    ch.AddNode("db_3")

    // 查询用户 12345 的分片
    dbName := ch.GetNode("user_12345")  // 返回 "db_2"

    // 扩容：新增一个分片（只需迁移约 1/5 的数据）
    ch.AddNode("db_4")
}
```

**优点：**
- 扩容时只需迁移 1/N 的数据（N 为节点数）
- 虚拟节点保证数据分布均匀

**缺点：**
- 实现复杂度高
- 仍不支持范围查询

**适用场景：** 需要频繁扩容的分布式缓存、对象存储

#### 4. 地理位置分片（Geo-based）

根据用户地理位置（城市/地区）分片。

```sql
-- 按地区分库（华北、华东、华南）
-- 用户表按 region_code 分片
INSERT INTO user_db_north.users VALUES (...);  -- 北京用户
INSERT INTO user_db_east.users VALUES (...);   -- 上海用户
INSERT INTO user_db_south.users VALUES (...);  -- 深圳用户
```

**优点：**
- 数据本地化，减少跨地域网络延迟
- 符合数据合规要求（如数据不出境）

**缺点：**
- 数据分布不均（一线城市用户量大）
- 跨地域查询复杂

### 三、分片键设计原则

**1. 高频查询字段**

选择最常用的查询条件作为分片键，避免跨库查询。

```go
// 错误示例：分片键不在查询条件中
// 按 user_id 分片，但查询用户订单号
SELECT * FROM orders WHERE order_no = 'ORD123456';
// 需要扫描所有 8 个分片，性能差

// 正确示例：分片键在查询条件中
SELECT * FROM orders WHERE user_id = 12345 AND order_no = 'ORD123456';
// 只查询 user_id=12345 所在的分片
```

**2. 数据分布均匀**

避免热点问题，防止某个分片数据量过大。

```go
// 错误示例：按 status 分片（数据严重倾斜）
// 订单状态：待支付(10%)、已完成(85%)、已取消(5%)
// 导致"已完成"分片压力巨大

// 正确示例：按 user_id 哈希分片
// 用户 ID 分布均匀，每个分片负载相近
```

**3. 避免跨库事务**

尽量保证同一个事务的数据在同一个分片。

```go
// 场景：创建订单 + 扣减库存
// 如果订单表和库存表分片键不同，会产生分布式事务

// 方案 1：使用相同分片键（推荐）
// 订单表和订单明细表都按 order_id 分片，保证在同一库

// 方案 2：业务层补偿（TCC/SAGA）
// 先扣库存（本地事务），再创建订单（本地事务）
// 失败时通过补偿回滚库存
```

**4. 支持业务扩展**

分片键一旦确定，修改成本极高，需要考虑未来业务变化。

```go
// 示例：电商订单系统
// 初期：按 user_id 分片（用户查询自己的订单）
// 后期：需要商家后台查询订单（按 shop_id 查询）

// 解决方案：冗余分片策略
// 1. 用户视图：按 user_id 分片的订单表（用户查询）
// 2. 商家视图：按 shop_id 分片的订单表（商家查询）
// 通过 binlog 同步或双写保证数据一致性
```

### 四、实战案例

**案例：社交平台私信系统分库分表**

**需求分析：**
- 用户量：5 亿
- 私信总量：1000 亿条
- 核心查询：查询用户 A 和用户 B 的聊天记录

**方案设计：**

```go
package message

import (
    "fmt"
    "sort"
)

// 分片策略：按对话双方 ID 的最小值哈希分片
// 保证 A→B 和 B→A 的消息在同一分片

func GetShardKey(userID1, userID2 int64) string {
    // 对话双方 ID 排序
    ids := []int64{userID1, userID2}
    sort.Slice(ids, func(i, j int) bool {
        return ids[i] < ids[j]
    })

    // 拼接为分片键
    return fmt.Sprintf("%d_%d", ids[0], ids[1])
}

func GetShardIndex(userID1, userID2 int64, shardCount int) int {
    shardKey := GetShardKey(userID1, userID2)
    hash := crc32.ChecksumIEEE([]byte(shardKey))
    return int(hash % uint32(shardCount))
}

// 发送消息
func SendMessage(fromUserID, toUserID int64, content string) error {
    shardIndex := GetShardIndex(fromUserID, toUserID, 64)  // 64 个分片
    dbName := fmt.Sprintf("message_db_%d", shardIndex)
    db := GetDB(dbName)

    message := &Message{
        FromUserID: fromUserID,
        ToUserID:   toUserID,
        Content:    content,
        CreateTime: time.Now(),
    }

    return db.Create(message).Error
}

// 查询聊天记录
func GetChatHistory(userID1, userID2 int64, limit int) ([]*Message, error) {
    shardIndex := GetShardIndex(userID1, userID2, 64)
    dbName := fmt.Sprintf("message_db_%d", shardIndex)
    db := GetDB(dbName)

    var messages []*Message
    err := db.Where("(from_user_id = ? AND to_user_id = ?) OR (from_user_id = ? AND to_user_id = ?)",
        userID1, userID2, userID2, userID1).
        Order("create_time DESC").
        Limit(limit).
        Find(&messages).Error

    return messages, err
}
```

**表结构设计：**
```sql
-- 每个库包含一张消息表
CREATE TABLE message_db_0.messages (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    from_user_id BIGINT NOT NULL,
    to_user_id BIGINT NOT NULL,
    content TEXT,
    create_time DATETIME NOT NULL,
    INDEX idx_from_to_time (from_user_id, to_user_id, create_time),
    INDEX idx_to_from_time (to_user_id, from_user_id, create_time)
) ENGINE=InnoDB;

-- 其他分片表结构相同（message_db_1 ... message_db_63）
```

**优势：**
- 同一对话的消息永远在同一分片，查询只需访问 1 个库
- 数据分布均匀，每个分片约 15 亿条消息
- 支持未来扩容到 128/256 个分片

### 五、分库分表中间件选型

| 中间件 | 类型 | 特点 | 适用场景 |
|--------|------|------|----------|
| **ShardingSphere** | 客户端代理 | 功能全面，支持分布式事务、读写分离 | Java/Go 应用，复杂分片场景 |
| **Vitess** | 服务端代理 | YouTube 开源，支持在线扩容 | 超大规模 MySQL 集群 |
| **MyCat** | 服务端代理 | 国内流行，配置简单 | 中小规模应用 |
| **自研方案** | 应用层 | 灵活可控，但需自行处理路由逻辑 | 特定业务场景 |

**ShardingSphere-Go 示例：**
```yaml
# sharding-sphere.yaml
dataSources:
  ds_0:
    url: jdbc:mysql://localhost:3306/order_db_0
  ds_1:
    url: jdbc:mysql://localhost:3306/order_db_1
  ds_2:
    url: jdbc:mysql://localhost:3306/order_db_2
  ds_3:
    url: jdbc:mysql://localhost:3306/order_db_3

shardingRule:
  tables:
    orders:
      actualDataNodes: ds_${0..3}.orders_${0..7}
      databaseStrategy:
        inline:
          shardingColumn: user_id
          algorithmExpression: ds_${user_id % 4}
      tableStrategy:
        inline:
          shardingColumn: user_id
          algorithmExpression: orders_${user_id % 8}
      keyGenerator:
        column: order_id
        type: SNOWFLAKE  # 雪花算法生成分布式 ID
```

### 六、总结

分片键设计是分库分表成败的关键，需要综合考虑：
1. 业务查询模式（80% 的查询应通过分片键定位）
2. 数据增长趋势（预估 3-5 年内数据量）
3. 扩容计划（哈希取模 vs 一致性哈希）
4. 跨库查询接受度（是否允许 ES/Hive 做全局查询）

推荐策略：**高频查询字段 + 哈希取模**（如用户维度业务用 user_id，商家维度业务用 shop_id），配合全局索引表或搜索引擎解决非分片键查询问题。
