---
title: Kubernetes 的 Operator 模式
tags:
  - Kubernetes
status: robot
class: Kubernetes
slug: kubernetes-operator-pattern
ref:
---

## 核心要点

- **Operator 定义**: 使用自定义资源(CRD)和控制器(Controller)实现应用自动化管理的模式
- **核心组成**: CRD(自定义资源定义) + Controller(控制循环) + Reconcile(协调逻辑)
- **控制循环**: 持续监听资源状态,自动将实际状态调谐到期望状态
- **应用场景**: 有状态应用(数据库、中间件)、复杂运维操作(备份、升级、扩缩容)
- **生态优势**: OperatorHub 提供数百个现成 Operator(Prometheus、MySQL、Kafka 等)

---

## 详细回答

### 一、为什么需要 Operator

#### 1. 传统部署方式的局限

**场景**: 部署 MySQL 集群

```yaml
# 使用 StatefulSet 部署
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
```

**问题**:
- ❌ **初始化配置**: 主从复制需要手动配置
- ❌ **故障恢复**: 主节点故障需要人工切换
- ❌ **备份恢复**: 需要编写脚本和 CronJob
- ❌ **版本升级**: 需要精心编排升级步骤
- ❌ **监控告警**: 需要单独配置 Prometheus 规则

#### 2. Operator 的解决方案

**使用 MySQL Operator**

```yaml
apiVersion: mysql.presslabs.org/v1alpha1
kind: MysqlCluster
metadata:
  name: my-cluster
spec:
  replicas: 3
  secretName: mysql-secret
  backupSchedule: "0 0 * * *"  # 自动备份
  podSpec:
    resources:
      requests:
        memory: 1Gi
        cpu: 500m
```

**Operator 自动完成**:
- ✅ 主从复制配置
- ✅ 自动故障切换
- ✅ 定时备份和恢复
- ✅ 滚动升级
- ✅ 监控指标暴露

---

### 二、Operator 核心概念

#### 1. 架构全景图

```
┌─────────────────────────────────────────────────────────┐
│              Kubernetes API Server                      │
│                                                          │
│  ┌────────────────────┐    ┌─────────────────────┐     │
│  │  Built-in Resources│    │  Custom Resources   │     │
│  │  - Pod             │    │  (CRD)              │     │
│  │  - Deployment      │    │  - MysqlCluster     │     │
│  │  - Service         │    │  - RedisCluster     │     │
│  └────────────────────┘    └─────────────────────┘     │
│                                      │                  │
└──────────────────────────────────────┼──────────────────┘
                                       │
                                       │ Watch (监听变化)
                                       ▼
                        ┌──────────────────────────┐
                        │      Operator Pod        │
                        │  (Controller Manager)    │
                        │                          │
                        │  ┌────────────────────┐  │
                        │  │  Controller        │  │
                        │  │  (控制循环)        │  │
                        │  │                    │  │
                        │  │  1. Watch Event    │  │
                        │  │  2. Get Desired    │  │
                        │  │  3. Get Current    │  │
                        │  │  4. Reconcile      │  │
                        │  └────────────────────┘  │
                        └──────────┬───────────────┘
                                   │
                                   │ Create/Update/Delete
                                   ▼
                        ┌──────────────────────────┐
                        │   Managed Resources      │
                        │                          │
                        │  - StatefulSet           │
                        │  - Service               │
                        │  - ConfigMap             │
                        │  - PVC                   │
                        └──────────────────────────┘
```

#### 2. 三大核心组件

**① CRD(Custom Resource Definition)**
- **作用**: 扩展 K8s API,定义自定义资源类型
- **类比**: 类似数据库表结构定义
- **示例**: 定义 `MysqlCluster` 资源

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: mysqlclusters.mysql.presslabs.org
spec:
  group: mysql.presslabs.org      # API 组
  versions:
  - name: v1alpha1                # 版本
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              replicas:           # 副本数
                type: integer
                minimum: 1
              secretName:         # 密码 Secret
                type: string
              backupSchedule:     # 备份计划
                type: string
  scope: Namespaced               # 作用域
  names:
    plural: mysqlclusters         # 复数形式
    singular: mysqlcluster        # 单数形式
    kind: MysqlCluster            # 资源类型名
    shortNames:
    - mysql
```

创建 CRD 后,可以像使用内置资源一样操作:
```bash
kubectl get mysqlclusters
kubectl describe mysqlcluster my-cluster
kubectl delete mysqlcluster my-cluster
```

**② Controller(控制器)**
- **作用**: 监听资源变化,执行协调逻辑
- **核心**: 控制循环(Control Loop)
- **实现**: 通常用 Go 编写,使用 client-go 或 controller-runtime

**控制循环伪代码**:
```go
for {
    // 1. 监听资源事件
    event := watchResource("MysqlCluster")

    // 2. 获取期望状态(Desired State)
    desired := event.Object.Spec

    // 3. 获取当前状态(Current State)
    current := getCurrentState()

    // 4. 协调(Reconcile): 将当前状态调整为期望状态
    if current != desired {
        reconcile(current, desired)
    }
}
```

**③ Reconcile(协调函数)**
- **作用**: 实现业务逻辑,确保资源达到期望状态
- **特点**:
  - **幂等性**: 多次执行结果一致
  - **声明式**: 只关心最终状态,不关心步骤
  - **持续运行**: 即使手动修改也会被自动恢复

**Reconcile 示例**:
```go
func (r *MysqlClusterReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    // 1. 获取 MysqlCluster 资源
    cluster := &mysqlv1.MysqlCluster{}
    if err := r.Get(ctx, req.NamespacedName, cluster); err != nil {
        return ctrl.Result{}, client.IgnoreNotFound(err)
    }

    // 2. 确保 StatefulSet 存在
    if err := r.ensureStatefulSet(ctx, cluster); err != nil {
        return ctrl.Result{}, err
    }

    // 3. 确保 Service 存在
    if err := r.ensureService(ctx, cluster); err != nil {
        return ctrl.Result{}, err
    }

    // 4. 配置主从复制
    if err := r.configureMasterSlave(ctx, cluster); err != nil {
        return ctrl.Result{}, err
    }

    // 5. 更新状态
    cluster.Status.ReadyReplicas = getReadyReplicas()
    r.Status().Update(ctx, cluster)

    return ctrl.Result{}, nil
}
```

---

### 三、Operator 工作原理

#### 1. 完整生命周期示例

**用户操作**:
```bash
kubectl apply -f mysql-cluster.yaml
```

**Operator 执行流程**:

```
1️⃣ 用户创建自定义资源
   ↓
   kubectl apply -f mysqlcluster.yaml
   ↓
2️⃣ API Server 存储资源
   ↓
   etcd: mysqlclusters/my-cluster
   ↓
3️⃣ Controller 监听到事件
   ↓
   Watch Event: ADDED
   ↓
4️⃣ 触发 Reconcile 函数
   ↓
   Reconcile(my-cluster)
   ↓
5️⃣ 创建底层资源
   ├─ StatefulSet (3 replicas)
   ├─ Headless Service
   ├─ ConfigMap (MySQL 配置)
   └─ PVC (持久化存储)
   ↓
6️⃣ 配置主从复制
   ├─ 选举 Master
   ├─ 配置 Slave
   └─ 启动复制
   ↓
7️⃣ 更新资源状态
   ↓
   Status.ReadyReplicas = 3
   Status.Phase = "Running"
```

#### 2. 自动故障恢复示例

**场景**: Master Pod 意外删除

```
1️⃣ Master Pod 被删除
   ↓
2️⃣ Controller 监听到事件
   ↓
   Watch Event: POD_DELETED (master-0)
   ↓
3️⃣ 触发 Reconcile
   ↓
4️⃣ 检测到 Master 缺失
   ↓
5️⃣ 自动故障切换
   ├─ 选举新 Master (从 Slave 中提升)
   ├─ 更新其他 Slave 指向新 Master
   └─ 等待旧 Master Pod 重建为 Slave
   ↓
6️⃣ 集群恢复正常
```

---

### 四、开发 Operator(使用 Kubebuilder)

#### 1. 环境准备

```bash
# 安装 Kubebuilder
curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH)
chmod +x kubebuilder && mv kubebuilder /usr/local/bin/

# 创建项目
mkdir myapp-operator && cd myapp-operator
kubebuilder init --domain example.com --repo github.com/myuser/myapp-operator
```

#### 2. 创建 API(CRD)

```bash
# 生成 API 和 Controller
kubebuilder create api --group apps --version v1alpha1 --kind MyApp
# Create Resource [y/n]: y
# Create Controller [y/n]: y
```

**生成的目录结构**:
```
myapp-operator/
├── api/
│   └── v1alpha1/
│       ├── myapp_types.go      # 定义资源结构
│       └── zz_generated.*.go   # 自动生成代码
├── config/
│   ├── crd/                    # CRD YAML
│   ├── rbac/                   # RBAC 权限
│   └── manager/                # Operator 部署配置
├── controllers/
│   └── myapp_controller.go     # Reconcile 逻辑
└── main.go                     # 程序入口
```

#### 3. 定义资源结构

**api/v1alpha1/myapp_types.go**

```go
package v1alpha1

import (
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// MyAppSpec 定义期望状态
type MyAppSpec struct {
    // 镜像名称
    // +kubebuilder:validation:Required
    Image string `json:"image"`

    // 副本数
    // +kubebuilder:validation:Minimum=1
    // +kubebuilder:validation:Maximum=10
    // +kubebuilder:default=1
    Replicas int32 `json:"replicas,omitempty"`

    // 端口
    // +kubebuilder:validation:Minimum=1
    // +kubebuilder:validation:Maximum=65535
    Port int32 `json:"port,omitempty"`

    // 资源限制
    Resources ResourceRequirements `json:"resources,omitempty"`
}

// MyAppStatus 定义实际状态
type MyAppStatus struct {
    // 可用副本数
    AvailableReplicas int32 `json:"availableReplicas"`

    // 当前阶段
    // +kubebuilder:validation:Enum=Pending;Running;Failed
    Phase string `json:"phase,omitempty"`

    // 错误信息
    Message string `json:"message,omitempty"`
}

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:subresource:scale:specpath=.spec.replicas,statuspath=.status.availableReplicas
// +kubebuilder:printcolumn:name="Replicas",type=integer,JSONPath=`.spec.replicas`
// +kubebuilder:printcolumn:name="Available",type=integer,JSONPath=`.status.availableReplicas`
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`

// MyApp 是自定义资源
type MyApp struct {
    metav1.TypeMeta   `json:",inline"`
    metav1.ObjectMeta `json:"metadata,omitempty"`

    Spec   MyAppSpec   `json:"spec,omitempty"`
    Status MyAppStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true

// MyAppList 包含多个 MyApp
type MyAppList struct {
    metav1.TypeMeta `json:",inline"`
    metav1.ListMeta `json:"metadata,omitempty"`
    Items           []MyApp `json:"items"`
}

func init() {
    SchemeBuilder.Register(&MyApp{}, &MyAppList{})
}
```

#### 4. 实现 Reconcile 逻辑

**controllers/myapp_controller.go**

```go
package controllers

import (
    "context"
    appsv1 "k8s.io/api/apps/v1"
    corev1 "k8s.io/api/core/v1"
    "k8s.io/apimachinery/pkg/api/errors"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/apimachinery/pkg/runtime"
    ctrl "sigs.k8s.io/controller-runtime"
    "sigs.k8s.io/controller-runtime/pkg/client"

    appsv1alpha1 "github.com/myuser/myapp-operator/api/v1alpha1"
)

type MyAppReconciler struct {
    client.Client
    Scheme *runtime.Scheme
}

// +kubebuilder:rbac:groups=apps.example.com,resources=myapps,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=apps.example.com,resources=myapps/status,verbs=get;update;patch
// +kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=core,resources=services,verbs=get;list;watch;create;update;patch;delete

func (r *MyAppReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    log := ctrl.LoggerFrom(ctx)

    // 1. 获取 MyApp 资源
    myApp := &appsv1alpha1.MyApp{}
    if err := r.Get(ctx, req.NamespacedName, myApp); err != nil {
        if errors.IsNotFound(err) {
            // 资源已删除,无需处理
            return ctrl.Result{}, nil
        }
        return ctrl.Result{}, err
    }

    // 2. 构造期望的 Deployment
    deployment := r.deploymentForMyApp(myApp)

    // 3. 检查 Deployment 是否存在
    found := &appsv1.Deployment{}
    err := r.Get(ctx, client.ObjectKeyFromObject(deployment), found)
    if err != nil && errors.IsNotFound(err) {
        // 不存在,创建新的
        log.Info("Creating Deployment", "Deployment.Name", deployment.Name)
        if err := r.Create(ctx, deployment); err != nil {
            return ctrl.Result{}, err
        }
        return ctrl.Result{Requeue: true}, nil
    } else if err != nil {
        return ctrl.Result{}, err
    }

    // 4. Deployment 存在,检查是否需要更新
    if *found.Spec.Replicas != myApp.Spec.Replicas {
        found.Spec.Replicas = &myApp.Spec.Replicas
        if err := r.Update(ctx, found); err != nil {
            return ctrl.Result{}, err
        }
        log.Info("Updated Deployment replicas", "Replicas", myApp.Spec.Replicas)
    }

    // 5. 更新 MyApp 状态
    myApp.Status.AvailableReplicas = found.Status.AvailableReplicas
    if found.Status.AvailableReplicas == myApp.Spec.Replicas {
        myApp.Status.Phase = "Running"
    } else {
        myApp.Status.Phase = "Pending"
    }

    if err := r.Status().Update(ctx, myApp); err != nil {
        return ctrl.Result{}, err
    }

    return ctrl.Result{}, nil
}

// 构造 Deployment
func (r *MyAppReconciler) deploymentForMyApp(m *appsv1alpha1.MyApp) *appsv1.Deployment {
    labels := map[string]string{
        "app":                          "myapp",
        "app.kubernetes.io/name":       m.Name,
        "app.kubernetes.io/managed-by": "myapp-operator",
    }

    replicas := m.Spec.Replicas

    dep := &appsv1.Deployment{
        ObjectMeta: metav1.ObjectMeta{
            Name:      m.Name,
            Namespace: m.Namespace,
            Labels:    labels,
        },
        Spec: appsv1.DeploymentSpec{
            Replicas: &replicas,
            Selector: &metav1.LabelSelector{
                MatchLabels: labels,
            },
            Template: corev1.PodTemplateSpec{
                ObjectMeta: metav1.ObjectMeta{
                    Labels: labels,
                },
                Spec: corev1.PodSpec{
                    Containers: []corev1.Container{{
                        Name:  "myapp",
                        Image: m.Spec.Image,
                        Ports: []corev1.ContainerPort{{
                            ContainerPort: m.Spec.Port,
                            Name:          "http",
                        }},
                    }},
                },
            },
        },
    }

    // 设置 OwnerReference(当 MyApp 删除时自动删除 Deployment)
    ctrl.SetControllerReference(m, dep, r.Scheme)

    return dep
}

// SetupWithManager 注册 Controller
func (r *MyAppReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&appsv1alpha1.MyApp{}).          // 主资源
        Owns(&appsv1.Deployment{}).          // 拥有的子资源
        Complete(r)
}
```

#### 5. 测试与部署

```bash
# 1. 生成 CRD YAML
make manifests

# 2. 安装 CRD 到集群
make install

# 3. 本地运行 Operator(开发调试)
make run

# 4. 创建自定义资源
cat <<EOF | kubectl apply -f -
apiVersion: apps.example.com/v1alpha1
kind: MyApp
metadata:
  name: myapp-sample
spec:
  image: nginx:1.21
  replicas: 3
  port: 80
EOF

# 5. 查看资源
kubectl get myapp
# NAME            REPLICAS   AVAILABLE   PHASE
# myapp-sample    3          3           Running

kubectl get deployment
# NAME            READY   UP-TO-DATE   AVAILABLE   AGE
# myapp-sample    3/3     3            3           1m

# 6. 构建镜像并部署到集群
make docker-build docker-push IMG=myregistry.com/myapp-operator:v1.0.0
make deploy IMG=myregistry.com/myapp-operator:v1.0.0
```

---

### 五、Operator 能力等级(Capability Levels)

根据 Operator Framework,Operator 分为 5 个成熟度级别:

```
Level 5: Auto Pilot (自动驾驶)
   ↑    - 自动扩缩容
   |    - 自动调优
   |    - 异常检测和自愈
   |
Level 4: Deep Insights (深度洞察)
   ↑    - 监控指标
   |    - 日志分析
   |    - 告警
   |
Level 3: Full Lifecycle (完整生命周期)
   ↑    - 备份和恢复
   |    - 故障切换
   |    - 升级和降级
   |
Level 2: Seamless Upgrades (无缝升级)
   ↑    - 应用升级
   |    - 配置更新
   |
Level 1: Basic Install (基础安装)
        - 自动化部署
        - 配置管理
```

**示例对比**:

| 能力 | Level 1 | Level 3 | Level 5 |
|------|---------|---------|---------|
| **部署** | ✅ 自动 | ✅ 自动 | ✅ 自动 |
| **升级** | ❌ 手动 | ✅ 自动 | ✅ 自动 + 金丝雀 |
| **备份** | ❌ 手动 | ✅ 定时备份 | ✅ 增量备份 + 自动恢复 |
| **扩缩容** | ❌ 手动 | ✅ 手动触发 | ✅ 基于指标自动扩缩 |
| **故障恢复** | ❌ 人工介入 | ✅ 自动切换 | ✅ 预测性维护 |

---

### 六、常见 Operator 示例

#### 1. Prometheus Operator

```yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  replicas: 2
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      team: frontend
  resources:
    requests:
      memory: 400Mi
```

**自动完成**:
- 部署 Prometheus StatefulSet
- 配置服务发现
- 管理 RBAC 权限
- 动态加载 PrometheusRule

#### 2. etcd Operator

```yaml
apiVersion: etcd.database.coreos.com/v1beta2
kind: EtcdCluster
metadata:
  name: etcd-cluster
spec:
  size: 3
  version: 3.5.0
  repository: quay.io/coreos/etcd
  pod:
    resources:
      requests:
        memory: 1Gi
        cpu: 500m
```

**自动完成**:
- 创建 etcd 集群
- 配置 TLS 证书
- 自动备份到 S3
- 故障成员自动替换

#### 3. Strimzi Kafka Operator

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    version: 3.3.1
    replicas: 3
    storage:
      type: persistent-claim
      size: 100Gi
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 10Gi
  entityOperator:
    topicOperator: {}
    userOperator: {}
```

**自动完成**:
- 部署 Kafka + Zookeeper
- Topic 和 User 管理
- 滚动升级
- 监控指标暴露

---

### 七、Operator 最佳实践

#### 1. 幂等性设计

**错误示例**:
```go
func (r *Reconciler) Reconcile(ctx context.Context, req ctrl.Request) {
    // ❌ 每次都创建,会导致重复资源
    deployment := newDeployment()
    r.Create(ctx, deployment)
}
```

**正确做法**:
```go
func (r *Reconciler) Reconcile(ctx context.Context, req ctrl.Request) {
    deployment := newDeployment()

    // ✅ 检查是否存在
    found := &appsv1.Deployment{}
    err := r.Get(ctx, client.ObjectKeyFromObject(deployment), found)
    if err != nil && errors.IsNotFound(err) {
        // 不存在才创建
        return r.Create(ctx, deployment)
    }

    // 存在则更新
    return r.Update(ctx, deployment)
}
```

#### 2. 使用 Finalizer 处理删除

```go
const myAppFinalizer = "apps.example.com/finalizer"

func (r *Reconciler) Reconcile(ctx context.Context, req ctrl.Request) {
    myApp := &appsv1alpha1.MyApp{}
    if err := r.Get(ctx, req.NamespacedName, myApp); err != nil {
        return ctrl.Result{}, client.IgnoreNotFound(err)
    }

    // 检查是否正在删除
    if !myApp.DeletionTimestamp.IsZero() {
        // 执行清理逻辑
        if controllerutil.ContainsFinalizer(myApp, myAppFinalizer) {
            // 清理外部资源(如云服务、数据库等)
            if err := r.cleanupExternalResources(ctx, myApp); err != nil {
                return ctrl.Result{}, err
            }

            // 移除 Finalizer
            controllerutil.RemoveFinalizer(myApp, myAppFinalizer)
            return ctrl.Result{}, r.Update(ctx, myApp)
        }
        return ctrl.Result{}, nil
    }

    // 添加 Finalizer(如果不存在)
    if !controllerutil.ContainsFinalizer(myApp, myAppFinalizer) {
        controllerutil.AddFinalizer(myApp, myAppFinalizer)
        return ctrl.Result{}, r.Update(ctx, myApp)
    }

    // 正常业务逻辑
    return ctrl.Result{}, nil
}
```

#### 3. 状态管理

```go
// 使用条件(Condition)记录状态历史
func (r *Reconciler) updateStatus(ctx context.Context, myApp *appsv1alpha1.MyApp) error {
    // 设置条件
    condition := metav1.Condition{
        Type:               "Ready",
        Status:             metav1.ConditionTrue,
        Reason:             "DeploymentReady",
        Message:            "All replicas are ready",
        LastTransitionTime: metav1.Now(),
    }

    meta.SetStatusCondition(&myApp.Status.Conditions, condition)

    return r.Status().Update(ctx, myApp)
}
```

#### 4. 错误处理与重试

```go
func (r *Reconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    // 临时错误:返回 error 触发指数退避重试
    if err := r.doSomething(); err != nil {
        return ctrl.Result{}, err  // 默认重试间隔:5ms → 10ms → 20ms ...
    }

    // 需要定期重新检查
    return ctrl.Result{RequeueAfter: 5 * time.Minute}, nil

    // 手动触发重新入队
    return ctrl.Result{Requeue: true}, nil
}
```

#### 5. RBAC 权限最小化

```yaml
# config/rbac/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: myapp-operator-role
rules:
# 只授予必需权限
- apiGroups: ["apps.example.com"]
  resources: ["myapps"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

- apiGroups: ["apps.example.com"]
  resources: ["myapps/status"]
  verbs: ["get", "update", "patch"]

- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
```

---

### 八、Operator vs Helm

| 维度 | Helm | Operator |
|------|------|----------|
| **定位** | 包管理工具 | 应用运维自动化 |
| **安装** | ✅ 优秀 | ✅ 支持 |
| **升级** | ✅ 支持回滚 | ✅ 支持回滚 |
| **配置管理** | ✅ 模板化 | ✅ CRD |
| **Day-2 运维** | ❌ 不支持 | ✅ 核心能力 |
| **备份恢复** | ❌ 需手动 | ✅ 自动 |
| **故障自愈** | ❌ 无 | ✅ 自动 |
| **扩缩容** | ⚠️ 手动触发 | ✅ 自动 |
| **监控集成** | ⚠️ 需配置 | ✅ 内置 |
| **学习曲线** | 低 | 高(需编程) |

**选择建议**:
- **Helm**: 无状态应用、简单有状态应用
- **Operator**: 复杂有状态应用(数据库、中间件)
- **组合使用**: Helm 安装 Operator,Operator 管理应用

---

### 九、常见问题与排查

#### 1. Reconcile 无限循环

**问题**: Operator CPU 占用高,日志不断输出 Reconcile

**原因**: 更新资源时触发了自己的 Watch 事件

**解决**:
```go
// ✅ 正确:只更新 Status(不会触发普通 Watch)
r.Status().Update(ctx, myApp)

// ❌ 错误:更新整个对象会触发 Watch
r.Update(ctx, myApp)

// ✅ 正确:比较后再更新
if !reflect.DeepEqual(found.Spec, desired.Spec) {
    found.Spec = desired.Spec
    r.Update(ctx, found)
}
```

#### 2. Finalizer 导致资源无法删除

**问题**: `kubectl delete` 后资源卡在 Terminating

**排查**:
```bash
# 查看 Finalizer
kubectl get myapp my-resource -o yaml | grep -A5 finalizers

# 强制删除(生产环境慎用)
kubectl patch myapp my-resource -p '{"metadata":{"finalizers":[]}}' --type=merge
```

**原因**: Finalizer 清理逻辑失败或超时

#### 3. RBAC 权限不足

**问题**: `forbidden: User "system:serviceaccount:..." cannot get resource`

**解决**:
```bash
# 检查 ServiceAccount
kubectl get sa -n myapp-system

# 检查 RoleBinding
kubectl get rolebinding,clusterrolebinding | grep myapp

# 更新 RBAC
make manifests
kubectl apply -f config/rbac/
```

---

### 十、总结

Operator 是 K8s 自动化运维的最佳实践,核心要点:

1. **声明式管理**: 通过 CRD 定义期望状态
2. **控制循环**: Controller 持续协调实际状态到期望状态
3. **领域知识封装**: 将运维经验编码到 Operator 中
4. **Day-2 运维**: 自动化备份、升级、扩缩容、故障恢复
5. **可扩展性**: 扩展 K8s API,管理任何类型的应用

面试建议关注:
- 理解 CRD + Controller + Reconcile 的工作原理
- 掌握 Kubebuilder/Operator SDK 的使用
- 了解控制循环的幂等性和声明式设计
- 熟悉 Finalizer、OwnerReference、Status 子资源
- 能对比 Operator 与 Helm 的适用场景
- 了解常见 Operator(Prometheus、etcd、Kafka)的功能
