---
title: Kubernetes 的日志收集（EFK）
tags:
  - Kubernetes
status: robot
class: Kubernetes
slug: kubernetes-log-collection-efk
ref:
---

## 核心要点

- **EFK 架构**: Elasticsearch(存储+检索) + Fluentd/Fluent Bit(采集) + Kibana(可视化)
- **日志分类**: 容器标准输出/错误 → 应用日志文件 → 节点系统日志 → K8s 审计日志
- **采集方式**: DaemonSet 部署采集器,挂载宿主机日志目录
- **数据流**: Pod 日志 → Docker 日志驱动 → 宿主机文件 → Fluentd 解析 → ES 索引 → Kibana 查询
- **优化要点**: 日志轮转、索引生命周期管理、采集过滤、多行日志聚合

---

## 详细回答

### 一、为什么需要集中式日志方案

在 Kubernetes 环境中,传统的日志查看方式面临严峻挑战:

#### 1. 传统方式的局限

```bash
# 只能查看单个 Pod 的日志
kubectl logs my-app-pod-xyz

# Pod 删除后日志丢失
kubectl delete pod my-app-pod-xyz  # 日志永久丢失

# 无法跨多个 Pod 搜索
# 如何查询所有 Pod 中包含 "error" 的日志?
```

#### 2. K8s 带来的新挑战

| 问题 | 影响 |
|------|------|
| **Pod 短暂性** | Pod 重启后日志丢失 |
| **多副本分散** | 无法统一查看同一应用的所有日志 |
| **节点故障** | 节点宕机导致日志无法访问 |
| **日志孤岛** | 日志分散在不同节点/命名空间 |
| **海量数据** | 数千个容器的日志难以检索 |

#### 3. EFK 的解决方案

- **集中存储**: 所有日志聚合到 Elasticsearch
- **持久化**: 即使 Pod 删除,日志依然保留
- **全文检索**: 毫秒级搜索海量日志
- **可视化**: Kibana 提供强大的图表和仪表盘
- **结构化**: 自动解析 JSON 日志,提取字段

---

### 二、EFK 架构详解

#### 1. 整体架构图

```
┌──────────────────────────────────────────────────────┐
│            Kubernetes Cluster                        │
│                                                       │
│  Node 1                    Node 2                    │
│  ┌─────────────┐           ┌─────────────┐          │
│  │ Pod A       │           │ Pod B       │          │
│  │  stdout/    │           │  stdout/    │          │
│  │  stderr     │           │  stderr     │          │
│  └──────┬──────┘           └──────┬──────┘          │
│         │                         │                  │
│         ▼                         ▼                  │
│  /var/log/pods/                                      │
│  namespace_podname_uid/                              │
│  container/0.log                                     │
│         │                         │                  │
│         ▼                         ▼                  │
│  ┌────────────────┐       ┌────────────────┐        │
│  │ Fluentd        │       │ Fluentd        │        │
│  │ DaemonSet      │       │ DaemonSet      │        │
│  │ - 挂载宿主机   │       │ - 挂载宿主机   │        │
│  │ - 解析日志     │       │ - 解析日志     │        │
│  │ - 打标签       │       │ - 打标签       │        │
│  └────────┬───────┘       └────────┬───────┘        │
│           │                        │                 │
└───────────┼────────────────────────┼─────────────────┘
            │                        │
            └────────┬───────────────┘
                     │ HTTP/JSON
                     ▼
          ┌─────────────────────┐
          │  Elasticsearch      │
          │  StatefulSet        │
          │  - 存储索引         │
          │  - 全文检索         │
          │  - 数据分片         │
          └──────────┬──────────┘
                     │ REST API
                     ▼
          ┌─────────────────────┐
          │     Kibana          │
          │  - Web UI           │
          │  - 查询可视化       │
          │  - 仪表盘           │
          └─────────────────────┘
```

#### 2. 核心组件职责

**① Fluentd/Fluent Bit(日志采集器)**
- **部署方式**: DaemonSet(每个节点一个 Pod)
- **功能**:
  - 监听宿主机日志目录 `/var/log/pods/`
  - 解析日志格式(JSON、多行、正则)
  - 提取元数据(Pod 名、命名空间、容器名)
  - 过滤敏感信息
  - 批量发送到 Elasticsearch
- **选择建议**:
  - **Fluentd**: 功能丰富,插件多,但内存占用较高(~100MB)
  - **Fluent Bit**: 轻量高效,内存占用低(~10MB),适合边缘节点

**② Elasticsearch(日志存储与检索)**
- **部署方式**: StatefulSet(有状态应用)
- **功能**:
  - 接收 Fluentd 推送的日志
  - 建立倒排索引(全文检索的基础)
  - 数据分片和副本(高可用)
  - 按时间创建索引(如 `logstash-2025.01.15`)
- **集群角色**:
  - **Master 节点**: 集群管理、索引创建
  - **Data 节点**: 存储数据、执行查询
  - **Coordinating 节点**: 路由请求、聚合结果

**③ Kibana(可视化与查询)**
- **部署方式**: Deployment
- **功能**:
  - Web UI 查询日志
  - KQL(Kibana Query Language)查询语法
  - 创建仪表盘(Dashboard)
  - 日志告警(X-Pack Alert)
  - 索引生命周期管理

---

### 三、部署实践(完整示例)

#### 1. 快速部署(使用 Helm)

```bash
# 1. 添加 Helm 仓库
helm repo add elastic https://helm.elastic.co
helm repo update

# 2. 创建命名空间
kubectl create namespace logging

# 3. 部署 Elasticsearch
helm install elasticsearch elastic/elasticsearch \
  --namespace logging \
  --set replicas=3 \
  --set volumeClaimTemplate.resources.requests.storage=100Gi \
  --set resources.requests.memory=4Gi \
  --set resources.limits.memory=8Gi

# 4. 部署 Kibana
helm install kibana elastic/kibana \
  --namespace logging \
  --set service.type=LoadBalancer

# 5. 部署 Fluentd(使用 DaemonSet)
kubectl apply -f fluentd-daemonset.yaml
```

#### 2. Fluentd DaemonSet 配置

**fluentd-daemonset.yaml**

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: logging
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
- apiGroups: [""]
  resources: ["pods", "namespaces"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluentd
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: logging
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.16-debian-elasticsearch8
        env:
        # Elasticsearch 连接
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch-master.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        # 日志索引配置
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: "k8s-logs"
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT
          value: "true"
        # 性能优化
        - name: FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE
          value: "8M"
        - name: FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH
          value: "32"
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 256Mi
        volumeMounts:
        # 挂载容器日志目录
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        # 自定义配置文件
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          subPath: fluent.conf
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config
        configMap:
          name: fluentd-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
data:
  fluent.conf: |
    # 输入: 监听容器日志
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    # 过滤: 解析 Kubernetes 元数据
    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
      kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
      verify_ssl true
      ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    </filter>

    # 过滤: 排除系统命名空间(可选)
    <filter kubernetes.**>
      @type grep
      <exclude>
        key $.kubernetes.namespace_name
        pattern ^(kube-system|kube-public)$
      </exclude>
    </filter>

    # 过滤: 解析应用 JSON 日志
    <filter kubernetes.**>
      @type parser
      key_name log
      reserve_data true
      remove_key_name_field true
      <parse>
        @type json
      </parse>
    </filter>

    # 输出: 发送到 Elasticsearch
    <match kubernetes.**>
      @type elasticsearch
      @id out_es
      @log_level info
      include_tag_key true
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
      scheme http
      # 索引名称: k8s-logs-YYYY.MM.DD
      logstash_format true
      logstash_prefix k8s-logs
      # 缓冲配置
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_interval 5s
        retry_max_interval 30s
        chunk_limit_size 8M
        queue_limit_length 32
        overflow_action block
      </buffer>
    </match>
```

#### 3. Elasticsearch 持久化配置

```yaml
apiVersion: v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs  # 根据云厂商调整
parameters:
  type: gp3
  iops: "3000"
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
        env:
        - name: cluster.name
          value: "k8s-logs"
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms4g -Xmx4g"  # 堆内存设置为物理内存的一半
        resources:
          limits:
            memory: 8Gi
            cpu: 2
          requests:
            memory: 4Gi
            cpu: 1
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 200Gi
```

---

### 四、日志采集的层次

#### 1. 容器标准输出/错误(最常用)

**应用直接打印到 stdout/stderr**

```go
// Go 应用示例
package main

import (
    "log"
    "encoding/json"
)

type LogEntry struct {
    Level   string `json:"level"`
    Message string `json:"message"`
    TraceID string `json:"trace_id"`
}

func main() {
    // 结构化日志输出(JSON 格式)
    entry := LogEntry{
        Level:   "INFO",
        Message: "User login success",
        TraceID: "abc123",
    }
    data, _ := json.Marshal(entry)
    log.Println(string(data))
}
```

**日志路径**: `/var/log/pods/<namespace>_<pod-name>_<pod-uid>/<container-name>/0.log`

Fluentd 自动解析并添加元数据:
```json
{
  "log": "{\"level\":\"INFO\",\"message\":\"User login success\",\"trace_id\":\"abc123\"}",
  "stream": "stdout",
  "time": "2025-01-15T10:30:00Z",
  "kubernetes": {
    "pod_name": "my-app-7d8f5c9b-xyz",
    "namespace_name": "production",
    "container_name": "app",
    "labels": {
      "app": "my-app",
      "version": "v1.2.3"
    }
  }
}
```

#### 2. 应用日志文件

**应用将日志写入文件**(如 `/app/logs/app.log`)

需要使用 **Sidecar 模式**:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  containers:
  # 主应用容器
  - name: app
    image: my-app:1.0
    volumeMounts:
    - name: logs
      mountPath: /app/logs

  # Sidecar 容器: 将日志重定向到 stdout
  - name: log-shipper
    image: busybox
    command: ["sh", "-c", "tail -f /app/logs/*.log"]
    volumeMounts:
    - name: logs
      mountPath: /app/logs

  volumes:
  - name: logs
    emptyDir: {}
```

#### 3. 节点系统日志

采集宿主机系统日志:

```yaml
# Fluentd 挂载宿主机目录
volumeMounts:
- name: systemd-log
  mountPath: /var/log/journal
  readOnly: true
- name: syslog
  mountPath: /var/log/syslog
  readOnly: true
```

#### 4. Kubernetes 审计日志

启用 API Server 审计日志:

```yaml
# kube-apiserver 启动参数
--audit-log-path=/var/log/audit.log
--audit-policy-file=/etc/kubernetes/audit-policy.yaml
--audit-log-maxage=30
--audit-log-maxbackup=10
--audit-log-maxsize=100
```

**审计策略示例**:
```yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata
  resources:
  - group: ""
    resources: ["secrets"]
- level: RequestResponse
  verbs: ["create", "update", "delete"]
```

---

### 五、Kibana 日志查询与分析

#### 1. 创建索引模式

访问 Kibana → Management → Index Patterns

- 索引模式: `k8s-logs-*`
- 时间字段: `@timestamp`

#### 2. KQL 查询语法

**基础查询**
```kql
# 查询特定命名空间
kubernetes.namespace_name: "production"

# 查询特定 Pod
kubernetes.pod_name: "my-app-*"

# 查询错误日志
level: "ERROR" OR log: "error"

# 组合条件
kubernetes.namespace_name: "production" AND level: "ERROR" AND NOT log: "connection reset"

# 时间范围(在 UI 右上角选择,如 Last 15 minutes)
```

**高级查询**
```kql
# 查询包含 traceID 的日志
trace_id: *

# 正则表达式
log: /timeout.*retry/

# 范围查询
response_time > 1000

# 字段存在性
_exists_: user_id
```

#### 3. 创建可视化面板

**① 错误率统计**
- Visualization Type: Line Chart
- Y-Axis: Count
- X-Axis: Date Histogram (@timestamp)
- Filter: `level: "ERROR"`

**② Top 10 错误消息**
- Visualization Type: Data Table
- Metrics: Count
- Buckets: Terms Aggregation on `message.keyword`
- Size: 10

**③ 命名空间日志量**
- Visualization Type: Pie Chart
- Metrics: Count
- Buckets: Terms on `kubernetes.namespace_name.keyword`

#### 4. 告警配置(使用 X-Pack)

```json
PUT _watcher/watch/error-alert
{
  "trigger": {
    "schedule": {
      "interval": "5m"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["k8s-logs-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                {"match": {"level": "ERROR"}},
                {"range": {"@timestamp": {"gte": "now-5m"}}}
              ]
            }
          }
        }
      }
    }
  },
  "condition": {
    "compare": {
      "ctx.payload.hits.total": {
        "gte": 100
      }
    }
  },
  "actions": {
    "send_slack": {
      "webhook": {
        "method": "POST",
        "url": "https://hooks.slack.com/services/xxx",
        "body": "5 分钟内产生了 {{ctx.payload.hits.total}} 条错误日志"
      }
    }
  }
}
```

---

### 六、生产环境最佳实践

#### 1. 索引生命周期管理(ILM)

**问题**: 日志无限增长导致磁盘爆满

**解决方案**: 自动删除旧索引

```json
PUT _ilm/policy/k8s-logs-policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "1d"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          },
          "forcemerge": {
            "max_num_segments": 1
          }
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

应用到索引模板:
```json
PUT _index_template/k8s-logs-template
{
  "index_patterns": ["k8s-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 3,
      "number_of_replicas": 1,
      "index.lifecycle.name": "k8s-logs-policy",
      "index.lifecycle.rollover_alias": "k8s-logs"
    }
  }
}
```

#### 2. 日志过滤优化

**减少无用日志**
```xml
# Fluentd 配置
<filter kubernetes.**>
  @type grep
  <exclude>
    key log
    pattern /healthcheck|readiness|liveness/
  </exclude>
</filter>

# 排除高频低价值日志
<filter kubernetes.**>
  @type grep
  <exclude>
    key kubernetes.container_name
    pattern ^(istio-proxy|linkerd-proxy)$
  </exclude>
</filter>
```

#### 3. 多行日志聚合

**问题**: Java 堆栈跟踪被拆分成多条日志

```
2025-01-15 10:00:00 ERROR - Exception occurred
java.lang.NullPointerException
    at com.example.App.main(App.java:10)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
```

**解决方案**: 使用 multiline 插件

```xml
<source>
  @type tail
  path /var/log/containers/*.log
  <parse>
    @type multiline
    format_firstline /^\d{4}-\d{2}-\d{2}/
    format1 /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?<level>\w+) - (?<message>.*)/
  </parse>
</source>
```

#### 4. 性能优化

**① Fluentd 资源限制**
```yaml
resources:
  limits:
    memory: 512Mi  # 防止内存泄漏
    cpu: 500m
  requests:
    memory: 256Mi
    cpu: 100m
```

**② Elasticsearch 调优**
```yaml
# JVM 堆内存(设置为物理内存的 50%)
ES_JAVA_OPTS: "-Xms8g -Xmx8g"

# 刷新间隔(默认 1s,可调整为 30s)
index.refresh_interval: 30s

# 副本数(生产环境至少 1)
number_of_replicas: 1
```

**③ 批量发送**
```xml
<match kubernetes.**>
  @type elasticsearch
  <buffer>
    flush_interval 5s        # 每 5 秒刷新一次
    chunk_limit_size 8M      # 每批最多 8MB
    queue_limit_length 32    # 队列深度
  </buffer>
</match>
```

---

### 七、常见问题排查

#### 1. 日志未出现在 Kibana

**排查步骤**:
```bash
# 1. 检查 Fluentd Pod 状态
kubectl -n logging get pods -l app=fluentd

# 2. 查看 Fluentd 日志
kubectl -n logging logs -l app=fluentd --tail=100

# 3. 验证 Elasticsearch 连接
kubectl -n logging exec -it <fluentd-pod> -- curl http://elasticsearch-master:9200/_cluster/health

# 4. 检查索引是否创建
kubectl -n logging exec -it <fluentd-pod> -- curl http://elasticsearch-master:9200/_cat/indices?v
```

**常见原因**:
- Fluentd 无法连接 Elasticsearch(网络策略/Service 错误)
- RBAC 权限不足,无法读取 Pod 元数据
- 日志格式解析失败(检查 Fluentd 配置)

#### 2. Elasticsearch 磁盘爆满

```bash
# 查看索引大小
GET _cat/indices?v&s=store.size:desc

# 手动删除旧索引
DELETE /k8s-logs-2024.01.01

# 检查 ILM 策略
GET _ilm/policy/k8s-logs-policy
```

#### 3. 查询性能差

**优化建议**:
- 缩小时间范围(避免查询全部索引)
- 使用字段过滤而非全文搜索
- 增加 Data 节点
- 使用 `keyword` 字段进行聚合(而非 `text`)

---

### 八、现代化替代方案

#### 1. EFK vs Loki

| 维度 | EFK | Loki (Grafana) |
|------|-----|----------------|
| **索引方式** | 全文索引(重) | 仅索引标签(轻) |
| **存储成本** | 高 | 低(约 1/10) |
| **查询语法** | KQL | LogQL(类 PromQL) |
| **适用场景** | 复杂全文检索 | 标签化查询 |
| **内存占用** | 高 | 低 |

**Loki 架构**:
```
Promtail(采集) → Loki(存储) → Grafana(查询)
```

**LogQL 示例**:
```logql
{namespace="production", app="my-app"} |= "error" | json | level="ERROR"
```

#### 2. EFK vs OpenSearch

OpenSearch 是 Elasticsearch 的开源分支(Apache 2.0 许可):
- 完全兼容 Elasticsearch API
- 免费的告警和安全功能
- 活跃的社区支持

---

### 九、总结

EFK 是 Kubernetes 日志管理的经典方案,核心优势:

1. **成熟稳定**: 经过大规模生产验证
2. **强大检索**: Elasticsearch 支持复杂全文搜索
3. **生态丰富**: 插件、集成方案众多
4. **可视化强**: Kibana 提供丰富的分析能力

面试建议关注:
- 理解日志采集的链路(容器 → Docker → 宿主机 → Fluentd → ES)
- 掌握 Fluentd 的核心配置(source、filter、match)
- 了解 Elasticsearch 的索引机制和性能优化
- 能对比 EFK 与 Loki 等方案的优劣
- 熟悉生产环境的 ILM、告警、多行日志处理
