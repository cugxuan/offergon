---
title: Kubernetes 的调度策略（Scheduler）
tags:
  - Kubernetes
status: robot
class: Kubernetes
slug: kubernetes-scheduler-strategy
ref:
---

## 核心要点

**调度流程两阶段**：Filtering（过滤不符合条件的节点）+ Scoring（对候选节点打分并选最优）；**核心策略**：资源调度（requests/limits）、亲和性（nodeAffinity/podAffinity）、反亲和性（podAntiAffinity）、污点与容忍（Taints & Tolerations）、优先级抢占（PriorityClass）；**高级特性**：拓扑分布约束（TopologySpreadConstraints）、自定义调度器、调度插件框架

---

## 详细回答

### 一、Scheduler 是什么？

**kube-scheduler** 是 Kubernetes 控制平面的核心组件，负责为**新创建的 Pod 选择最合适的节点**（Node）。

**核心职责**：
- 监听 API Server 中未调度的 Pod（`spec.nodeName` 为空）
- 根据调度策略筛选和评分节点
- 将 Pod 绑定到最优节点（更新 `spec.nodeName`）

**调度器不负责**：
- Pod 的实际创建和启动（由 Kubelet 负责）
- 已调度 Pod 的重新调度（除非 Pod 被删除重建）

---

### 二、调度流程（Scheduling Workflow）

Scheduler 采用**两阶段调度**机制：

#### **阶段 1：Filtering（预选 / 过滤）**

**目标**：筛选出**可以**运行 Pod 的节点。

**过滤条件示例**：
- **资源充足**：节点剩余 CPU/内存满足 Pod 的 `requests`
- **端口可用**：Pod 需要的 `hostPort` 未被占用
- **卷可用**：PersistentVolume 可以挂载到该节点
- **节点选择器**：节点标签匹配 `nodeSelector`
- **亲和性**：满足 `nodeAffinity` 和 `podAffinity` 规则
- **污点容忍**：Pod 容忍节点的 `Taints`

**结果**：
- 如果**没有节点**通过过滤 → Pod 保持 **Pending** 状态
- 如果有节点通过 → 进入评分阶段

---

#### **阶段 2：Scoring（优选 / 打分）**

**目标**：对候选节点**打分**，选择得分最高的节点。

**评分维度示例**：
- **资源均衡**（LeastRequestedPriority）：优先选择资源使用率低的节点，避免单节点过载
- **亲和性权重**：满足软亲和性（preferred）的节点加分
- **数据本地性**（ImageLocality）：节点已有容器镜像，减少拉取时间
- **拓扑分布**：平衡 Pod 在可用区/节点的分布

**最终选择**：
- 选择得分最高的节点
- 如果多个节点得分相同 → 随机选择一个

---

#### **阶段 3：Binding（绑定）**

Scheduler 将 Pod 与节点的绑定关系写入 API Server，Kubelet 监听后开始创建容器。

---

### 三、核心调度策略

#### 1. **资源调度（Resource Scheduling）**

**资源请求（requests）与限制（limits）**：
```yaml
resources:
  requests:      # 调度依据：确保节点至少有这么多可用资源
    cpu: "500m"
    memory: "512Mi"
  limits:        # 运行时限制：容器最多使用这么多资源
    cpu: "1"
    memory: "1Gi"
```

**调度行为**：
- Scheduler 只考虑 `requests`（不考虑 `limits`）
- 节点**可分配资源**（Allocatable）= 总资源 - 系统保留 - 已分配 requests
- 如果节点资源不足，Pod 会 Pending（触发事件：`Insufficient cpu/memory`）

**最佳实践**：
- 设置合理的 `requests`：过低可能导致节点过载，过高导致资源浪费
- `limits` > `requests`：允许应用突发使用资源
- QoS 类别：
  - **Guaranteed**（requests = limits）：最高优先级，OOM 时最后被杀
  - **Burstable**（requests < limits）：中等优先级
  - **BestEffort**（未设置 requests/limits）：最低优先级，资源不足时优先被驱逐

---

#### 2. **节点选择器（Node Selector）**

**简单的节点筛选机制**，基于标签精确匹配。

**示例**：
```yaml
spec:
  nodeSelector:
    disktype: ssd
    region: us-west
```

**使用场景**：
- 将 Pod 调度到特定类型的节点（如 GPU 节点、高性能节点）
- 按地域部署应用

**局限性**：
- 只能做**精确匹配**（不支持逻辑运算）
- 被更强大的 `nodeAffinity` 取代

---

#### 3. **节点亲和性（Node Affinity）**

**nodeAffinity** 是节点选择器的增强版，支持更复杂的逻辑表达式。

**两种类型**：
- **requiredDuringSchedulingIgnoredDuringExecution**（硬约束）：
  - 必须满足条件，否则 Pod 无法调度
- **preferredDuringSchedulingIgnoredDuringExecution**（软约束）：
  - 尽量满足，但不强制（不满足时也可调度，只是优先级低）

**示例**：
```yaml
affinity:
  nodeAffinity:
    # 硬约束：必须在 us-west 或 us-east
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: region
          operator: In
          values: ["us-west", "us-east"]
    # 软约束：优先选择 SSD 节点
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 80  # 权重（1-100）
      preference:
        matchExpressions:
        - key: disktype
          operator: In
          values: ["ssd"]
```

**操作符**：
- `In`：标签值在列表中
- `NotIn`：标签值不在列表中
- `Exists`：标签存在（不关心值）
- `DoesNotExist`：标签不存在
- `Gt` / `Lt`：大于/小于（用于数值标签）

---

#### 4. **Pod 亲和性（Pod Affinity）**

**将 Pod 调度到与其他 Pod 相邻的节点**（共享拓扑域，如同一节点、同一可用区）。

**应用场景**：
- **数据本地性**：将应用和缓存/数据库部署在同一节点，减少网络延迟
- **关联服务**：前端和后端部署在同一可用区

**示例**：
```yaml
affinity:
  podAffinity:
    # 硬约束：必须与标签 app=database 的 Pod 在同一节点
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values: ["database"]
      topologyKey: kubernetes.io/hostname  # 拓扑域：节点级别
```

**topologyKey 常用值**：
- `kubernetes.io/hostname`：同一节点
- `topology.kubernetes.io/zone`：同一可用区
- `topology.kubernetes.io/region`：同一地域

---

#### 5. **Pod 反亲和性（Pod Anti-Affinity）**

**将 Pod 调度到与其他 Pod 分散的节点**，避免单点故障。

**应用场景**：
- **高可用**：将同一应用的多个副本分散到不同节点/可用区
- **避免资源竞争**：将资源密集型 Pod 分散部署

**示例**：
```yaml
affinity:
  podAntiAffinity:
    # 硬约束：不能与同应用的 Pod 在同一节点
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values: ["web"]
      topologyKey: kubernetes.io/hostname
    # 软约束：尽量不在同一可用区
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values: ["web"]
        topologyKey: topology.kubernetes.io/zone
```

---

#### 6. **污点（Taints）与容忍（Tolerations）**

**Taints**：节点打上"标记"，拒绝不容忍该污点的 Pod 调度上来。

**Tolerations**：Pod 声明可以"容忍"某些污点，从而可以调度到有污点的节点。

**应用场景**：
- **专用节点**：将节点保留给特定类型的 Pod（如 GPU 节点只给 AI 任务）
- **故障隔离**：将有问题的节点标记为污点，避免新 Pod 调度上去

---

**设置污点**：
```bash
kubectl taint nodes node1 key=value:NoSchedule
```

**三种效果（Effect）**：
- **NoSchedule**：不允许调度新 Pod（已有 Pod 不受影响）
- **PreferNoSchedule**：尽量不调度（软约束）
- **NoExecute**：不仅不调度新 Pod，还会驱逐已有 Pod（除非 Pod 容忍该污点）

---

**Pod 容忍污点**：
```yaml
tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"
```

**操作符**：
- `Equal`：精确匹配 key 和 value
- `Exists`：只要 key 存在即可（不关心 value）

---

**特殊污点（系统自动添加）**：
- `node.kubernetes.io/not-ready`：节点未就绪
- `node.kubernetes.io/unreachable`：节点不可达
- `node.kubernetes.io/memory-pressure`：内存压力
- `node.kubernetes.io/disk-pressure`：磁盘压力

**默认容忍时间**：
```yaml
tolerations:
- key: "node.kubernetes.io/not-ready"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 300  # 节点故障后 300 秒才驱逐 Pod
```

---

#### 7. **优先级与抢占（Priority & Preemption）**

**PriorityClass** 定义 Pod 的优先级，**高优先级 Pod 可以抢占低优先级 Pod 的资源**。

**应用场景**：
- 关键服务优先调度（如支付服务 > 日志收集）
- 资源不足时，驱逐低优先级 Pod 为高优先级 Pod 腾出空间

---

**定义 PriorityClass**：
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000  # 值越大优先级越高（系统最高 20 亿）
globalDefault: false
description: "High priority for critical services"
```

**在 Pod 中使用**：
```yaml
spec:
  priorityClassName: high-priority
```

---

**抢占流程**：
1. Pod 因资源不足无法调度
2. Scheduler 尝试抢占：选择一个节点，驱逐低优先级 Pod
3. 等待被驱逐 Pod 的宽限期结束
4. 调度高优先级 Pod 到该节点

**注意**：
- 抢占不是即时的（需等待宽限期）
- 可通过 `preemptionPolicy: Never` 禁用抢占

---

#### 8. **拓扑分布约束（Topology Spread Constraints）**

**自动平衡 Pod 在不同拓扑域（节点/可用区/地域）的分布**，避免倾斜。

**示例**：
```yaml
topologySpreadConstraints:
- maxSkew: 1  # 最大允许的不均衡程度
  topologyKey: topology.kubernetes.io/zone  # 拓扑域：可用区
  whenUnsatisfiable: DoNotSchedule  # 无法满足时：不调度（或 ScheduleAnyway）
  labelSelector:
    matchLabels:
      app: web
```

**解释**：
- 确保 `app=web` 的 Pod 在各可用区的数量差异不超过 1
- 例如：3 个可用区，9 个副本 → 每个可用区 3 个（或 3/3/2）

---

### 四、高级特性

#### 1. **自定义调度器**

Kubernetes 支持运行**多个调度器**，不同 Pod 可指定不同调度器。

**使用场景**：
- 特殊调度逻辑（如批处理任务的 Gang Scheduling）
- A/B 测试新调度算法

**指定调度器**：
```yaml
spec:
  schedulerName: my-custom-scheduler
```

---

#### 2. **调度框架（Scheduling Framework）**

Kubernetes 1.15+ 引入**可扩展调度框架**，允许通过插件自定义调度行为。

**插件扩展点**：
- **PreFilter**：预处理（如检查 PVC 是否就绪）
- **Filter**：过滤节点
- **Score**：节点打分
- **Reserve**：资源预留
- **Bind**：绑定 Pod 到节点

---

#### 3. **节点资源管理**

**节点可分配资源（Allocatable）计算**：
```
Allocatable = Capacity - Reserved(系统保留) - Eviction Threshold(驱逐阈值)
```

**查看节点资源**：
```bash
kubectl describe node <node-name>
```

---

### 五、常见问题与面试追问

#### **Q1：Pod 一直 Pending，如何排查？**

**步骤**：
1. 查看事件：`kubectl describe pod <pod-name>`
2. 常见原因及解决：
   - `Insufficient cpu/memory`：节点资源不足 → 调整 requests 或扩容节点
   - `No nodes are available`：没有满足亲和性/污点的节点 → 检查调度策略
   - `PersistentVolumeClaim is not bound`：PVC 未绑定 → 检查 PV 配置

---

#### **Q2：如何保证 Pod 高可用？**

1. **多副本部署**：`replicas >= 3`
2. **反亲和性**：将副本分散到不同节点/可用区
3. **PodDisruptionBudget**：限制同时被驱逐的 Pod 数量
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
spec:
  minAvailable: 2  # 至少保留 2 个 Pod 运行
  selector:
    matchLabels:
      app: web
```

---

#### **Q3：Affinity 和 NodeSelector 的区别？**

| 特性 | NodeSelector | Node Affinity |
|------|-------------|--------------|
| 匹配方式 | 精确匹配 | 支持逻辑运算符 |
| 软约束 | 不支持 | 支持（preferred） |
| 灵活性 | 低 | 高 |

**推荐**：使用 NodeAffinity 替代 NodeSelector。

---

#### **Q4：如何实现 GPU 节点专用调度？**

**方案 1：Taints + Tolerations**
```bash
# 给 GPU 节点打污点
kubectl taint nodes gpu-node gpu=true:NoSchedule
```
```yaml
# GPU Pod 容忍污点
tolerations:
- key: "gpu"
  operator: "Equal"
  value: "true"
  effect: "NoSchedule"
```

**方案 2：Node Affinity**
```yaml
nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchExpressions:
      - key: accelerator
        operator: In
        values: ["nvidia-tesla-v100"]
```

---

#### **Q5：什么是 Gang Scheduling（批调度）？**

**场景**：分布式训练任务需要多个 Pod 同时启动（如 TensorFlow 的 Worker 和 PS）。

**问题**：默认调度器逐个调度 Pod，可能导致部分 Pod 启动而资源不足以启动剩余 Pod，造成死锁。

**解决方案**：
- 使用支持 Gang Scheduling 的调度器（如 Volcano、Kube-batch）
- 确保所有 Pod 要么全部调度，要么全部不调度

---

### 六、调度器最佳实践

1. **设置资源请求（requests）**：
   - 所有 Pod 都应设置 `requests`，帮助 Scheduler 做出合理决策
   - 避免过高（浪费资源）或过低（节点过载）

2. **利用亲和性实现高可用**：
   - 使用 `podAntiAffinity` 分散副本
   - 使用 `topologySpreadConstraints` 平衡分布

3. **合理使用污点与容忍**：
   - 将特殊节点（GPU、高性能）保留给特定工作负载
   - 避免过度使用污点导致资源浪费

4. **配置 PriorityClass**：
   - 关键服务设置高优先级
   - 避免所有 Pod 都设置高优先级（失去优先级意义）

5. **监控调度延迟**：
   - 使用 Prometheus 监控指标：`scheduler_scheduling_duration_seconds`
   - 调度延迟过高可能是节点过多或调度策略过于复杂

---

### 七、总结

Kubernetes Scheduler 的核心要点：

1. **两阶段调度**：Filtering（过滤）+ Scoring（打分）
2. **核心策略**：
   - 资源调度（requests/limits）
   - 亲和性/反亲和性（Affinity/Anti-Affinity）
   - 污点与容忍（Taints/Tolerations）
   - 优先级抢占（Priority/Preemption）
3. **高级特性**：
   - 拓扑分布约束（Topology Spread Constraints）
   - 自定义调度器
   - 调度框架插件
4. **最佳实践**：
   - 合理设置资源请求
   - 利用反亲和性保证高可用
   - 为关键服务配置优先级

掌握调度策略有助于优化资源利用率、提升应用性能和可靠性。
