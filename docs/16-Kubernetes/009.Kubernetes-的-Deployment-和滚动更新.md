---
title: Kubernetes 的 Deployment 和滚动更新
tags:
  - Kubernetes
status: robot
class: Kubernetes
slug: kubernetes-deployment-rolling-update
ref:
---

## 核心要点

**Deployment 是 K8s 最常用的无状态应用管理资源,提供声明式更新、滚动发布、版本回滚、自动扩缩容等能力。滚动更新通过 maxSurge(最大新增 Pod)和 maxUnavailable(最大不可用 Pod)控制发布节奏,实现零停机部署。**

---

## 详细回答

### 一、Deployment 是什么?为什么需要它?

在 Kubernetes 中,Pod 是最小的调度单元,但手动管理 Pod 存在诸多问题:

- **无法自动重启**:Pod 删除或节点故障后不会自动恢复
- **难以扩缩容**:手动创建/删除多个 Pod 繁琐且易错
- **无法滚动更新**:应用升级需手动替换 Pod,导致服务中断

**Deployment 解决的核心问题**:

1. **声明式管理**:定义期望状态(副本数、镜像版本等),K8s 自动维护
2. **自动恢复**:Pod 异常退出时自动重建
3. **滚动更新**:逐步替换旧 Pod,实现零停机部署
4. **版本回滚**:记录发布历史,快速回退到之前版本
5. **水平扩缩容**:通过修改 replicas 动态调整实例数

### 二、Deployment 架构

**层级关系**:

```
Deployment(声明期望状态)
    ↓ 管理
ReplicaSet(维护副本数,版本管理)
    ↓ 管理
Pod(实际运行的容器)
```

**关键概念**:

- **Deployment**:顶层控制器,负责更新策略和版本管理
- **ReplicaSet**(RS):确保指定数量的 Pod 副本运行,每次更新创建新 RS
- **Pod**:实际运行的应用实例

**为什么需要 ReplicaSet?**

- Deployment 每次更新都会创建新的 ReplicaSet,保留旧 RS 用于回滚
- RS 负责实际的 Pod 创建和销毁,Deployment 只管理 RS

### 三、Deployment 定义与操作

#### 1. 创建 Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: default
  labels:
    app: nginx
spec:
  replicas: 3  # 副本数
  selector:    # 选择器(必须匹配 template.metadata.labels)
    matchLabels:
      app: nginx
  template:    # Pod 模板
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
```

**关键字段**:

- **replicas**:期望的 Pod 副本数
- **selector**:用于匹配 Pod 的标签选择器(一旦创建不可修改)
- **template**:Pod 模板,定义 Pod 的规格(镜像、资源限制等)

#### 2. 常用操作命令

```bash
# 创建 Deployment
kubectl apply -f deployment.yaml

# 查看 Deployment 状态
kubectl get deployments
kubectl describe deployment nginx-deployment

# 查看 ReplicaSet(每个版本对应一个 RS)
kubectl get replicasets
kubectl get rs -l app=nginx

# 查看 Pod
kubectl get pods -l app=nginx

# 扩缩容
kubectl scale deployment nginx-deployment --replicas=5

# 查看滚动更新状态
kubectl rollout status deployment nginx-deployment

# 查看发布历史
kubectl rollout history deployment nginx-deployment

# 回滚到上一个版本
kubectl rollout undo deployment nginx-deployment

# 回滚到指定版本
kubectl rollout undo deployment nginx-deployment --to-revision=2

# 暂停/恢复滚动更新
kubectl rollout pause deployment nginx-deployment
kubectl rollout resume deployment nginx-deployment

# 删除 Deployment(会级联删除 RS 和 Pod)
kubectl delete deployment nginx-deployment
```

### 四、滚动更新机制详解

#### 1. 滚动更新流程

**场景**:将 nginx 从 1.21 升级到 1.22

```bash
kubectl set image deployment/nginx-deployment nginx=nginx:1.22
```

**更新过程**:

1. **创建新 ReplicaSet**:Deployment 创建新的 RS(nginx:1.22)
2. **逐步扩容新 RS**:新 RS 的 Pod 数从 0 → 1 → 2 → 3
3. **同时缩容旧 RS**:旧 RS(nginx:1.21)的 Pod 数从 3 → 2 → 1 → 0
4. **健康检查**:每个新 Pod 必须通过 readinessProbe 才算就绪
5. **完成更新**:所有 Pod 替换完成,旧 RS 保留(副本数为 0)

**可视化流程**:

```
旧 RS(nginx:1.21):  [Pod1] [Pod2] [Pod3]  → [Pod2] [Pod3]  → [Pod3]  → []
新 RS(nginx:1.22):  []                     → [Pod4]          → [Pod4] [Pod5] → [Pod4] [Pod5] [Pod6]
```

#### 2. 滚动更新策略配置

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 10
  strategy:
    type: RollingUpdate  # 更新类型(默认)
    rollingUpdate:
      maxSurge: 2         # 最多允许超出 replicas 的 Pod 数(25%)
      maxUnavailable: 1   # 最多允许不可用的 Pod 数(25%)
  minReadySeconds: 10     # Pod 就绪后等待时间,防止误判
  revisionHistoryLimit: 10  # 保留的历史版本数(用于回滚)
  progressDeadlineSeconds: 600  # 更新超时时间(10 分钟)
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.22
        readinessProbe:  # 就绪探针(关键!)
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
```

**关键参数解析**:

##### maxSurge(最大新增 Pod 数)

- **作用**:控制新 Pod 创建速度,避免资源不足
- **值**:整数(如 2)或百分比(如 25%)
- **示例**:replicas=10,maxSurge=2
  - 更新时最多同时存在 12 个 Pod(10 + 2)
  - 新 RS 最多比目标多 2 个 Pod

##### maxUnavailable(最大不可用 Pod 数)

- **作用**:控制旧 Pod 删除速度,保证服务可用性
- **值**:整数(如 1)或百分比(如 25%)
- **示例**:replicas=10,maxUnavailable=1
  - 更新时至少保持 9 个 Pod 可用(10 - 1)
  - 旧 RS 最多比目标少 1 个 Pod

##### 两者配合效果

| replicas | maxSurge | maxUnavailable | 更新时 Pod 范围 | 说明                       |
| -------- | -------- | -------------- | --------------- | -------------------------- |
| 10       | 2        | 1              | 9 ~ 12          | 快速更新,资源消耗中等      |
| 10       | 0        | 1              | 9 ~ 10          | 慢速更新,节省资源          |
| 10       | 3        | 0              | 10 ~ 13         | 高可用更新,资源消耗大      |
| 10       | 1        | 1              | 9 ~ 11          | 平衡更新(默认 25%)         |

**推荐配置**:

- **生产环境**:maxSurge=25%,maxUnavailable=0(保证服务不中断)
- **资源受限**:maxSurge=0,maxUnavailable=25%(避免资源超限)
- **快速发布**:maxSurge=50%,maxUnavailable=25%(加速更新)

##### minReadySeconds(最小就绪时间)

- **作用**:Pod 就绪后额外等待时间,防止应用假就绪(如启动后立即崩溃)
- **场景**:Java 应用启动慢,需预热流量
- **示例**:minReadySeconds=30,表示 Pod 就绪后等待 30 秒才算真正可用

##### revisionHistoryLimit(历史版本保留数)

- **默认值**:10
- **作用**:保留的 ReplicaSet 数量,用于回滚
- **建议**:生产环境设为 5~10,避免占用过多 etcd 空间

#### 3. 更新策略类型

| 策略类型       | 说明                                 | 适用场景           |
| -------------- | ------------------------------------ | ------------------ |
| **RollingUpdate**(默认)| 逐步替换 Pod,零停机部署        | 无状态应用(推荐)   |
| **Recreate**   | 先删除所有旧 Pod,再创建新 Pod       | 有状态应用或测试环境 |

**Recreate 示例**:

```yaml
spec:
  strategy:
    type: Recreate  # 重建策略
```

- **优点**:简单粗暴,不会出现新旧版本同时运行
- **缺点**:服务中断(所有 Pod 被删除后才创建新 Pod)

### 五、健康检查与滚动更新

**滚动更新能否成功,关键在于健康检查**。

#### 1. readinessProbe(就绪探针)

决定 Pod 是否接入 Service 流量,未就绪的 Pod 不会被标记为可用。

```yaml
spec:
  containers:
  - name: app
    image: myapp:2.0
    readinessProbe:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 10  # 容器启动后等待 10 秒再探测
      periodSeconds: 5         # 每 5 秒探测一次
      timeoutSeconds: 3        # 探测超时时间
      successThreshold: 1      # 连续 1 次成功才算就绪
      failureThreshold: 3      # 连续 3 次失败才算未就绪
```

**如果不配置 readinessProbe 会怎样?**

- Pod 一启动就被标记为 Ready,即使应用还在初始化
- 流量立即导入,导致大量 502 错误
- 滚动更新可能误判为成功,实际新版本不可用

#### 2. livenessProbe(存活探针)

决定是否重启容器,与滚动更新无直接关系,但能防止死锁。

```yaml
spec:
  containers:
  - name: app
    image: myapp:2.0
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 30  # 启动慢的应用设置久一点
      periodSeconds: 10
```

#### 3. startupProbe(启动探针,K8s 1.16+)

用于启动慢的应用(如 Java),避免被 livenessProbe 误杀。

```yaml
spec:
  containers:
  - name: java-app
    image: spring-boot-app:2.0
    startupProbe:
      httpGet:
        path: /actuator/health
        port: 8080
      initialDelaySeconds: 0
      periodSeconds: 10
      failureThreshold: 30  # 最多等待 300 秒(30 * 10)
    livenessProbe:
      httpGet:
        path: /actuator/health
        port: 8080
      periodSeconds: 10
      failureThreshold: 3
```

**探针配置原则**:

- **readinessProbe**:必须配置,决定流量接入
- **livenessProbe**:可选,防止死锁
- **startupProbe**:启动慢的应用必配,避免误杀

### 六、版本管理与回滚

#### 1. 查看发布历史

```bash
kubectl rollout history deployment nginx-deployment
```

输出示例:

```
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment/nginx-deployment nginx=nginx:1.22
3         kubectl apply -f deployment.yaml
```

**记录变更原因**(CHANGE-CAUSE):

```bash
# 方法 1:使用 --record(已废弃,K8s 1.28+)
kubectl set image deployment/nginx-deployment nginx=nginx:1.23 --record

# 方法 2:使用 annotation(推荐)
kubectl annotate deployment nginx-deployment \
  kubernetes.io/change-cause="升级到 nginx 1.23 修复安全漏洞"
```

#### 2. 快速回滚

```bash
# 回滚到上一个版本
kubectl rollout undo deployment nginx-deployment

# 回滚到指定版本
kubectl rollout undo deployment nginx-deployment --to-revision=2

# 查看指定版本详情
kubectl rollout history deployment nginx-deployment --revision=2
```

**回滚原理**:

- 每个 Deployment 更新都会创建新的 ReplicaSet
- 回滚时将旧 RS 的 replicas 设为目标值,新 RS 设为 0
- 速度快(秒级),因为旧 RS 和 Pod 模板都已存在

### 七、灰度发布(金丝雀发布)

**需求**:先发布 10% 流量验证新版本,无问题再全量发布。

#### 方法 1:手动控制(简单但不够精确)

```bash
# 1. 暂停自动更新
kubectl rollout pause deployment nginx-deployment

# 2. 更新镜像(只会创建少量新 Pod)
kubectl set image deployment nginx-deployment nginx=nginx:1.23

# 3. 手动控制新版本 Pod 数量(如 10%)
kubectl scale deployment nginx-deployment --replicas=1 # 假设原来 10 个副本,新版本先跑 1 个

# 4. 验证新版本无问题后,恢复更新
kubectl rollout resume deployment nginx-deployment
```

#### 方法 2:使用 Ingress Canary(Nginx Ingress)

通过 Ingress 注解控制流量分配:

```yaml
# 生产版本 Service
apiVersion: v1
kind: Service
metadata:
  name: myapp-stable
spec:
  selector:
    app: myapp
    version: v1
  ports:
  - port: 80

---
# 金丝雀版本 Service
apiVersion: v1
kind: Service
metadata:
  name: myapp-canary
spec:
  selector:
    app: myapp
    version: v2
  ports:
  - port: 80

---
# 金丝雀 Ingress(10% 流量)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-canary
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "10"  # 10% 流量
spec:
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp-canary
            port:
              number: 80
```

#### 方法 3:使用 Flagger(自动化金丝雀)

Flagger 是 CNCF 项目,支持自动化渐进式交付:

```yaml
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: myapp
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  service:
    port: 80
  analysis:
    interval: 1m
    threshold: 5
    maxWeight: 50
    stepWeight: 10  # 每次增加 10% 流量
    metrics:
    - name: request-success-rate
      thresholdRange:
        min: 99  # 成功率 < 99% 则回滚
```

**流量分配过程**:0% → 10% → 20% → 30% → 40% → 50% → 100%

### 八、生产环境最佳实践

#### 1. 资源配置

```yaml
spec:
  template:
    spec:
      containers:
      - name: app
        resources:
          requests:  # 调度依据
            cpu: 500m
            memory: 512Mi
          limits:    # 硬限制
            cpu: 1000m
            memory: 1Gi
```

**原则**:

- **requests**:设为实际使用的 80%
- **limits**:设为 requests 的 1.5~2 倍(防止突发流量)

#### 2. 反亲和性(避免单点故障)

```yaml
spec:
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: nginx
              topologyKey: kubernetes.io/hostname  # 不同节点
```

**效果**:确保 Pod 分散在不同节点,避免单节点故障导致服务不可用。

#### 3. PodDisruptionBudget(中断预算)

防止节点维护或驱逐时影响服务可用性:

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: nginx-pdb
spec:
  minAvailable: 2  # 至少保持 2 个 Pod 运行
  # 或使用 maxUnavailable: 1(最多 1 个 Pod 不可用)
  selector:
    matchLabels:
      app: nginx
```

#### 4. 更新策略推荐

```yaml
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 0  # 生产环境避免服务中断
  minReadySeconds: 10
  revisionHistoryLimit: 5
  progressDeadlineSeconds: 600
```

### 九、常见问题排查

| 问题                       | 原因                                 | 解决方法                             |
| -------------------------- | ------------------------------------ | ------------------------------------ |
| 滚动更新卡住               | 新 Pod 未就绪(readinessProbe 失败)  | 检查探针配置,查看 Pod 日志           |
| 更新后服务不可用           | 未配置 readinessProbe                | 添加探针,确保 Pod 就绪后再接入流量   |
| Pod 频繁重启               | livenessProbe 配置过于严格           | 增加 initialDelaySeconds 和 failureThreshold |
| 回滚失败                   | 历史版本被清理(revisionHistoryLimit)| 增大 revisionHistoryLimit            |
| 资源不足导致更新失败       | maxSurge 过大,集群资源不足           | 降低 maxSurge 或扩容集群             |

**调试命令**:

```bash
# 查看更新状态
kubectl rollout status deployment nginx-deployment

# 查看事件
kubectl describe deployment nginx-deployment

# 查看 Pod 日志
kubectl logs -l app=nginx --tail=100

# 查看 ReplicaSet 状态
kubectl get rs -l app=nginx

# 强制删除卡住的 Pod
kubectl delete pod <pod-name> --grace-period=0 --force
```

### 十、总结

**Deployment 是 K8s 无状态应用管理的核心**,通过声明式配置和滚动更新机制,实现了应用的自动化部署、扩缩容和版本管理。

**核心能力**:

- **声明式管理**:定义期望状态,K8s 自动维护
- **滚动更新**:零停机部署,通过 maxSurge/maxUnavailable 控制节奏
- **版本回滚**:秒级回退到历史版本
- **自动恢复**:Pod 异常时自动重建
- **水平扩缩容**:动态调整副本数

**最佳实践总结**:

1. **必须配置 readinessProbe**,确保流量只导入就绪的 Pod
2. **生产环境设置 maxUnavailable=0**,避免服务中断
3. **使用 PodDisruptionBudget 保护关键服务**
4. **结合 HPA 实现自动扩缩容**
5. **使用 Flagger/ArgoCD 实现自动化金丝雀发布**

Deployment 适用于无状态应用,对于有状态应用(如数据库、消息队列),应使用 StatefulSet。
