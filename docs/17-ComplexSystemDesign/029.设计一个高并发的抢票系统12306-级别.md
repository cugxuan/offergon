---
title: 设计一个高并发的抢票系统（12306 级别）
tags:
  - 复杂系统设计
status: robot
class: 复杂系统设计
slug: high-concurrency-ticket-booking-system
ref:
---

## 核心要点

**挑战分析**：峰值QPS达百万级、库存超卖防范、数据一致性保证、系统可用性要求99.99%
**架构策略**：分层分流、异步处理、缓存优化、限流熔断、实时监控
**技术重点**：Redis分布式锁、消息队列削峰、数据库分库分表、CDN加速

---

## 详细回答

### 1. 系统挑战分析

**流量特征**：
- 峰值QPS：100万+（节假日抢票高峰）
- 并发用户：千万级同时在线
- 数据量：车次信息、座位库存、用户订单
- 时效性：毫秒级响应，强一致性要求

**核心难点**：
- **高并发处理**：海量用户同时访问
- **库存超卖**：余票数据一致性保证
- **系统稳定性**：避免雪崩效应
- **用户体验**：排队机制、实时反馈

### 2. 整体架构设计

```
                    用户端
                     ↓
            [CDN + 负载均衡]
                     ↓
        ┌─────────────────────────────┐
        │       API Gateway           │
        │   (限流、熔断、鉴权)          │
        └─────────────────────────────┘
                     ↓
        ┌─────────────────────────────┐
        │       业务服务集群           │
        │ ┌─────────┐ ┌─────────────┐ │
        │ │抢票服务  │ │ 支付服务     │ │
        │ │订单服务  │ │ 用户服务     │ │
        │ └─────────┘ └─────────────┘ │
        └─────────────────────────────┘
                     ↓
        ┌─────────────────────────────┐
        │         缓存层               │
        │    Redis Cluster            │
        │  (库存、用户、会话)          │
        └─────────────────────────────┘
                     ↓
        ┌─────────────────────────────┐
        │       消息队列               │
        │      Kafka/RocketMQ         │
        │   (异步订单处理)             │
        └─────────────────────────────┘
                     ↓
        ┌─────────────────────────────┐
        │       数据库层               │
        │     MySQL分库分表            │
        │   (车次、订单、用户)          │
        └─────────────────────────────┘
```

### 3. 核心技术方案

#### 3.1 分层限流策略

**接入层限流**：
```nginx
# Nginx限流配置
limit_req_zone $binary_remote_addr zone=ticket:100m rate=100r/s;
limit_req zone=ticket burst=200 nodelay;
```

**API Gateway限流**：
- 基于用户ID限流：每用户每秒最多5次请求
- 基于IP限流：每IP每秒最多100次请求
- 熔断策略：错误率>50%时熔断5秒

**服务级限流**：
```go
// 令牌桶算法
type TokenBucket struct {
    capacity  int64
    tokens    int64
    rate      int64  // 每秒补充速率
    lastTime  time.Time
    mutex     sync.Mutex
}

func (tb *TokenBucket) TryAcquire() bool {
    tb.mutex.Lock()
    defer tb.mutex.Unlock()

    now := time.Now()
    elapsed := now.Sub(tb.lastTime).Seconds()
    tb.tokens = min(tb.capacity, tb.tokens + int64(elapsed * float64(tb.rate)))
    tb.lastTime = now

    if tb.tokens > 0 {
        tb.tokens--
        return true
    }
    return false
}
```

#### 3.2 分布式锁防超卖

**Redis分布式锁实现**：
```go
type DistributedLock struct {
    redis  *redis.Client
    key    string
    value  string
    expire time.Duration
}

func (dl *DistributedLock) TryLock() bool {
    script := `
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
    `

    result := dl.redis.SetNX(dl.key, dl.value, dl.expire)
    return result.Val()
}

// 抢票核心逻辑
func (s *TicketService) BookTicket(trainNo string, seatType string, userID int64) error {
    lockKey := fmt.Sprintf("lock:ticket:%s:%s", trainNo, seatType)
    lock := NewDistributedLock(s.redis, lockKey, generateUUID(), 10*time.Second)

    if !lock.TryLock() {
        return errors.New("系统繁忙，请稍后重试")
    }
    defer lock.Unlock()

    // 检查库存
    stockKey := fmt.Sprintf("stock:%s:%s", trainNo, seatType)
    stock, _ := s.redis.Get(stockKey).Int()
    if stock <= 0 {
        return errors.New("余票不足")
    }

    // 扣减库存
    newStock := s.redis.Decr(stockKey).Val()
    if newStock < 0 {
        s.redis.Incr(stockKey) // 回滚
        return errors.New("余票不足")
    }

    // 异步创建订单
    order := &Order{
        TrainNo:  trainNo,
        SeatType: seatType,
        UserID:   userID,
        Status:   OrderStatusPending,
    }
    s.orderQueue.Publish(order)

    return nil
}
```

#### 3.3 缓存优化策略

**多级缓存架构**：
```go
// L1: 本地缓存 (Caffeine/BigCache)
type LocalCache struct {
    cache *freecache.Cache
    ttl   int
}

// L2: Redis分布式缓存
type RedisCache struct {
    client *redis.ClusterClient
}

// L3: 数据库
type CacheService struct {
    local  *LocalCache
    redis  *RedisCache
    db     *gorm.DB
}

func (cs *CacheService) GetTrainInfo(trainNo string) (*TrainInfo, error) {
    // L1缓存
    if data, hit := cs.local.Get(trainNo); hit {
        return data.(*TrainInfo), nil
    }

    // L2缓存
    if data, err := cs.redis.Get(trainNo); err == nil {
        train := &TrainInfo{}
        json.Unmarshal([]byte(data), train)
        cs.local.Set(trainNo, train, 60) // 本地缓存1分钟
        return train, nil
    }

    // L3数据库
    train := &TrainInfo{}
    if err := cs.db.Where("train_no = ?", trainNo).First(train).Error; err != nil {
        return nil, err
    }

    // 写回缓存
    data, _ := json.Marshal(train)
    cs.redis.Set(trainNo, string(data), 5*time.Minute)
    cs.local.Set(trainNo, train, 60)

    return train, nil
}
```

#### 3.4 异步订单处理

**消息队列削峰**：
```go
// 订单处理流程
type OrderProcessor struct {
    consumer *kafka.Consumer
    db       *gorm.DB
    redis    *redis.Client
}

func (op *OrderProcessor) ProcessOrder(message *kafka.Message) {
    order := &Order{}
    json.Unmarshal(message.Value, order)

    // 开启事务
    tx := op.db.Begin()
    defer func() {
        if r := recover(); r != nil {
            tx.Rollback()
        }
    }()

    // 1. 创建订单记录
    if err := tx.Create(order).Error; err != nil {
        tx.Rollback()
        op.handleOrderFailure(order)
        return
    }

    // 2. 分配座位
    seat, err := op.allocateSeat(tx, order.TrainNo, order.SeatType)
    if err != nil {
        tx.Rollback()
        op.handleOrderFailure(order)
        return
    }

    // 3. 更新订单状态
    order.SeatNo = seat.SeatNo
    order.Status = OrderStatusConfirmed
    tx.Save(order)

    // 4. 提交事务
    if err := tx.Commit().Error; err != nil {
        op.handleOrderFailure(order)
        return
    }

    // 5. 发送确认通知
    op.sendConfirmation(order)
}
```

#### 3.5 数据库分库分表

**分片策略**：
```go
// 按时间+用户ID分片
type ShardingStrategy struct {
    dbCount    int
    tableCount int
}

func (ss *ShardingStrategy) GetShardInfo(userID int64, date time.Time) (string, string) {
    // 按月份分库
    dbIndex := int(date.Month()) % ss.dbCount
    dbName := fmt.Sprintf("ticket_db_%d", dbIndex)

    // 按用户ID分表
    tableIndex := userID % int64(ss.tableCount)
    tableName := fmt.Sprintf("orders_%d", tableIndex)

    return dbName, tableName
}

// 订单查询
func (s *OrderService) GetUserOrders(userID int64, startDate, endDate time.Time) ([]*Order, error) {
    var allOrders []*Order

    // 遍历时间范围内的所有分片
    for date := startDate; date.Before(endDate); date = date.AddDate(0, 1, 0) {
        dbName, tableName := s.strategy.GetShardInfo(userID, date)
        db := s.getDBByName(dbName)

        var orders []*Order
        err := db.Table(tableName).Where("user_id = ? AND created_at >= ? AND created_at < ?",
            userID, date, date.AddDate(0, 1, 0)).Find(&orders).Error
        if err != nil {
            continue
        }
        allOrders = append(allOrders, orders...)
    }

    return allOrders, nil
}
```

### 4. 可用性保障

#### 4.1 容错机制

**服务降级**：
```go
type TicketService struct {
    primary   TicketServiceInterface
    fallback  TicketServiceInterface
    breaker   *CircuitBreaker
}

func (ts *TicketService) BookTicket(req *BookTicketRequest) (*BookTicketResponse, error) {
    if ts.breaker.IsOpen() {
        // 降级到队列模式
        return ts.fallback.BookTicket(req)
    }

    resp, err := ts.primary.BookTicket(req)
    if err != nil {
        ts.breaker.RecordFailure()
        return ts.fallback.BookTicket(req)
    }

    ts.breaker.RecordSuccess()
    return resp, nil
}
```

**实时监控**：
```go
type MetricsCollector struct {
    prometheus.Collector
}

var (
    requestTotal = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "ticket_requests_total",
            Help: "Total number of ticket requests",
        },
        []string{"method", "status"},
    )

    requestDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "ticket_request_duration_seconds",
            Help: "Request duration in seconds",
        },
        []string{"method"},
    )
)

func (mc *MetricsCollector) RecordRequest(method, status string, duration time.Duration) {
    requestTotal.WithLabelValues(method, status).Inc()
    requestDuration.WithLabelValues(method).Observe(duration.Seconds())
}
```

#### 4.2 灾备方案

**多活部署**：
- 主备机房：北京、上海双活部署
- 数据同步：MySQL主从复制 + Binlog增量同步
- 流量切换：DNS智能解析 + LVS负载均衡

**容灾演练**：
- 定期故障注入测试
- 全链路压测验证
- 应急预案自动化执行

### 5. 性能优化

#### 5.1 前端优化

**静态资源优化**：
```javascript
// 资源预加载
<link rel="preload" href="/api/trains" as="fetch" crossorigin>

// 接口并行请求
Promise.all([
    fetch('/api/trains'),
    fetch('/api/stations'),
    fetch('/api/user-info')
]).then(responses => {
    // 处理响应
});

// 防抖搜索
const searchTrains = debounce((keyword) => {
    fetch(`/api/trains/search?q=${keyword}`)
        .then(response => response.json())
        .then(data => updateUI(data));
}, 300);
```

**用户体验优化**：
- 排队机制：显示队列位置和预计等待时间
- 乐观更新：先更新UI，后台异步验证
- 错误重试：自动重试机制，指数退避

#### 5.2 后端优化

**连接池调优**：
```go
// 数据库连接池
db.SetMaxIdleConns(100)    // 最大空闲连接
db.SetMaxOpenConns(200)    // 最大打开连接
db.SetConnMaxLifetime(time.Hour) // 连接最大生存时间

// Redis连接池
redisClient := redis.NewClusterClient(&redis.ClusterOptions{
    Addrs:       []string{"redis1:6379", "redis2:6379", "redis3:6379"},
    PoolSize:    100,
    MaxRetries:  3,
    PoolTimeout: 5 * time.Second,
})
```

### 6. 总结

高并发抢票系统的核心是**分层架构**和**分流策略**：

1. **接入层**：CDN + 负载均衡分散流量
2. **业务层**：限流熔断 + 异步处理
3. **存储层**：多级缓存 + 分库分表
4. **保障层**：监控告警 + 容灾备份

关键成功因素：
- **性能**：多级缓存，减少DB压力
- **一致性**：分布式锁，防止超卖
- **可用性**：服务降级，保证核心功能
- **扩展性**：水平扩容，应对流量突增

这套架构可以支撑千万级用户同时抢票，保证系统稳定性和用户体验。
