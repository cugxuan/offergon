---
title: 设计一个分布式搜索引擎（Elasticsearch 级别）
tags:
  - 分布式
  - 复杂系统设计
  - 索引
status: robot
class: 复杂系统设计
slug: distributed-search-engine-design
ref:
---

## 核心要点

**架构关键词：** 倒排索引、分片(Shard)、副本(Replica)、路由、负载均衡、相关性评分、近实时搜索

**技术选型：** Lucene核心引擎、分布式协调(Raft/Paxos)、LSM-Tree存储、布隆过滤器、分词器、TF-IDF/BM25算法

**核心挑战：** 海量数据索引、毫秒级查询响应、高可用容错、数据一致性、相关性排序、多语言分词

---

## 详细设计方案

### 一、需求分析与系统规模

#### 1.1 功能需求

**核心功能：**
1. **文档索引（Index）**：支持增删改查文档
2. **全文检索（Search）**：关键词搜索、短语搜索、模糊搜索
3. **聚合分析（Aggregation）**：统计、分组、排序
4. **高级查询**：布尔查询、范围查询、地理位置查询、向量搜索
5. **实时性**：近实时搜索（NRT，延迟<1秒）

**扩展功能：**
- 拼写纠错、同义词、停用词
- 高亮显示
- 分页与游标
- 多租户隔离

#### 1.2 非功能需求

| 指标 | 要求 | 说明 |
|-----|------|------|
| 数据规模 | PB级 | 支持100亿+文档 |
| 查询QPS | 10万+ | 高并发读取 |
| 索引TPS | 10万+ | 高吞吐写入 |
| 查询延迟 | P99 < 100ms | 毫秒级响应 |
| 可用性 | 99.9% | 支持节点故障 |
| 数据可靠性 | 副本机制 | 零数据丢失 |

#### 1.3 容量估算

```
假设场景：电商搜索
- 文档数：10亿商品
- 单文档大小：1KB（标题+描述+属性）
- 总数据量：1TB原始数据
- 倒排索引：原始数据的3-5倍 = 3-5TB
- 副本数：2副本 = 9-15TB总存储
- 日新增：100万文档
- 日查询：10亿次（QPS峰值10万）
```

---

### 二、整体架构设计

#### 2.1 系统架构图

```
┌─────────────────────────────────────────────┐
│               客户端（SDK/RESTful API）      │
└──────────────┬──────────────────────────────┘
               ↓
┌─────────────────────────────────────────────┐
│           负载均衡层（Nginx/HAProxy）        │
└──────────────┬──────────────────────────────┘
               ↓
┌─────────────────────────────────────────────┐
│  协调节点（Coordinating Node）集群          │
│  - 请求路由、结果聚合、查询解析              │
└──────────────┬──────────────────────────────┘
               ↓
     ┌─────────┴─────────┐
     ↓                   ↓
┌──────────┐      ┌──────────┐
│ 主节点    │      │ 数据节点  │
│ (Master) │      │ (Data)   │
│ - 集群管理│      │ - 索引存储│
│ - 分片分配│      │ - 查询执行│
│ - 元数据  │      │ - 分片管理│
└──────────┘      └──────────┘
                       ↓
              ┌────────┴────────┐
              ↓                 ↓
         主分片(Primary)    副本分片(Replica)
         Shard-0           Shard-0-R1
         Shard-1           Shard-1-R1
         ...               ...
```

**节点角色划分：**

| 节点类型 | 职责 | 数量 |
|---------|------|------|
| **Master Node** | 集群状态管理、分片分配、索引创建删除 | 3节点（奇数，选举） |
| **Data Node** | 存储数据、执行索引和查询操作 | N节点（根据数据量） |
| **Coordinating Node** | 接收客户端请求、路由到Data Node、聚合结果 | M节点（无状态，可扩展） |
| **Ingest Node** | 数据预处理（管道、转换） | 可选 |

#### 2.2 数据模型

**索引（Index）**：类似数据库的表
```json
{
  "products": {
    "settings": {
      "number_of_shards": 10,      // 主分片数
      "number_of_replicas": 2,     // 副本数
      "refresh_interval": "1s"     // 刷新间隔
    },
    "mappings": {
      "properties": {
        "title": {"type": "text", "analyzer": "ik_max_word"},
        "price": {"type": "float"},
        "category": {"type": "keyword"},
        "description": {"type": "text"},
        "location": {"type": "geo_point"},
        "created_at": {"type": "date"}
      }
    }
  }
}
```

**文档（Document）**：数据记录
```json
{
  "_index": "products",
  "_id": "12345",
  "_source": {
    "title": "iPhone 15 Pro Max",
    "price": 9999,
    "category": "手机",
    "description": "最新款苹果手机...",
    "location": {"lat": 39.9, "lon": 116.4},
    "created_at": "2024-01-01T10:00:00Z"
  }
}
```

---

### 三、核心模块详细设计

#### 3.1 倒排索引（Inverted Index）

**原理：** 词条 → 文档ID列表

**示例：**
```
原始文档：
Doc1: "快速的棕色狐狸跳过懒狗"
Doc2: "懒狗躺在阳光下"
Doc3: "快速的行动很重要"

分词后倒排索引：
Term       | Doc IDs        | Positions
-----------|----------------|----------
快速       | [1, 3]         | [1:0, 3:0]
棕色       | [1]            | [1:2]
狐狸       | [1]            | [1:4]
跳过       | [1]            | [1:6]
懒狗       | [1, 2]         | [1:8, 2:0]
躺         | [2]            | [2:2]
阳光       | [2]            | [2:5]
行动       | [3]            | [3:2]
重要       | [3]            | [3:5]
```

**数据结构实现（基于Lucene）：**

```
倒排索引 = {
    词典（Dictionary）：Term → TermID 映射
        - 使用 FST（Finite State Transducer）压缩存储
        - 支持前缀查询、模糊匹配

    倒排列表（Posting List）：TermID → DocID列表
        - 使用差分编码（Delta Encoding）压缩
        - 使用跳表（Skip List）加速查找

    位置信息（Positions）：DocID → 词条位置
        - 支持短语查询（Phrase Query）

    词频统计（Term Frequency）：DocID → 词条出现次数
        - 用于相关性评分
}
```

**写入流程（索引过程）：**
```
1. 接收文档 → 解析JSON
2. 分词（Analyzer）：
   - Tokenizer：分割文本（IK分词器/标准分词器）
   - Token Filter：小写转换、停用词过滤、同义词扩展
3. 构建倒排索引：
   - 内存缓冲区（Buffer）累积文档
   - 当Buffer满或超时 → Flush到Segment
4. 写入Segment文件：
   - 不可变（Immutable）文件
   - 包含倒排索引、正排索引、列式存储
5. 刷新（Refresh）：
   - 每秒将内存Segment可见，实现NRT
6. 合并（Merge）：
   - 后台异步合并小Segment为大Segment
   - 删除标记为删除的文档
```

**Segment文件结构：**
```
Segment-N/
├── .tim     # 词典（Terms Dictionary）
├── .tip     # 词典索引（Terms Index，FST）
├── .doc     # 文档ID列表（Doc IDs）
├── .pos     # 位置信息（Positions）
├── .pay     # 负载（Payloads）
├── .nvd     # 列式存储（DocValues，聚合用）
└── .fdt     # 正排索引（Stored Fields，_source字段）
```

**Segment合并策略：**
```go
// Tiered Merge Policy（分层合并策略）
type TieredMergePolicy struct {
    MaxMergedSegmentMB   int  // 单个Segment最大大小（默认5GB）
    SegmentsPerTier      int  // 每层Segment数量（默认10）
    MaxMergeAtOnce       int  // 单次最多合并数（默认10）
}

func (p *TieredMergePolicy) FindMerges(segments []Segment) []MergeTask {
    // 按大小分组Segment
    tiers := groupBySize(segments)

    var merges []MergeTask
    for _, tier := range tiers {
        if len(tier) >= p.SegmentsPerTier {
            // 合并该层的Segment
            merges = append(merges, MergeTask{
                Segments: tier[:p.MaxMergeAtOnce],
            })
        }
    }
    return merges
}
```

---

#### 3.2 分片与路由（Sharding & Routing）

**分片策略：**
```
分片ID = hash(routing_value) % number_of_primary_shards

默认routing_value = document_id
可自定义：routing=user_id（将同一用户的文档路由到同一分片）
```

**写入路由：**
```
1. 客户端 → Coordinating Node
2. Coordinating Node计算分片ID
3. 转发请求到Primary Shard所在的Data Node
4. Primary Shard写入成功后，同步到所有Replica Shard
5. 所有Replica确认后，返回成功
```

**查询路由（查询所有分片）：**
```
1. 客户端发送查询 → Coordinating Node
2. Coordinating Node广播查询到所有Primary或Replica Shard
   - 使用轮询（Round-Robin）选择Primary或Replica
3. 每个分片返回Top N结果（Document ID + Score）
4. Coordinating Node合并排序（Global Top N）
5. Fetch阶段：根据Document ID从分片获取完整文档
6. 返回给客户端
```

**分片分配算法：**
```go
// 主分片与副本分片不能在同一节点
// 尽量均匀分配到所有节点

type ShardAllocator struct {
    nodes []*DataNode
}

func (a *ShardAllocator) AllocateShard(shard *Shard) *DataNode {
    // 1. 过滤掉已有该分片副本的节点
    candidates := a.filterNodes(shard)

    // 2. 选择磁盘使用率最低的节点
    sort.Slice(candidates, func(i, j int) bool {
        return candidates[i].DiskUsage < candidates[j].DiskUsage
    })

    // 3. 分配到最优节点
    return candidates[0]
}

// 分片迁移（Rebalance）
func (a *ShardAllocator) Rebalance() {
    for _, node := range a.nodes {
        if node.ShardCount > avgShardCount + threshold {
            // 迁移分片到负载较低的节点
            shard := node.PickShardToMove()
            targetNode := a.findLeastLoadedNode()
            a.moveShard(shard, node, targetNode)
        }
    }
}
```

**分片数量规划：**
```
推荐分片大小：20-50GB
分片数计算公式：
    分片数 = 总数据量 / 单分片目标大小
    例：1TB数据 / 30GB = 34个分片（取整40）

注意：
- 分片数过多：协调开销大、内存占用高
- 分片数过少：单分片过大、查询慢、难以扩展
- 分片数在索引创建后无法修改（需Reindex）
```

---

#### 3.3 集群协调与容错

**主节点选举（Master Election）：**
```
使用 Bully算法或Raft协议

Bully算法流程：
1. 节点发现Master不可达
2. 发起选举，向所有节点ID大于自己的节点发送选举消息
3. 如果收到更大ID节点的"我是Master"消息，则退出选举
4. 如果超时未收到，则宣布自己为Master
5. 广播自己的Master身份

配置：
discovery.zen.minimum_master_nodes = (master_nodes / 2) + 1
例：3个Master节点 → 最少2个节点同意才能选举成功（防脑裂）
```

**脑裂（Split-Brain）防护：**
```
问题：网络分区导致两个独立集群同时存在

解决方案：
1. 最小主节点数（Quorum）：
   minimum_master_nodes = (N/2) + 1
   只有获得多数票的节点才能成为Master

2. 节点发现机制：
   - 定期心跳检测（ping）
   - 超时未响应 → 标记节点失败

3. 分片分配延迟：
   - 节点失败后延迟5分钟再重新分配分片
   - 防止短暂网络抖动导致大量数据迁移
```

**数据副本与一致性：**
```
写入一致性级别：
- one：Primary Shard写入成功即返回（最快，风险高）
- all：所有副本写入成功才返回（最慢，最安全）
- quorum（默认）：多数副本写入成功（平衡）
  quorum = (primary + replicas) / 2 + 1
  例：1主2副 → 至少2个成功

读取一致性：
- preference=_primary：只读主分片（强一致）
- preference=_replica：只读副本（降低主分片压力）
- preference=_local：优先读本地节点（最快）
```

**故障恢复流程：**
```
场景1：Data Node故障
1. Master检测到节点心跳丢失
2. 将该节点上的Primary Shard标记为失败
3. 提升某个Replica Shard为Primary
4. 在其他节点创建新的Replica（达到副本数）

场景2：Master Node故障
1. 其他候选Master节点发起选举
2. 选出新Master
3. 新Master接管集群管理（分片分配、索引管理）

场景3：整个集群宕机（断电）
1. 节点重启后加入集群
2. Master从持久化的元数据恢复集群状态
3. 从WAL（Write-Ahead Log）恢复未刷盘的数据
```

---

#### 3.4 查询执行引擎

**查询DSL（Domain Specific Language）：**
```json
{
  "query": {
    "bool": {
      "must": [
        {"match": {"title": "iPhone"}},
        {"range": {"price": {"gte": 5000, "lte": 10000}}}
      ],
      "should": [
        {"term": {"category": "手机"}}
      ],
      "filter": [
        {"geo_distance": {
          "distance": "10km",
          "location": {"lat": 39.9, "lon": 116.4}
        }}
      ]
    }
  },
  "sort": [{"price": "asc"}],
  "from": 0,
  "size": 20
}
```

**查询执行流程（Two-Phase Search）：**

**Phase 1: Query Phase（查询阶段）**
```
1. Coordinating Node解析查询
2. 广播到所有相关分片（Primary或Replica）
3. 每个分片执行查询：
   - 使用倒排索引查找匹配文档
   - 计算相关性得分（TF-IDF/BM25）
   - 返回Top N的(DocID, Score)
4. Coordinating Node全局排序，选出最终Top N
```

**Phase 2: Fetch Phase（获取阶段）**
```
1. Coordinating Node根据DocID定位到对应分片
2. 请求分片返回完整文档（_source字段）
3. 组装最终结果返回给客户端
```

**相关性评分算法：**

**TF-IDF（词频-逆文档频率）：**
```
Score = TF × IDF

TF（Term Frequency）= 词条在文档中出现次数 / 文档总词数
IDF（Inverse Document Frequency）= log(总文档数 / 包含该词的文档数)

示例：
查询"iPhone"
Doc1: "iPhone iPhone Pro" (3词)
    TF = 2/3 = 0.67
    IDF = log(1000万 / 10万) = 2.0
    Score = 0.67 × 2.0 = 1.34

Doc2: "买 iPhone" (2词)
    TF = 1/2 = 0.5
    IDF = 2.0
    Score = 0.5 × 2.0 = 1.0

Doc1分数更高，排序在前
```

**BM25（Best Matching 25，Elasticsearch默认）：**
```
BM25 = IDF × (TF × (k1 + 1)) / (TF + k1 × (1 - b + b × (文档长度 / 平均文档长度)))

参数：
k1 = 1.2（控制词频饱和度）
b = 0.75（控制文档长度归一化）

优点：
- 词频饱和：出现10次和100次的差异不大（避免刷词）
- 文档长度归一化：短文档不会被长文档压制
```

**查询优化技术：**

1. **缓存（Query Cache）**
```go
// 查询结果缓存（LRU）
type QueryCache struct {
    cache *lru.Cache
    size  int
}

func (c *QueryCache) Get(queryHash string) *SearchResult {
    if val, ok := c.cache.Get(queryHash); ok {
        return val.(*SearchResult)
    }
    return nil
}

func (c *QueryCache) Put(queryHash string, result *SearchResult) {
    c.cache.Add(queryHash, result)
}

// 缓存失效：文档更新/删除时清除相关缓存
```

2. **Filter Cache（过滤器缓存）**
```
Filter查询（不计算分数）结果缓存为Bitset
例：{"term": {"status": "published"}} → [1,0,1,1,0,...]

多个Filter组合使用位运算：
status=published AND category=phone
→ Bitset1 & Bitset2
```

3. **预加载（Warmup）**
```bash
# 集群启动后预热热门查询
POST /products/_search
{
  "query": {"match_all": {}},
  "size": 0
}
```

4. **查询改写（Query Rewrite）**
```
原查询：prefix查询（前缀匹配）
改写为：使用FST快速查找所有匹配词条，转为TermQuery

例：prefix=app → [apple, application, ...] → OR查询
```

---

#### 3.5 聚合分析（Aggregation）

**聚合类型：**

1. **Bucket Aggregation（桶聚合）**
```json
// 按类别分组统计
{
  "aggs": {
    "group_by_category": {
      "terms": {"field": "category"}
    }
  }
}

// 结果：
{
  "aggregations": {
    "group_by_category": {
      "buckets": [
        {"key": "手机", "doc_count": 1000},
        {"key": "电脑", "doc_count": 500}
      ]
    }
  }
}
```

2. **Metric Aggregation（指标聚合）**
```json
// 计算平均价格
{
  "aggs": {
    "avg_price": {
      "avg": {"field": "price"}
    }
  }
}

// 结果：
{
  "aggregations": {
    "avg_price": {"value": 5999}
  }
}
```

3. **Pipeline Aggregation（管道聚合）**
```json
// 计算销量同比增长率
{
  "aggs": {
    "sales_per_month": {
      "date_histogram": {"field": "date", "interval": "month"},
      "aggs": {
        "total_sales": {"sum": {"field": "amount"}}
      }
    },
    "growth_rate": {
      "derivative": {
        "buckets_path": "sales_per_month>total_sales"
      }
    }
  }
}
```

**聚合实现原理（基于DocValues）：**
```
DocValues = 列式存储（Column-Oriented Storage）

传统倒排索引（行式）：
DocID → 所有字段值
1 → {title: "iPhone", price: 9999}

DocValues（列式）：
price字段：[9999, 5999, 3999, ...]
         DocID: [1, 2, 3, ...]

优点：
- 聚合时只读取需要的字段，IO更少
- 压缩友好（相同类型数据连续存储）
- 支持排序和聚合
```

**高基数聚合优化：**
```
问题：字段唯一值过多（如UserID，1亿+）
     terms聚合需要大量内存

解决方案：
1. Composite Aggregation（分页聚合）
   每次返回一批结果，使用after_key继续

2. Significant Terms（显著词聚合）
   只返回统计显著的词条（如异常检测）

3. 采样（Sampler Aggregation）
   从Top N文档中采样，减少计算量
```

---

#### 3.6 近实时搜索（Near Real-Time）

**NRT实现原理：**
```
传统搜索引擎：写入 → 提交(Commit) → 可搜索（延迟数秒到数分钟）

Elasticsearch NRT：
1. 写入 → 内存Buffer
2. 每1秒Refresh：Buffer → 新Segment（内存）
3. Segment可搜索（但未持久化）
4. 后台Flush：内存Segment → 磁盘（每30分钟或TransLog满）

延迟：默认1秒（可调整refresh_interval）
```

**TransLog（事务日志）：**
```
作用：数据持久化保证，防止数据丢失

流程：
1. 文档写入时，先写TransLog（磁盘）
2. 再写内存Buffer
3. Refresh后Segment可见（但TransLog未清空）
4. Flush时：
   - 内存Segment写入磁盘
   - 清空TransLog

故障恢复：
- 节点宕机重启后，从TransLog恢复未刷盘的数据
```

**写入性能优化：**

1. **批量写入（Bulk API）**
```json
POST /_bulk
{"index": {"_index": "products", "_id": "1"}}
{"title": "iPhone 15", "price": 9999}
{"index": {"_index": "products", "_id": "2"}}
{"title": "MacBook Pro", "price": 19999}

// 单次批量1000-5000条文档性能最优
```

2. **异步刷新**
```json
PUT /products/_settings
{
  "index": {
    "refresh_interval": "30s",  // 降低刷新频率
    "translog.durability": "async",  // 异步刷盘（风险：丢失5秒数据）
    "translog.sync_interval": "5s"
  }
}
```

3. **写入流程优化**
```go
// 写入缓冲区
type IndexBuffer struct {
    docs     []Document
    mu       sync.Mutex
    size     int
    maxSize  int  // 默认10% JVM堆内存
}

func (b *IndexBuffer) Add(doc Document) {
    b.mu.Lock()
    defer b.mu.Unlock()

    b.docs = append(b.docs, doc)
    b.size += doc.Size()

    if b.size >= b.maxSize {
        // 触发Refresh
        b.flush()
    }
}

func (b *IndexBuffer) flush() {
    segment := buildSegment(b.docs)
    writeSegmentToDisk(segment)
    b.docs = nil
    b.size = 0
}
```

---

### 四、高级特性

#### 4.1 分布式协调（Distributed Coordination）

**跨分片事务（Transaction）：**
```
ES不支持ACID事务，但提供以下保证：

1. 单文档操作原子性：
   - 单个文档的Index/Update/Delete是原子的
   - 使用版本号（_version）实现乐观锁

2. Bulk操作部分失败：
   - 返回每个操作的成功/失败状态
   - 应用层需处理失败重试

3. 分布式锁（Global Checkpoint）：
   - 用于分片间同步
   - 保证副本一致性
```

**并发控制（Optimistic Concurrency Control）：**
```json
// 基于版本号的乐观锁
PUT /products/_doc/1?if_seq_no=5&if_primary_term=1
{
  "title": "iPhone 15 Pro",
  "price": 10999
}

// 如果版本号不匹配，返回409 Conflict

// 使用场景：
- 防止并发更新覆盖
- 实现CAS（Compare-And-Swap）
```

#### 4.2 安全与权限控制

**认证与授权：**
```yaml
# 启用X-Pack Security
xpack.security.enabled: true

# 用户角色定义
roles:
  search_user:
    cluster: ['monitor']
    indices:
      - names: ['products*']
        privileges: ['read']

  admin_user:
    cluster: ['all']
    indices:
      - names: ['*']
        privileges: ['all']

# 用户创建
POST /_security/user/alice
{
  "password": "strong_password",
  "roles": ["search_user"]
}
```

**字段级安全（Field-Level Security）：**
```json
// 限制用户只能看到特定字段
{
  "indices": [{
    "names": ["products"],
    "privileges": ["read"],
    "field_security": {
      "grant": ["title", "price"],  // 只能访问这两个字段
      "except": ["cost"]              // 隐藏成本字段
    }
  }]
}
```

**审计日志（Audit Logging）：**
```yaml
xpack.security.audit.enabled: true
xpack.security.audit.logfile.events.include: [
  "access_granted", "access_denied", "authentication_failed"
]

# 记录所有查询请求
xpack.security.audit.logfile.events.emit_request_body: true
```

#### 4.3 监控与运维

**集群健康监控：**
```bash
# 集群状态
GET /_cluster/health
{
  "status": "green",  # green/yellow/red
  "number_of_nodes": 10,
  "active_primary_shards": 100,
  "active_shards": 300,
  "relocating_shards": 0,
  "unassigned_shards": 0
}

# 节点统计
GET /_nodes/stats
{
  "nodes": {
    "node-1": {
      "jvm": {"mem": {"heap_used_percent": 75}},
      "fs": {"total": {"available_in_bytes": 500000000000}},
      "indices": {"search": {"query_time_in_millis": 10000}}
    }
  }
}
```

**性能指标（Metrics）：**
```
关键指标：
1. 查询QPS：indices.search.query_total / time
2. 索引TPS：indices.indexing.index_total / time
3. 查询延迟：indices.search.query_time_in_millis / query_total
4. JVM堆内存使用率：jvm.mem.heap_used_percent < 75%
5. 磁盘使用率：fs.total.available_in_bytes > 15%
6. 分片数：每GB堆内存 < 20个分片

告警阈值：
- 堆内存 > 85%：触发Full GC，性能下降
- 磁盘 < 5%：触发只读模式
- 查询延迟P99 > 1秒：需优化
```

**慢查询日志：**
```yaml
# 配置慢查询阈值
index.search.slowlog.threshold.query.warn: 10s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.fetch.warn: 1s

# 查看慢查询
GET /_nodes/stats/indices/search
```

**索引生命周期管理（ILM）：**
```json
// 定义策略
PUT /_ilm/policy/logs_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "7d"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "forcemerge": {"max_num_segments": 1},
          "shrink": {"number_of_shards": 1}
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "freeze": {}
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}

// 应用到索引模板
PUT /_index_template/logs_template
{
  "index_patterns": ["logs-*"],
  "template": {
    "settings": {
      "index.lifecycle.name": "logs_policy"
    }
  }
}
```

---

### 五、性能优化最佳实践

#### 5.1 索引设计优化

**1. 合理设置Mapping**
```json
{
  "mappings": {
    "properties": {
      // 精确匹配用keyword（不分词）
      "category": {"type": "keyword"},

      // 全文搜索用text
      "description": {"type": "text", "analyzer": "ik_max_word"},

      // 数值范围查询
      "price": {"type": "float"},

      // 不需要搜索的字段禁用索引
      "internal_id": {"type": "keyword", "index": false},

      // 不需要存储原始值（节省空间）
      "thumbnail_url": {"type": "keyword", "store": false}
    }
  }
}
```

**2. 控制字段数量**
```
问题：字段过多（>1000）导致Mapping膨胀
解决：
- 使用嵌套对象（Nested）或父子关系
- 动态模板（Dynamic Templates）控制字段类型
- 禁用动态Mapping（"dynamic": "strict"）
```

**3. 合理使用Nested和Join**
```json
// Nested：嵌套文档（独立索引）
{
  "product": "iPhone",
  "reviews": [
    {"author": "Alice", "rating": 5},
    {"author": "Bob", "rating": 4}
  ]
}

// 查询：找到评分>4的评论
{
  "query": {
    "nested": {
      "path": "reviews",
      "query": {"range": {"reviews.rating": {"gt": 4}}}
    }
  }
}

// Join：父子关系（适合一对多关系，子文档独立更新）
{
  "relations": {
    "product": "comment"  // 产品-评论关系
  }
}
```

#### 5.2 查询优化

**1. 使用Filter代替Query（不计算分数）**
```json
// 慢（计算分数）
{"query": {"term": {"status": "published"}}}

// 快（Filter Cache）
{"query": {"bool": {"filter": {"term": {"status": "published"}}}}}
```

**2. 避免深分页**
```json
// 深分页问题：from=10000需要所有分片返回前10020条
{"from": 10000, "size": 20}

// 解决方案1：Scroll API（游标）
POST /products/_search?scroll=1m
{"size": 1000, "query": {...}}

// 解决方案2：Search After（推荐）
{
  "size": 20,
  "query": {...},
  "search_after": [1609459200000, "doc#10000"],  // 上次最后文档的sort值
  "sort": [{"timestamp": "asc"}, {"_id": "asc"}]
}
```

**3. 减少返回字段**
```json
{
  "query": {...},
  "_source": ["title", "price"],  // 只返回需要的字段
  "stored_fields": []
}
```

**4. 并行查询优化**
```json
// Multi-Search（批量查询）
POST /_msearch
{"index": "products"}
{"query": {"term": {"category": "phone"}}}
{"index": "products"}
{"query": {"term": {"category": "laptop"}}}

// 单次HTTP请求，多个查询并行执行
```

#### 5.3 硬件与配置优化

**1. 内存配置**
```bash
# JVM堆内存（推荐设置为物理内存的50%，最大31GB）
-Xms16g -Xmx16g

# 留50%给操作系统缓存（Lucene使用mmap）
物理内存64GB → JVM 31GB + OS Cache 31GB
```

**2. 磁盘选择**
```
- 优先使用SSD（IOPS提升10倍+）
- RAID 0（性能）或RAID 10（可靠性）
- 禁用swap（避免堆内存被交换到磁盘）
```

**3. 网络优化**
```bash
# 增大TCP缓冲区
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216

# 启用TCP BBR拥塞控制
net.ipv4.tcp_congestion_control = bbr
```

**4. 操作系统优化**
```bash
# 增大文件描述符限制
ulimit -n 65536

# 增大虚拟内存区域
vm.max_map_count = 262144

# 禁用透明大页（影响性能）
echo never > /sys/kernel/mm/transparent_hugepage/enabled
```

---

### 六、实战案例

#### 6.1 电商搜索系统

**需求：**
- 支持10亿商品全文搜索
- 查询延迟P99 < 100ms
- 支持拼写纠错、同义词、相关推荐

**架构设计：**
```
集群规模：
- 100台Data Node（64核128GB内存，8TB SSD × 2）
- 3台Master Node
- 10台Coordinating Node

索引设计：
- 索引名：products-YYYY.MM（按月滚动）
- 分片数：200个主分片 × 2副本 = 600个分片
- 单分片大小：30GB

Mapping设计：
{
  "properties": {
    "title": {
      "type": "text",
      "analyzer": "ik_max_word",
      "search_analyzer": "ik_smart",
      "fields": {
        "pinyin": {"type": "text", "analyzer": "pinyin"},
        "suggest": {"type": "completion"}  // 自动补全
      }
    },
    "price": {"type": "float"},
    "sales": {"type": "long"},
    "category": {"type": "keyword"},
    "brand": {"type": "keyword"},
    "attributes": {"type": "nested"},  // 商品属性（颜色/尺寸）
    "embedding": {"type": "dense_vector", "dims": 128}  // 向量搜索
  }
}
```

**查询示例：**
```json
// 综合查询（文本 + 过滤 + 排序 + 聚合）
{
  "query": {
    "function_score": {
      "query": {
        "bool": {
          "must": [{"match": {"title": "iPhone"}}],
          "filter": [
            {"range": {"price": {"gte": 5000, "lte": 10000}}},
            {"term": {"category": "手机"}}
          ]
        }
      },
      "functions": [
        {"field_value_factor": {"field": "sales", "modifier": "log1p", "factor": 0.5}},
        {"gauss": {"created_at": {"origin": "now", "scale": "30d"}}}
      ],
      "score_mode": "sum",
      "boost_mode": "multiply"
    }
  },
  "aggs": {
    "brands": {"terms": {"field": "brand", "size": 10}},
    "price_ranges": {
      "range": {
        "field": "price",
        "ranges": [
          {"to": 2000},
          {"from": 2000, "to": 5000},
          {"from": 5000}
        ]
      }
    }
  },
  "sort": [{"_score": "desc"}, {"sales": "desc"}],
  "from": 0,
  "size": 20
}
```

**性能数据：**
```
- 查询QPS：10万
- 索引TPS：5万
- P99查询延迟：80ms
- 集群存储：5PB（含副本）
```

#### 6.2 日志分析系统（ELK Stack）

**架构：**
```
应用日志 → Filebeat → Logstash → Elasticsearch → Kibana

Logstash处理：
1. 解析日志格式（Grok）
2. 提取字段（IP、时间戳、错误码）
3. 地理位置解析（GeoIP）
4. 写入Elasticsearch

索引模板：
- 索引名：logs-app-YYYY.MM.DD（按天滚动）
- ILM策略：7天后合并，30天后冷存储，90天后删除
```

**查询示例：**
```json
// 查询过去1小时的5xx错误
{
  "query": {
    "bool": {
      "must": [
        {"range": {"@timestamp": {"gte": "now-1h"}}},
        {"range": {"status_code": {"gte": 500, "lt": 600}}}
      ]
    }
  },
  "aggs": {
    "error_trend": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "1m"
      }
    },
    "top_errors": {
      "terms": {"field": "error_message.keyword", "size": 10}
    }
  }
}
```

---

### 七、总结

#### 7.1 技术亮点

| 模块 | 技术方案 | 收益 |
|-----|---------|------|
| 倒排索引 | FST + Delta Encoding + Skip List | 压缩率3:1，查询加速10倍 |
| 分片路由 | 一致性哈希 + 动态分配 | 线性扩展，支持PB级数据 |
| 近实时搜索 | Refresh + TransLog | 1秒延迟，数据零丢失 |
| 查询优化 | Cache + Filter + Rewrite | P99延迟<100ms |
| 容错机制 | 主备复制 + 自动故障转移 | 99.9%可用性 |

#### 7.2 关键指标

```
✅ 数据规模：支持100亿+文档（PB级）
✅ 查询性能：QPS 10万+，P99延迟 < 100ms
✅ 索引吞吐：TPS 10万+
✅ 可用性：99.9%（单节点故障自动恢复）
✅ 扩展性：水平扩展，分钟级加节点
```

#### 7.3 与Elasticsearch对比

| 特性 | 自研引擎 | Elasticsearch |
|-----|---------|--------------|
| 核心引擎 | 基于Lucene二次开发 | Lucene |
| 分布式协调 | Raft/Paxos | Zen Discovery |
| 查询语言 | 自定义DSL | Query DSL |
| 扩展性 | 支持插件 | 丰富插件生态 |
| 运维成本 | 需自建监控 | X-Pack全套工具 |

**未来优化方向：**
1. **向量搜索**：集成FAISS/Milvus，支持图像/语义搜索
2. **机器学习**：异常检测、智能推荐
3. **多租户隔离**：资源配额、流量隔离
4. **云原生**：Kubernetes Operator、自动扩缩容
5. **HTAP融合**：支持OLAP分析（与ClickHouse集成）
