---
title: 设计一个分布式缓存集群（类似 Redis Cluster）
tags:
  - 分布式
  - 复杂系统设计
  - 缓存
  - 缓存策略
status: robot
class: 复杂系统设计
slug: distributed-cache-cluster-design
ref:
---

## 核心要点提炼

- **分片策略**：一致性哈希算法实现数据分布
- **高可用性**：主从复制+自动故障转移
- **集群管理**：无中心化架构，节点自治
- **数据一致性**：最终一致性模型
- **横向扩展**：动态节点添加删除

## 详细设计方案

### 1. 整体架构设计

#### 1.1 分布式架构选择
```
┌─────────────────────────────────────────────────────────────┐
│                    Redis Cluster 架构                        │
├─────────────────────────────────────────────────────────────┤
│  Client    Client    Client    Client    Client             │
│     │        │        │        │        │                  │
│     └────────┼────────┼────────┼────────┘                  │
│              │        │        │                           │
│        ┌─────▼────────▼────────▼─────┐                     │
│        │      Smart Client Router     │                     │
│        └─────┬────────┬────────┬─────┘                     │
│              │        │        │                           │
│   ┌──────────▼─┐  ┌───▼───┐  ┌─▼──────────┐                │
│   │ Master A1  │  │Master │  │ Master C1  │                │
│   │   Slave A2 │  │  B1   │  │   Slave C2 │                │
│   │            │  │Slave  │  │            │                │
│   │ Slots:     │  │  B2   │  │ Slots:     │                │
│   │ 0-5460     │  │Slots: │  │ 10923-16383│                │
│   └────────────┘  │5461-  │  └────────────┘                │
│                   │10922  │                                 │
│                   └───────┘                                 │
└─────────────────────────────────────────────────────────────┘
```

#### 1.2 核心设计原则
- **无中心化架构**：所有节点地位平等，避免单点故障
- **数据分片**：将数据分散到多个节点，提高并发能力
- **高可用性**：主从复制机制保证服务连续性
- **弹性伸缩**：支持动态扩缩容

### 2. 数据分片策略

#### 2.1 一致性哈希实现
```go
// 一致性哈希环结构
type ConsistentHash struct {
    hashRing    map[uint32]string // 哈希环
    sortedKeys  []uint32          // 排序的节点key
    virtualNodes int              // 虚拟节点数量
    mutex       sync.RWMutex
}

// 计算key应该分配到哪个节点
func (ch *ConsistentHash) GetNode(key string) string {
    ch.mutex.RLock()
    defer ch.mutex.RUnlock()

    if len(ch.sortedKeys) == 0 {
        return ""
    }

    hash := ch.hashFunc(key)
    // 在环上顺时针查找第一个大于等于hash的节点
    idx := sort.Search(len(ch.sortedKeys), func(i int) bool {
        return ch.sortedKeys[i] >= hash
    })

    // 如果没找到，则分配给第一个节点（环形特性）
    if idx == len(ch.sortedKeys) {
        idx = 0
    }

    return ch.hashRing[ch.sortedKeys[idx]]
}
```

#### 2.2 Redis Cluster槽位机制
```
Slot分配策略：
- 总共16384个槽位（0-16383）
- 每个主节点负责一定范围的槽位
- 通过CRC16(key) % 16384计算槽位
- 支持槽位在线迁移
```

### 3. 节点间通信协议

#### 3.1 Gossip协议实现
```go
// Gossip消息类型
type GossipMessage struct {
    Type     MessageType `json:"type"`
    NodeID   string      `json:"node_id"`
    Data     interface{} `json:"data"`
    Timestamp int64      `json:"timestamp"`
}

// 节点状态信息
type NodeInfo struct {
    NodeID    string        `json:"node_id"`
    Address   string        `json:"address"`
    Port      int           `json:"port"`
    Status    NodeStatus    `json:"status"` // ONLINE/OFFLINE/SUSPECTED
    Slots     []SlotRange   `json:"slots"`
    IsMaster  bool          `json:"is_master"`
    MasterID  string        `json:"master_id,omitempty"`
    LastSeen  int64         `json:"last_seen"`
}

// Gossip协议核心逻辑
func (n *Node) gossipTick() {
    // 1. 选择随机节点发送状态
    targetNodes := n.selectRandomNodes(3)

    for _, target := range targetNodes {
        msg := &GossipMessage{
            Type:      MSG_PING,
            NodeID:    n.nodeID,
            Data:      n.getNodeInfo(),
            Timestamp: time.Now().Unix(),
        }
        n.sendGossipMessage(target, msg)
    }

    // 2. 处理接收到的消息
    n.processIncomingMessages()

    // 3. 检测故障节点
    n.detectFailedNodes()
}
```

#### 3.2 集群配置同步
```go
// 集群配置信息
type ClusterConfig struct {
    Version   int64                `json:"version"`
    Nodes     map[string]*NodeInfo `json:"nodes"`
    SlotMap   map[int]string       `json:"slot_map"` // slot -> nodeID
    Epoch     int64                `json:"epoch"`
    UpdatedAt int64                `json:"updated_at"`
}

// 配置变更传播
func (c *Cluster) propagateConfigChange(change *ConfigChange) error {
    // 增加配置版本号
    c.config.Version++
    c.config.UpdatedAt = time.Now().Unix()

    // 广播到所有节点
    for nodeID, node := range c.config.Nodes {
        if nodeID != c.localNodeID {
            go c.sendConfigUpdate(node, change)
        }
    }

    return nil
}
```

### 4. 高可用机制

#### 4.1 主从复制策略
```go
// 主从复制管理器
type ReplicationManager struct {
    master      *Node
    slaves      []*Node
    replicaLog  *ReplicationLog
    asyncWriter chan *WriteCommand
}

// 异步复制实现
func (rm *ReplicationManager) replicateAsync(cmd *WriteCommand) {
    // 1. 主节点立即返回
    rm.master.executeCommand(cmd)

    // 2. 异步复制到从节点
    go func() {
        for _, slave := range rm.slaves {
            err := slave.replicateCommand(cmd)
            if err != nil {
                log.Errorf("复制到从节点失败: %v", err)
                rm.handleReplicationError(slave, err)
            }
        }
    }()
}

// 主从切换逻辑
func (c *Cluster) promoteSlaveToMaster(slaveID string) error {
    slave := c.getNode(slaveID)
    if slave == nil {
        return errors.New("从节点不存在")
    }

    // 1. 更新节点角色
    slave.IsMaster = true
    slave.MasterID = ""

    // 2. 更新槽位映射
    for slot, nodeID := range c.config.SlotMap {
        if nodeID == slave.MasterID {
            c.config.SlotMap[slot] = slaveID
        }
    }

    // 3. 广播配置变更
    return c.propagateConfigChange(&ConfigChange{
        Type:   PROMOTE_SLAVE,
        NodeID: slaveID,
    })
}
```

#### 4.2 故障检测与恢复
```go
// 故障检测器
type FailureDetector struct {
    suspectTimeout   time.Duration
    failureTimeout   time.Duration
    suspectedNodes   map[string]int64
    failedNodes      map[string]int64
}

// 故障检测逻辑
func (fd *FailureDetector) detectFailures() {
    now := time.Now().Unix()

    for nodeID, lastSeen := range fd.suspectedNodes {
        // 如果怀疑超时，标记为失败
        if now-lastSeen > int64(fd.failureTimeout.Seconds()) {
            fd.markNodeAsFailed(nodeID)
            delete(fd.suspectedNodes, nodeID)
        }
    }
}

// 自动故障转移
func (c *Cluster) handleNodeFailure(failedNodeID string) error {
    failedNode := c.getNode(failedNodeID)
    if failedNode == nil {
        return nil
    }

    if failedNode.IsMaster {
        // 主节点故障，选择从节点提升
        bestSlave := c.selectBestSlave(failedNodeID)
        if bestSlave != nil {
            return c.promoteSlaveToMaster(bestSlave.NodeID)
        }
    } else {
        // 从节点故障，从集群中移除
        return c.removeNodeFromCluster(failedNodeID)
    }

    return nil
}
```

### 5. 客户端路由机制

#### 5.1 智能客户端实现
```go
// 智能客户端
type SmartClient struct {
    clusterNodes map[string]*Connection
    slotMap      map[int]string
    redirections map[string]int
    mutex        sync.RWMutex
}

// 路由请求到正确节点
func (sc *SmartClient) routeRequest(key string, cmd *Command) (*Response, error) {
    // 1. 计算槽位
    slot := crc16(key) % 16384

    // 2. 查找负责该槽位的节点
    sc.mutex.RLock()
    nodeID, exists := sc.slotMap[slot]
    sc.mutex.RUnlock()

    if !exists {
        return nil, errors.New("未找到负责该槽位的节点")
    }

    // 3. 发送请求
    conn := sc.clusterNodes[nodeID]
    resp, err := conn.sendCommand(cmd)

    // 4. 处理重定向
    if err != nil && resp.IsRedirection() {
        return sc.handleRedirection(resp, cmd)
    }

    return resp, err
}

// 处理MOVED和ASK重定向
func (sc *SmartClient) handleRedirection(resp *Response, cmd *Command) (*Response, error) {
    if resp.Type == MOVED {
        // 永久重定向，更新本地槽位映射
        sc.updateSlotMapping(resp.Slot, resp.NodeID)
    }

    // 重定向到新节点
    return sc.sendToNode(resp.NodeID, cmd)
}
```

#### 5.2 连接池管理
```go
// 连接池
type ConnectionPool struct {
    pools   map[string]*NodePool
    config  *PoolConfig
    mutex   sync.RWMutex
}

type NodePool struct {
    addr        string
    connections chan *Connection
    active      int32
    maxActive   int32
    maxIdle     int32
}

// 获取连接
func (cp *ConnectionPool) getConnection(nodeID string) (*Connection, error) {
    cp.mutex.RLock()
    pool, exists := cp.pools[nodeID]
    cp.mutex.RUnlock()

    if !exists {
        return nil, errors.New("节点连接池不存在")
    }

    select {
    case conn := <-pool.connections:
        return conn, nil
    default:
        if atomic.LoadInt32(&pool.active) < pool.maxActive {
            return cp.createConnection(pool.addr)
        }
        return nil, errors.New("连接池已满")
    }
}
```

### 6. 数据迁移机制

#### 6.1 在线槽位迁移
```go
// 槽位迁移管理器
type SlotMigrationManager struct {
    sourceNode      string
    targetNode      string
    migratingSlots  []int
    importingSlots  []int
    migrationState  map[int]MigrationState
}

// 迁移状态
type MigrationState int
const (
    MIGRATION_PREPARING MigrationState = iota
    MIGRATION_MIGRATING
    MIGRATION_COMPLETED
    MIGRATION_FAILED
)

// 执行槽位迁移
func (smm *SlotMigrationManager) migrateSlot(slot int, targetNode string) error {
    // 1. 标记槽位为迁移状态
    smm.migrationState[slot] = MIGRATION_MIGRATING

    // 2. 获取槽位中的所有key
    keys, err := smm.getSlotKeys(slot)
    if err != nil {
        return err
    }

    // 3. 逐个迁移key
    for _, key := range keys {
        err := smm.migrateKey(key, targetNode)
        if err != nil {
            smm.migrationState[slot] = MIGRATION_FAILED
            return err
        }
    }

    // 4. 更新槽位映射
    smm.updateSlotMapping(slot, targetNode)
    smm.migrationState[slot] = MIGRATION_COMPLETED

    return nil
}

// 迁移单个key
func (smm *SlotMigrationManager) migrateKey(key, targetNode string) error {
    // 1. 从源节点获取数据
    value, ttl, err := smm.sourceNode.dump(key)
    if err != nil {
        return err
    }

    // 2. 写入目标节点
    err = smm.targetNode.restore(key, value, ttl)
    if err != nil {
        return err
    }

    // 3. 删除源节点数据
    return smm.sourceNode.delete(key)
}
```

#### 6.2 一致性保证
```go
// 迁移期间的读写处理
func (n *Node) handleRequestDuringMigration(slot int, key string, cmd *Command) (*Response, error) {
    state := n.getMigrationState(slot)

    switch state {
    case MIGRATION_MIGRATING:
        if cmd.IsWrite() {
            // 写操作：先写本地，再同步到目标节点
            resp, err := n.executeCommand(cmd)
            if err == nil {
                n.syncKeyToTarget(key, n.migrationTarget[slot])
            }
            return resp, err
        } else {
            // 读操作：先读本地，如果不存在则ASK重定向
            resp, err := n.executeCommand(cmd)
            if err != nil && err == ErrKeyNotFound {
                return &Response{
                    Type:   ASK,
                    Slot:   slot,
                    NodeID: n.migrationTarget[slot],
                }, nil
            }
            return resp, err
        }
    default:
        return n.executeCommand(cmd)
    }
}
```

### 7. 性能优化策略

#### 7.1 批量操作优化
```go
// 批量操作处理器
type BatchProcessor struct {
    batchSize    int
    flushTimeout time.Duration
    buffer       []*Command
    mutex        sync.Mutex
}

// 批量执行命令
func (bp *BatchProcessor) batchExecute(commands []*Command) error {
    // 按节点分组
    nodeGroups := make(map[string][]*Command)
    for _, cmd := range commands {
        nodeID := bp.getNodeForKey(cmd.Key)
        nodeGroups[nodeID] = append(nodeGroups[nodeID], cmd)
    }

    // 并行发送到各节点
    var wg sync.WaitGroup
    errChan := make(chan error, len(nodeGroups))

    for nodeID, cmds := range nodeGroups {
        wg.Add(1)
        go func(node string, commands []*Command) {
            defer wg.Done()
            err := bp.sendBatchToNode(node, commands)
            if err != nil {
                errChan <- err
            }
        }(nodeID, cmds)
    }

    wg.Wait()
    close(errChan)

    // 检查错误
    for err := range errChan {
        if err != nil {
            return err
        }
    }

    return nil
}
```

#### 7.2 内存管理优化
```go
// 内存回收策略
type MemoryManager struct {
    maxMemory     int64
    usedMemory    int64
    evictionPolicy EvictionPolicy
    lru           *LRUCache
    lfu           *LFUCache
}

// LRU淘汰算法
func (mm *MemoryManager) evictLRU(needSpace int64) error {
    for mm.usedMemory+needSpace > mm.maxMemory {
        key := mm.lru.removeOldest()
        if key == "" {
            return errors.New("无法释放足够内存")
        }
        mm.deleteKey(key)
    }
    return nil
}

// 内存使用监控
func (mm *MemoryManager) monitorMemoryUsage() {
    ticker := time.NewTicker(1 * time.Second)
    for range ticker.C {
        if mm.getMemoryUsageRatio() > 0.8 {
            mm.triggerEviction()
        }
    }
}
```

### 8. 监控与运维

#### 8.1 指标收集
```go
// 集群指标
type ClusterMetrics struct {
    NodeCount         int               `json:"node_count"`
    AliveNodes        int               `json:"alive_nodes"`
    TotalSlots        int               `json:"total_slots"`
    AssignedSlots     int               `json:"assigned_slots"`
    MigratingSlots    int               `json:"migrating_slots"`
    RequestsPerSecond float64           `json:"requests_per_second"`
    AverageLatency    time.Duration     `json:"average_latency"`
    MemoryUsage       map[string]int64  `json:"memory_usage"`
    NetworkIO         map[string]int64  `json:"network_io"`
}

// 指标收集器
func (c *Cluster) collectMetrics() *ClusterMetrics {
    metrics := &ClusterMetrics{
        NodeCount:      len(c.nodes),
        TotalSlots:     16384,
        AssignedSlots:  len(c.slotMap),
    }

    // 统计存活节点
    for _, node := range c.nodes {
        if node.isAlive() {
            metrics.AliveNodes++
        }
    }

    // 统计迁移中的槽位
    metrics.MigratingSlots = c.countMigratingSlots()

    // 收集性能指标
    metrics.RequestsPerSecond = c.calculateRPS()
    metrics.AverageLatency = c.calculateAvgLatency()

    return metrics
}
```

#### 8.2 自动化运维
```go
// 自动运维管理器
type AutoOpsManager struct {
    cluster          *Cluster
    healthChecker    *HealthChecker
    autoScaler       *AutoScaler
    alertManager     *AlertManager
}

// 健康检查
func (aom *AutoOpsManager) healthCheck() {
    // 1. 检查节点健康状态
    unhealthyNodes := aom.healthChecker.checkAllNodes()

    // 2. 自动恢复故障节点
    for _, nodeID := range unhealthyNodes {
        go aom.recoverNode(nodeID)
    }

    // 3. 检查负载均衡
    if aom.needRebalance() {
        aom.triggerRebalance()
    }

    // 4. 发送告警
    if len(unhealthyNodes) > 0 {
        aom.alertManager.sendAlert("节点故障", unhealthyNodes)
    }
}

// 自动扩缩容
func (aom *AutoOpsManager) autoScale() {
    metrics := aom.cluster.collectMetrics()

    // 根据负载决定扩缩容
    if metrics.RequestsPerSecond > aom.autoScaler.scaleUpThreshold {
        aom.scaleUp()
    } else if metrics.RequestsPerSecond < aom.autoScaler.scaleDownThreshold {
        aom.scaleDown()
    }
}
```

## 关键技术难点

### 1. 脑裂问题解决
- 使用多数派投票机制
- 设置最小集群节点数要求
- 实现集群隔离检测

### 2. 数据一致性保证
- 最终一致性模型
- 向量时钟冲突检测
- 读修复机制

### 3. 网络分区处理
- CAP理论权衡
- 分区期间服务降级
- 分区恢复后数据合并

这套分布式缓存集群设计提供了完整的高可用、高性能缓存解决方案，具备自动故障转移、弹性伸缩、数据迁移等企业级特性。
