---
title: 设计一个全球化的社交媒体平台（十亿级用户）
tags:
  - 复杂系统设计
status: robot
class: 复杂系统设计
slug: global-social-media-platform-design-billion-users
ref:
---

## 核心要点

**架构关键词：** 地理分布式、多活数据中心、CDN加速、读写分离、缓存优先、异步化、最终一致性

**技术选型：** 分片MySQL/TiDB、Redis集群、Kafka消息队列、ElasticSearch、对象存储、GraphQL/gRPC

**核心挑战：** 跨地域延迟、数据一致性、热点数据、流量突发、内容审核、成本优化

---

## 详细设计方案

### 一、需求分析与系统规模估算

#### 1.1 功能需求
- **核心功能**：用户注册/登录、发布动态（文字/图片/视频）、关注/粉丝、点赞/评论/转发、Feed流、搜索、消息通知
- **高级功能**：直播、故事（24小时）、推荐算法、内容审核、广告系统

#### 1.2 非功能需求
- **规模**：10亿注册用户，1亿DAU，峰值QPS 100万+
- **性能**：Feed流加载 < 300ms，发布延迟 < 1s，消息推送 < 3s
- **可用性**：99.99%（年停机 < 53分钟）
- **扩展性**：支持10倍用户增长，秒级弹性扩容

#### 1.3 容量估算
```
- 日活用户：1亿
- 每用户日均发帖：0.5条
- 每用户日均浏览：100条
- 日发帖量：5000万条
- 日读请求：100亿次（QPS峰值：11.5万 × 10倍 = 115万）
- 日写请求：5000万次（QPS峰值：578 × 10倍 = 5780）
- 存储需求：
  - 文本：50MB × 365天 = 18TB/年
  - 图片：5000万 × 50% × 500KB = 12.5TB/天
  - 视频：5000万 × 30% × 10MB = 150TB/天
  - 总计：约 60PB/年
```

---

### 二、整体架构设计

#### 2.1 全球多活架构

```
                       全球 DNS 智能解析
                              ↓
        ┌─────────────────────┴─────────────────────┐
        ↓                     ↓                     ↓
   北美数据中心          欧洲数据中心          亚太数据中心
   (主Region A)         (主Region B)         (主Region C)
        │                     │                     │
        └──────────── 跨数据中心同步 ─────────────────┘
                    (双向复制+冲突解决)
```

**设计原则：**
1. **就近接入**：通过GeoDNS将用户路由到最近的数据中心
2. **单元化架构**：每个Region独立提供完整服务能力
3. **数据分区**：
   - 用户主数据按UID哈希分片到Home Region
   - 内容数据按ContentID分片
   - 社交关系按图分区算法切割
4. **跨Region同步**：
   - 核心数据（用户资料、社交关系）：强同步（Paxos/Raft）
   - 内容数据（帖子、评论）：异步复制 + 最终一致性
   - 多媒体文件：就近存储 + CDN分发

#### 2.2 分层架构

```
┌─────────────────────────────────────────────────┐
│  客户端层：iOS/Android/Web/Desktop              │
└────────────┬────────────────────────────────────┘
             ↓
┌─────────────────────────────────────────────────┐
│  CDN + 边缘节点：静态资源、视频流、API加速       │
└────────────┬────────────────────────────────────┘
             ↓
┌─────────────────────────────────────────────────┐
│  接入层：API Gateway (Kong/Envoy)               │
│  - 认证授权、限流、协议转换、路由                │
└────────────┬────────────────────────────────────┘
             ↓
┌─────────────────────────────────────────────────┐
│  业务层：微服务集群 (K8s + Service Mesh)        │
│  ┌──────────┬──────────┬──────────┬──────────┐ │
│  │ User服务 │ Post服务 │ Feed服务 │ 社交图服务│ │
│  ├──────────┼──────────┼──────────┼──────────┤ │
│  │ 搜索服务 │ 通知服务 │ 推荐服务 │ 审核服务 │ │
│  └──────────┴──────────┴──────────┴──────────┘ │
└────────────┬────────────────────────────────────┘
             ↓
┌─────────────────────────────────────────────────┐
│  缓存层：Redis Cluster (L1) + Memcached (L2)    │
│  - 用户会话、热点数据、Feed缓存、计数器          │
└────────────┬────────────────────────────────────┘
             ↓
┌─────────────────────────────────────────────────┐
│  存储层：                                        │
│  - MySQL分片集群（元数据、关系）                 │
│  - 时序数据库（Feed流、日志）                    │
│  - 图数据库（社交关系）                          │
│  - 对象存储（OSS：图片/视频）                    │
│  - ElasticSearch（全文搜索）                     │
└─────────────────────────────────────────────────┘
```

---

### 三、核心模块详细设计

#### 3.1 用户系统（User Service）

**数据模型：**
```sql
-- 用户基础表（按UID分片）
CREATE TABLE user (
    uid BIGINT PRIMARY KEY,
    username VARCHAR(50) UNIQUE,
    email VARCHAR(100),
    phone VARCHAR(20),
    password_hash CHAR(60),
    avatar_url VARCHAR(255),
    bio TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    region VARCHAR(10),  -- 用户Home Region
    INDEX idx_username (username),
    INDEX idx_email (email)
) PARTITION BY HASH(uid) PARTITIONS 1024;

-- 用户扩展信息（KV存储，Redis/HBase）
uid:profile:{uid} -> JSON {
    "followers_count": 10000,
    "following_count": 500,
    "posts_count": 1200,
    "verified": true,
    "settings": {...}
}
```

**分片策略：**
- **UID生成**：Snowflake算法（64位 = 1位符号 + 41位时间戳 + 5位RegionID + 5位MachineID + 12位序列号）
- **跨Region访问**：
  ```
  用户A(北美) 访问 用户B(亚洲):
  1. 本地Redis缓存（命中率95%）
  2. 跨Region RPC（P99 < 100ms）
  3. 缓存预热（关注用户信息）
  ```

**关键优化：**
- **热点账号**：超大V（粉丝>100万）独立缓存集群，广播到所有Region
- **在线状态**：Redis Sorted Set + 心跳（TTL 5分钟）

---

#### 3.2 社交关系系统（Social Graph Service）

**挑战：**
- 超级大V：单个账号粉丝数亿（Lady Gaga式）
- 双向好友：好友上限5000（Facebook模式）

**方案一：推拉结合模型**

```
关注表（Following）- 推模型
┌─────┬──────────┬──────────┐
│ uid │ followee │ created  │
├─────┼──────────┼──────────┤
│ 100 │ 200      │ ...      │
│ 100 │ 300      │ ...      │
└─────┴──────────┴──────────┘
分片键：uid（按关注者分片）
索引：(uid, created)

粉丝表（Followers）- 拉模型
┌──────────┬─────┬──────────┐
│ followee │ uid │ created  │
├──────────┼─────┼──────────┤
│ 200      │ 100 │ ...      │
│ 200      │ 999 │ ...      │
└──────────┴─────┴──────────┘
分片键：followee（按被关注者分片）
索引：(followee, created)
```

**Feed流生成策略：**

| 用户类型 | 策略 | 实现 |
|---------|------|------|
| 普通用户（粉丝<1000） | 推模型 | 发帖时写入所有粉丝的Feed收件箱 |
| 中等影响力（1000-10万） | 推拉混合 | 活跃粉丝推送，僵尸粉拉取 |
| 超级大V（>10万） | 拉模型 | 不写粉丝收件箱，读时实时拉取 |

```go
// 发帖伪代码
func PublishPost(post Post) {
    // 1. 持久化帖子
    postID := db.SavePost(post)

    // 2. 异步写扩散（仅对非超级大V）
    if !IsSuperStar(post.UID) {
        followers := GetActiveFollowers(post.UID, limit=10000)
        kafka.Produce("feed-fanout", FanoutTask{
            PostID: postID,
            Followers: followers,
        })
    }

    // 3. 更新作者的发帖列表（拉取源头）
    redis.ZAdd(fmt.Sprintf("user:posts:%d", post.UID), postID, time.Now().Unix())
}

// 读取Feed流
func GetFeed(uid int64, cursor string, limit int) []Post {
    // 1. 从收件箱读取（推模式）
    inbox := redis.ZRevRangeByScore(fmt.Sprintf("feed:inbox:%d", uid), cursor, limit)
    posts := GetPostsByIDs(inbox)

    // 2. 拉取大V最新动态（拉模式）
    superStars := GetFollowedSuperStars(uid)
    for _, star := range superStars {
        latestPosts := redis.ZRevRange(fmt.Sprintf("user:posts:%d", star), 0, 10)
        posts = Merge(posts, latestPosts)
    }

    // 3. 排序+去重+截断
    return TopK(posts, limit)
}
```

**方案二：图数据库（备选）**

使用 Neo4j/JanusGraph 存储社交关系：
```cypher
// 查询二度人脉
MATCH (me:User {uid: 12345})-[:FOLLOWS]->(friend)-[:FOLLOWS]->(fof)
WHERE NOT (me)-[:FOLLOWS]->(fof)
RETURN fof
LIMIT 50
```

**适用场景**：推荐算法、共同好友、路径分析

---

#### 3.3 帖子系统（Post Service）

**数据模型：**
```sql
-- 帖子主表（按PostID分片）
CREATE TABLE post (
    post_id BIGINT PRIMARY KEY,
    uid BIGINT,
    content TEXT,
    media_ids JSON,  -- ["img_123", "video_456"]
    visibility ENUM('public', 'friends', 'private'),
    location VARCHAR(100),
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    INDEX idx_uid_time (uid, created_at DESC)
) PARTITION BY HASH(post_id) PARTITIONS 512;

-- 帖子统计表（Redis计数器 + MySQL定时落盘）
post:stats:{post_id} -> HASH {
    "likes": 1000,
    "comments": 50,
    "shares": 20,
    "views": 50000
}

-- 评论表（时序数据库 InfluxDB/Cassandra）
CREATE TABLE comment (
    comment_id BIGINT PRIMARY KEY,
    post_id BIGINT,
    uid BIGINT,
    content TEXT,
    parent_id BIGINT,  -- 支持楼中楼
    created_at TIMESTAMP
) WITH CLUSTERING ORDER BY (post_id, created_at DESC);
```

**写入流程：**
```
客户端上传
    ↓
对象存储（S3/OSS）← 多媒体文件
    ↓
Post Service
    ↓
├─→ MySQL（元数据）
├─→ Redis（缓存+计数）
├─→ Kafka（异步任务）
│     ├─→ Feed Fanout Worker
│     ├─→ 搜索索引同步（ES）
│     ├─→ 推荐系统特征提取
│     └─→ 内容审核队列
└─→ 返回客户端（post_id）
```

**读取优化：**
- **多级缓存**：
  ```
  L1: Redis (热点帖子，TTL=1h)
      命中率 80%
  L2: Memcached (温数据，TTL=24h)
      命中率 15%
  L3: MySQL (冷数据)
      命中率 5%
  ```
- **对象存储+CDN**：
  - 图片：WebP格式 + 多尺寸（缩略图/中图/原图）
  - 视频：HLS/DASH自适应码率 + 预转码（720p/1080p/4K）

**点赞/评论写入优化：**
```go
// 点赞（高并发优化）
func LikePost(uid, postID int64) error {
    // 1. 写入Redis Set（秒级响应）
    redis.SAdd(fmt.Sprintf("post:likes:%d", postID), uid)
    redis.Incr(fmt.Sprintf("post:stats:%d:likes", postID))

    // 2. 异步批量落盘MySQL（降低DB压力）
    kafka.Produce("like-events", LikeEvent{UID: uid, PostID: postID})

    return nil
}

// 后台合并worker（每10秒批量写入）
func LikeMergeWorker() {
    for batch := range kafka.Consume("like-events", batchSize=1000) {
        db.BatchInsert("post_likes", batch)  // 单次插入1000条
    }
}
```

---

#### 3.4 Feed流系统（Feed Service）

**核心挑战：**
- 每用户关注500人 × 每天发10条 = 5000条候选内容
- 实时性要求（新发帖3秒内送达）
- 个性化排序（非时间轴，基于兴趣+互动）

**方案：三段式架构**

```
┌──────────────────────────────────────────────┐
│ 1. 召回层 (Recall)                            │
│    多路召回：                                  │
│    - 关注用户最新帖子（时间序）               │
│    - 热门内容（点赞/评论数高）                │
│    - 个性化推荐（协同过滤）                   │
│    - 地理位置附近                              │
│    召回量：500-1000条                          │
└──────────────┬───────────────────────────────┘
               ↓
┌──────────────────────────────────────────────┐
│ 2. 粗排层 (Coarse Ranking)                   │
│    快速打分模型：                              │
│    Score = w1*时效性 + w2*亲密度 + w3*热度   │
│    保留Top 100                                 │
└──────────────┬───────────────────────────────┘
               ↓
┌──────────────────────────────────────────────┐
│ 3. 精排层 (Fine Ranking)                     │
│    深度学习模型（TensorFlow Serving）：       │
│    - 用户画像特征（年龄/性别/兴趣标签）        │
│    - 内容特征（文本语义/图片识别）            │
│    - 交互特征（历史点赞/停留时长）            │
│    输出：最终Feed流（20条）                   │
└──────────────────────────────────────────────┘
```

**缓存策略：**
```
用户打开App
    ↓
查询 feed:cache:{uid} （Redis）
    ↓
├─ 命中 → 直接返回（P50 < 50ms）
└─ 未命中 → 实时计算 → 写入缓存（TTL=5min）
                  ↓
             后台预计算（针对活跃用户）
             - 每小时更新一次
             - 覆盖70%日活用户
```

**数据结构：**
```redis
# Feed收件箱（Sorted Set按时间戳排序）
feed:inbox:{uid} -> ZSET {
    score: timestamp,
    member: post_id
}

# 容量控制：每用户最多保留1000条（滚动窗口）
ZREMRANGEBYRANK feed:inbox:{uid} 0 -1001
```

---

#### 3.5 搜索系统（Search Service）

**技术选型：ElasticSearch 集群**

**索引结构：**
```json
// post_index
{
  "mappings": {
    "properties": {
      "post_id": {"type": "long"},
      "uid": {"type": "long"},
      "content": {
        "type": "text",
        "analyzer": "ik_max_word",  // 中文分词
        "search_analyzer": "ik_smart"
      },
      "hashtags": {"type": "keyword"},
      "location": {"type": "geo_point"},
      "created_at": {"type": "date"},
      "likes_count": {"type": "integer"},
      "hot_score": {"type": "float"}  // 热度分数
    }
  }
}
```

**搜索查询：**
```json
// 综合搜索（文本+热度+时效性）
POST /post_index/_search
{
  "query": {
    "function_score": {
      "query": {
        "bool": {
          "must": [
            {"match": {"content": "人工智能"}}
          ],
          "filter": [
            {"range": {"created_at": {"gte": "now-7d"}}}
          ]
        }
      },
      "functions": [
        {"field_value_factor": {"field": "likes_count", "modifier": "log1p"}},
        {"gauss": {"created_at": {"origin": "now", "scale": "7d"}}}
      ]
    }
  },
  "sort": ["_score", {"created_at": "desc"}]
}
```

**实时性保障：**
- 帖子发布 → Kafka → Logstash → ES （延迟<3秒）
- 热点数据预加载到内存

---

#### 3.6 通知系统（Notification Service）

**通知类型：**
1. 互动通知（点赞/评论/@提及）
2. 关注通知（新粉丝/好友请求）
3. 系统通知（违规警告/活动推送）

**技术架构：**
```
事件源（Post/Like/Comment Service）
    ↓
Kafka 消息队列
    ↓
通知聚合服务（5秒窗口聚合）
    ↓
├─→ WebSocket长连接（在线用户）
├─→ APNs/FCM推送（离线用户）
└─→ 消息中心存储（MySQL+Redis）
```

**聚合策略：**
```
原始事件：
- UserA 点赞了你的帖子
- UserB 点赞了你的帖子
- UserC 点赞了你的帖子

聚合后：
- "UserA、UserB和1位好友 点赞了你的帖子"
```

**去重与限流：**
```go
// 每用户每小时最多50条推送
rateLimiter := redis.NewRateLimiter(
    key: fmt.Sprintf("push:limit:%d", uid),
    maxTokens: 50,
    refillRate: 50/hour,
)

// 防止重复推送（去重窗口10分钟）
dedupKey := fmt.Sprintf("push:dedup:%d:%s", uid, notificationType)
if redis.SetNX(dedupKey, 1, 10*time.Minute) {
    sendPush(notification)
}
```

---

### 四、关键技术难点解决

#### 4.1 热点数据处理

**问题：**
- 超级热点帖子（明星爆料）瞬间QPS=10万+
- 缓存穿透/击穿/雪崩

**解决方案：**

1. **多级缓存 + 本地缓存**
```
API Gateway (Nginx缓存，TTL=1s)
    ↓
应用层本地缓存（Caffeine，10万容量）
    ↓
Redis集群（16分片，主从+哨兵）
    ↓
MySQL
```

2. **热点发现与自动降级**
```go
// 滑动窗口统计QPS
func DetectHotKey(key string) bool {
    count := redis.Incr(fmt.Sprintf("qps:%s", key))
    redis.Expire(fmt.Sprintf("qps:%s", key), 1*time.Second)

    if count > 1000 {  // 超过阈值
        // 1. 本地缓存晋升
        localCache.Set(key, GetFromRedis(key), 10*time.Second)

        // 2. 告警
        alert("Hot key detected: " + key)

        return true
    }
    return false
}
```

3. **布隆过滤器防穿透**
```go
// 启动时加载所有PostID到布隆过滤器
bloomFilter := bloom.NewWithEstimates(10_000_000_000, 0.01)

func GetPost(postID int64) (*Post, error) {
    // 快速判断是否存在
    if !bloomFilter.Test([]byte(strconv.FormatInt(postID, 10))) {
        return nil, ErrNotFound
    }

    // 正常查询流程
    return getPostFromCache(postID)
}
```

#### 4.2 数据一致性

**分布式事务场景：**
```
发帖操作 = {
    1. 保存帖子到MySQL
    2. 写入Feed收件箱（Redis）
    3. 更新用户发帖计数
    4. 发送MQ消息（搜索索引）
}
```

**解决方案：本地消息表 + 最终一致性**

```sql
-- 本地消息表（与业务表在同一个数据库）
CREATE TABLE outbox_message (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    aggregate_id BIGINT,  -- 业务主键（post_id）
    event_type VARCHAR(50),
    payload JSON,
    status ENUM('PENDING', 'SENT', 'FAILED'),
    retry_count INT DEFAULT 0,
    created_at TIMESTAMP
);

-- 发帖事务
BEGIN;
    INSERT INTO post (...) VALUES (...);
    INSERT INTO outbox_message (aggregate_id, event_type, payload)
    VALUES (post_id, 'PostPublished', '{"post_id":123,...}');
COMMIT;

-- 后台worker扫描发送
SELECT * FROM outbox_message WHERE status='PENDING' LIMIT 100;
FOR EACH message:
    kafka.Send(message.payload)
    UPDATE outbox_message SET status='SENT' WHERE id=message.id
```

**跨Region数据同步：**
- 用户Profile修改 → 强同步（2PC/Paxos）
- 帖子内容 → 异步复制（Binlog订阅 → Kafka → 其他Region消费）
- 冲突解决：Last Write Wins（LWW，基于时间戳 + RegionID）

#### 4.3 长连接维持（消息推送）

**挑战：** 1亿在线用户 = 1亿TCP连接

**架构：**
```
客户端
    ↓
长连接网关集群（10000台，每台1万连接）
    ↓
消息路由服务（查询用户在哪台网关机器）
    ↓
Redis（uid -> gateway_server映射）
```

**优化措施：**
- 使用 epoll（Linux）/ kqueue（macOS）实现单机百万并发
- 心跳包压缩（60秒间隔，仅2字节）
- 连接迁移（服务器重启时主动通知客户端切换）

```go
// 伪代码
func HandleConnection(conn net.Conn, uid int64) {
    // 注册连接
    gatewayID := GetLocalGatewayID()
    redis.Set(fmt.Sprintf("conn:%d", uid), gatewayID, 24*time.Hour)

    // 订阅该用户的消息频道
    pubsub := redis.Subscribe(fmt.Sprintf("user:msg:%d", uid))

    for msg := range pubsub.Channel() {
        conn.Write(msg.Payload)
    }
}

// 发送消息
func SendMessage(uid int64, msg []byte) {
    gatewayID := redis.Get(fmt.Sprintf("conn:%d", uid))
    if gatewayID == "" {
        // 离线，存储到离线消息表
        return
    }

    // 发布到对应网关的频道
    redis.Publish(fmt.Sprintf("user:msg:%d", uid), msg)
}
```

---

### 五、性能优化与容量规划

#### 5.1 数据库优化

**MySQL分片：**
```
用户表：按 uid % 1024 分片（1024个物理库）
帖子表：按 post_id % 512 分片
关系表：按 uid % 512 分片（减少跨库Join）

单库容量：< 500GB
单表行数：< 5000万
```

**读写分离：**
```
写操作 → 主库（Master）
    ↓ 半同步复制
读操作 → 从库（Slave × 5）
    ↓ 异步复制
OLAP分析 → 离线从库（延迟1小时）
```

**慢查询优化：**
```sql
-- 覆盖索引（避免回表）
CREATE INDEX idx_uid_time ON post(uid, created_at)
INCLUDE (content, media_ids);

-- 分页优化（深分页问题）
-- 错误做法：
SELECT * FROM post WHERE uid=123 ORDER BY created_at DESC LIMIT 10000, 20;

-- 正确做法（游标分页）：
SELECT * FROM post
WHERE uid=123 AND created_at < '2024-01-01 10:00:00'
ORDER BY created_at DESC LIMIT 20;
```

#### 5.2 缓存优化

**Redis集群规划：**
```
- 集群规模：128节点（16分片 × 8副本）
- 单节点内存：256GB
- 总容量：32TB
- 网络：万兆网卡
```

**缓存更新策略：**
```go
// Cache-Aside模式（读多写少）
func GetPost(postID int64) (*Post, error) {
    // 1. 尝试从缓存读
    if post := cache.Get(postID); post != nil {
        return post, nil
    }

    // 2. 缓存未命中，查数据库
    post := db.GetPost(postID)

    // 3. 写回缓存（异步）
    go cache.Set(postID, post, 1*time.Hour)

    return post, nil
}

// Write-Through模式（写多读多）
func UpdatePost(post *Post) error {
    // 1. 同时更新DB和缓存
    db.Update(post)
    cache.Set(post.ID, post, 1*time.Hour)

    // 2. 发送失效消息到其他Region
    kafka.Send("cache-invalidate", post.ID)

    return nil
}
```

**缓存预热：**
```bash
# 每天凌晨3点预热热门数据
0 3 * * * /usr/local/bin/cache-warmup.sh

# 脚本内容
#!/bin/bash
# 1. 统计昨日Top 10000热门帖子
hot_posts=$(mysql -e "SELECT post_id FROM post_stats ORDER BY views DESC LIMIT 10000")

# 2. 批量加载到Redis
for post_id in $hot_posts; do
    curl "http://api.internal/cache/warmup?post_id=$post_id"
done
```

#### 5.3 CDN与静态资源

**架构：**
```
用户请求图片
    ↓
边缘CDN节点（全球1000+）← 命中率 95%
    ↓ 回源
区域CDN节点（10个） ← 命中率 4%
    ↓ 回源
源站对象存储（S3/OSS）← 命中率 1%
```

**图片处理链：**
```
用户上传原图（10MB）
    ↓
对象存储
    ↓
Lambda函数（触发式处理）
    ├─→ 生成缩略图（100KB，150x150）
    ├─→ 生成中图（500KB，800x800）
    ├─→ WebP转换（压缩50%）
    ├─→ 鉴黄识别（百度AI）
    └─→ 写入CDN预热队列
```

**视频处理：**
```
FFmpeg转码集群（K8s Job）
    ├─→ 720p (2Mbps)
    ├─→ 1080p (5Mbps)
    ├─→ 4K (20Mbps)
    └─→ HLS切片（10秒/片段）

存储格式：
/videos/
    /{video_id}/
        /720p/
            segment_0.ts
            segment_1.ts
            ...
        /1080p/...
        manifest.m3u8
```

---

### 六、监控与运维

#### 6.1 监控体系

**四个维度：**
```
1. 基础设施监控（Prometheus + Grafana）
   - CPU/内存/磁盘/网络
   - 容器/K8s集群状态

2. 应用监控（OpenTelemetry）
   - QPS/响应时间/错误率
   - 分布式链路追踪（Jaeger）

3. 业务监控
   - DAU/发帖量/互动率
   - 收入指标（广告点击）

4. 用户体验监控（RUM）
   - 前端性能（FCP/LCP/CLS）
   - API耗时分布
```

**告警规则：**
```yaml
groups:
- name: api-alerts
  rules:
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
    for: 5m
    annotations:
      summary: "API error rate > 5%"

  - alert: SlowResponse
    expr: histogram_quantile(0.99, http_request_duration_seconds) > 1
    for: 10m
    annotations:
      summary: "P99 latency > 1s"
```

#### 6.2 灰度发布

**流程：**
```
代码提交 → CI构建 → 镜像打包
    ↓
部署到金丝雀环境（1%流量）
    ↓ 观察30分钟（错误率/延迟/业务指标）
逐步扩大（5% → 25% → 50% → 100%）
    ↓ 任何阶段出现问题
自动回滚（保留上一版本镜像）
```

**K8s配置：**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: post-service
spec:
  selector:
    app: post-service
  ports:
  - port: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: post-service-v2  # 新版本
spec:
  replicas: 1  # 初始1个Pod
  selector:
    matchLabels:
      app: post-service
      version: v2
  template:
    metadata:
      labels:
        app: post-service
        version: v2
---
# Istio流量切分
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: post-service
spec:
  http:
  - match:
    - headers:
        x-canary:
          exact: "true"
    route:
    - destination:
        host: post-service
        subset: v2
      weight: 100
  - route:
    - destination:
        host: post-service
        subset: v1
      weight: 99
    - destination:
        host: post-service
        subset: v2
      weight: 1  # 灰度1%流量
```

#### 6.3 容灾与高可用

**多Region容灾：**
```
单Region故障 → DNS自动切换到其他Region（TTL=60s）
    ↓
数据恢复：
- 用户数据：从最近的Region同步（延迟<5分钟）
- 帖子数据：从备份Region拉取（可容忍10分钟延迟）
```

**数据库容灾：**
```
MySQL主从架构 + MHA（Master High Availability）
    ↓ 主库宕机
自动Failover（30秒内完成）
    ↓
1. MHA检测主库心跳丢失
2. 选择延迟最小的从库提升为主
3. 修复其他从库指向新主
4. 更新应用配置（VIP漂移）
```

**限流与降级：**
```go
// 令牌桶限流（Sentinel/Hystrix）
limiter := rate.NewLimiter(10000, 10000)  // 10000 QPS

func HandleRequest(ctx context.Context) error {
    if !limiter.Allow() {
        return ErrTooManyRequests  // 返回429
    }

    // 正常处理
    return processRequest(ctx)
}

// 降级策略
func GetFeed(uid int64) ([]Post, error) {
    // 1. 尝试完整流程
    posts, err := getPersonalizedFeed(uid)
    if err == nil {
        return posts, nil
    }

    // 2. 降级：返回热门Feed
    posts, err = getHotFeed()
    if err == nil {
        return posts, nil
    }

    // 3. 兜底：返回缓存的静态内容
    return getCachedStaticFeed(), nil
}
```

---

### 七、成本优化

#### 7.1 存储成本

**冷热数据分离：**
```
热数据（30天内）：SSD存储 + Redis缓存
    ↓ 30天后
温数据（1年内）：HDD存储 + 降低副本数
    ↓ 1年后
冷数据（>1年）：归档到对象存储（S3 Glacier）
    ↓ 3年后
删除或压缩（用户可申请找回）
```

**容量节省技巧：**
```
- 图片去重（MD5哈希，重复率约20%）
- 视频转码优化（H.265编码，节省50%带宽）
- 日志压缩（Gzip，压缩比10:1）
- 数据库字段优化（VARCHAR(255) → VARCHAR(50)）
```

#### 7.2 计算成本

**弹性伸缩：**
```yaml
# K8s HPA（Horizontal Pod Autoscaler）
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: post-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: post-service
  minReplicas: 10
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
```

**Spot实例使用：**
- 非关键任务（视频转码、数据分析）使用Spot实例，节省70%成本
- 关键服务（API/数据库）使用On-Demand或Reserved实例

#### 7.3 带宽成本

**优化措施：**
```
1. 启用HTTP/2（多路复用，减少连接数）
2. Brotli压缩（比Gzip再节省20%）
3. WebP/AVIF图片格式
4. 视频自适应码率（根据网络质量动态调整）
5. P2P分发（WebRTC，用户间互传，节省30%带宽）
```

---

### 八、安全与隐私

#### 8.1 认证授权

**OAuth 2.0 + JWT：**
```
用户登录
    ↓
Auth Service验证（密码/手机验证码/第三方登录）
    ↓
签发JWT（HS256/RS256）
{
  "sub": "user_123",
  "iat": 1704067200,
  "exp": 1704670000,
  "scope": ["read:feed", "write:post"]
}
    ↓
客户端存储Token（Cookie/LocalStorage）
    ↓
每次请求携带：Authorization: Bearer <token>
    ↓
API Gateway验证签名 + 过期时间
```

**权限控制（RBAC）：**
```sql
CREATE TABLE role (
    role_id INT PRIMARY KEY,
    role_name VARCHAR(50)  -- admin/moderator/user
);

CREATE TABLE permission (
    permission_id INT PRIMARY KEY,
    resource VARCHAR(50),  -- post/comment/user
    action VARCHAR(20)     -- create/read/update/delete
);

CREATE TABLE role_permission (
    role_id INT,
    permission_id INT,
    PRIMARY KEY (role_id, permission_id)
);

-- 判断权限
SELECT COUNT(*) FROM role_permission rp
JOIN user_role ur ON rp.role_id = ur.role_id
WHERE ur.uid = ?
  AND rp.permission_id = (
      SELECT permission_id FROM permission
      WHERE resource='post' AND action='delete'
  );
```

#### 8.2 内容安全

**审核流程：**
```
用户发帖
    ↓
机器审核（实时，延迟<500ms）
    ├─→ 文本：敏感词过滤（AC自动机）
    ├─→ 图片：鉴黄/暴恐识别（阿里云/腾讯云API）
    └─→ 视频：关键帧抽取 + 图片审核
    ↓
风险等级判定
    ├─→ 低风险：直接发布
    ├─→ 中风险：人工复审队列
    └─→ 高风险：自动拦截 + 账号警告
```

**敏感词库：**
```go
// AC自动机（Aho-Corasick）
ac := ahocorasick.NewTrieBuilder().
    AddStrings([]string{"违禁词1", "违禁词2", ...}).
    Build()

func FilterContent(text string) (string, bool) {
    matches := ac.Match(text)
    if len(matches) > 0 {
        // 替换为***
        for _, m := range matches {
            text = strings.Replace(text, m.MatchString, "***", -1)
        }
        return text, true  // 命中敏感词
    }
    return text, false
}
```

#### 8.3 数据隐私（GDPR合规）

**用户权利：**
```
1. 数据导出：
   POST /api/user/export
   → 生成包含所有个人数据的JSON文件

2. 数据删除（被遗忘权）：
   DELETE /api/user/me
   → 软删除账号 + 30天冷静期
   → 30天后物理删除（级联删除所有关联数据）

3. 数据可携带：
   支持导出为标准格式（ActivityPub协议）
```

**数据加密：**
```
- 传输加密：TLS 1.3
- 存储加密：
  - 敏感字段（手机号/邮箱）：AES-256-GCM
  - 数据库：透明数据加密（TDE）
  - 备份文件：GPG加密
- 密钥管理：AWS KMS/HashiCorp Vault
```

---

### 九、总结

**技术亮点：**
1. ✅ **多Region多活**：任意Region故障不影响全球服务
2. ✅ **推拉结合Feed流**：平衡实时性与系统负载
3. ✅ **多级缓存**：95%+命中率，P99延迟<100ms
4. ✅ **弹性扩展**：分钟级应对10倍流量峰值
5. ✅ **最终一致性**：通过消息队列保证数据最终同步

**关键指标：**
| 指标 | 目标 | 实现方案 |
|-----|------|---------|
| 可用性 | 99.99% | 多Region + 自动故障转移 |
| Feed加载 | <300ms | Redis缓存 + CDN |
| 发帖延迟 | <1s | 异步处理 + 本地消息表 |
| 存储成本 | <$0.01/GB/月 | 冷热分离 + 对象存储 |
| DAU | 1亿+ | 分片+缓存+CDN |

**未来演进：**
- 引入图神经网络（GNN）优化推荐算法
- 区块链技术实现内容确权
- 边缘计算降低跨洋延迟
- AI生成内容（AIGC）审核
