---
title: 设计一个分布式任务调度平台（类似 Airflow）
tags:
  - 分布式
  - 复杂系统设计
status: robot
class: 复杂系统设计
slug: distributed-task-scheduling-platform-design
ref:
---

## 核心要点

**分布式调度架构**:DAG工作流引擎+分布式任务执行+元数据管理+监控告警,支持任务依赖、失败重试、资源隔离

**关键技术栈**:DAG解析引擎、调度器(时间触发/依赖触发)、执行器(本地/分布式/Kubernetes)、元数据存储(MySQL/PostgreSQL)、消息队列(Redis/RabbitMQ)

**核心挑战**:任务依赖管理、调度性能、高可用性、资源调度、状态一致性、可观测性

---

## 详细回答

### 一、系统架构设计

#### 1.1 整体架构

```
┌─────────────────────────────────────────────────────────────┐
│                        Web UI / API                          │
│          (任务定义、监控、日志查看、手动触发)                    │
└──────────────────────┬──────────────────────────────────────┘
                       │
┌──────────────────────▼──────────────────────────────────────┐
│                   Scheduler (调度器)                          │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │ DAG Parser   │  │ Time Trigger │  │ Dependency   │      │
│  │ (DAG解析)    │  │ (时间触发)   │  │ Resolver     │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼ (推送任务到队列)
             ┌─────────────────┐
             │  Message Queue   │
             │  (Redis/RabbitMQ)│
             └─────────┬───────┘
                       │
        ┌──────────────┼──────────────┐
        │              │              │
        ▼              ▼              ▼
   ┌────────┐    ┌────────┐    ┌────────┐
   │Worker 1│    │Worker 2│    │Worker N│  (任务执行器集群)
   └────┬───┘    └────┬───┘    └────┬───┘
        │             │             │
        └─────────────┼─────────────┘
                      │
                      ▼ (写入状态、日志)
         ┌──────────────────────┐
         │   Metadata Database   │
         │   (MySQL/PostgreSQL)  │
         │  - DAG定义            │
         │  - 任务实例状态        │
         │  - 执行日志            │
         └──────────────────────┘
```

#### 1.2 核心组件

**1. DAG管理模块**
- **DAG定义**:用户通过Python/YAML定义任务依赖关系
- **DAG解析**:周期性扫描DAG文件,解析并加载到内存
- **DAG版本控制**:支持DAG版本管理,灰度发布

**2. 调度器(Scheduler)**
- **时间触发**:Cron表达式定时触发
- **依赖触发**:上游任务完成后触发下游任务
- **手动触发**:Web UI手动启动任务
- **优先级调度**:高优先级任务优先执行

**3. 执行器(Executor)**
- **LocalExecutor**:单机执行,适合开发测试
- **CeleryExecutor**:基于Celery的分布式执行
- **KubernetesExecutor**:动态创建K8s Pod执行任务
- **DaskExecutor**:基于Dask的并行计算

**4. 元数据管理**
- **DAG元数据**:DAG定义、任务列表、调度配置
- **任务实例**:每次执行的任务实例(状态、开始时间、结束时间)
- **执行日志**:标准输出/错误输出

**5. 监控告警**
- **任务状态监控**:成功/失败/运行中/超时
- **SLA监控**:任务执行时长超过阈值告警
- **告警通知**:邮件/Slack/钉钉/PagerDuty

### 二、DAG工作流引擎设计

#### 2.1 DAG定义(类Airflow)

```python
from datetime import datetime, timedelta
from scheduler import DAG, Task

# 定义DAG
dag = DAG(
    dag_id='data_pipeline',
    description='每日数据处理流水线',
    schedule_interval='0 2 * * *',  # 每天凌晨2点
    start_date=datetime(2025, 1, 1),
    catchup=False,  # 不补跑历史任务
    default_args={
        'owner': 'data-team',
        'retries': 3,
        'retry_delay': timedelta(minutes=5),
        'execution_timeout': timedelta(hours=2),
    }
)

# 定义任务
extract_task = Task(
    task_id='extract_data',
    task_type='BashOperator',
    command='python /scripts/extract.py',
    dag=dag
)

transform_task = Task(
    task_id='transform_data',
    task_type='PythonOperator',
    python_callable=transform_function,
    dag=dag
)

load_task = Task(
    task_id='load_to_warehouse',
    task_type='SQLOperator',
    sql='COPY INTO warehouse.table FROM s3://bucket/data',
    dag=dag
)

# 设置依赖关系
extract_task >> transform_task >> load_task
# 或者: transform_task.set_upstream(extract_task)
```

#### 2.2 DAG解析与验证

```python
class DAGParser:
    def parse_dag_file(self, file_path):
        """解析DAG文件"""
        # 1. 加载Python文件
        module = import_module_from_path(file_path)

        # 2. 提取DAG对象
        dags = []
        for attr_name in dir(module):
            attr = getattr(module, attr_name)
            if isinstance(attr, DAG):
                dags.append(attr)

        # 3. 验证DAG有效性
        for dag in dags:
            self.validate_dag(dag)

        return dags

    def validate_dag(self, dag):
        """验证DAG"""
        # 检查是否有循环依赖
        if self.has_cycle(dag):
            raise ValueError(f"DAG {dag.dag_id} has cycle dependency")

        # 检查任务ID唯一性
        task_ids = [task.task_id for task in dag.tasks]
        if len(task_ids) != len(set(task_ids)):
            raise ValueError(f"Duplicate task_id in DAG {dag.dag_id}")

        # 检查任务依赖是否存在
        for task in dag.tasks:
            for upstream_id in task.upstream_task_ids:
                if upstream_id not in task_ids:
                    raise ValueError(f"Task {task.task_id} depends on non-existent task {upstream_id}")

    def has_cycle(self, dag):
        """拓扑排序检测循环依赖"""
        in_degree = {task.task_id: 0 for task in dag.tasks}
        graph = {task.task_id: [] for task in dag.tasks}

        for task in dag.tasks:
            for upstream_id in task.upstream_task_ids:
                graph[upstream_id].append(task.task_id)
                in_degree[task.task_id] += 1

        # 拓扑排序
        queue = [task_id for task_id, degree in in_degree.items() if degree == 0]
        sorted_count = 0

        while queue:
            task_id = queue.pop(0)
            sorted_count += 1
            for downstream_id in graph[task_id]:
                in_degree[downstream_id] -= 1
                if in_degree[downstream_id] == 0:
                    queue.append(downstream_id)

        return sorted_count != len(dag.tasks)
```

### 三、调度器设计

#### 3.1 调度器架构

```python
class Scheduler:
    def __init__(self, executor, metadata_db):
        self.executor = executor
        self.metadata_db = metadata_db
        self.dag_bag = DAGBag()  # 所有DAG的集合

    def run(self):
        """调度主循环"""
        while True:
            # 1. 重新加载DAG文件(每分钟扫描一次)
            self.dag_bag.reload_dags_if_changed()

            # 2. 创建DAG运行实例
            self.create_dag_runs()

            # 3. 调度可执行任务
            self.schedule_tasks()

            # 4. 处理超时任务
            self.handle_timeouts()

            # 5. 清理历史数据
            self.cleanup_old_data()

            time.sleep(10)  # 每10秒调度一次

    def create_dag_runs(self):
        """根据调度间隔创建DAG运行实例"""
        for dag in self.dag_bag.dags.values():
            # 获取上次执行时间
            last_run = self.metadata_db.get_last_dag_run(dag.dag_id)

            # 计算下次执行时间
            next_run_time = self.calculate_next_run_time(dag, last_run)

            if datetime.now() >= next_run_time:
                # 创建DagRun实例
                dag_run = DagRun(
                    dag_id=dag.dag_id,
                    execution_date=next_run_time,
                    state='RUNNING',
                    external_trigger=False
                )
                self.metadata_db.save_dag_run(dag_run)

    def schedule_tasks(self):
        """调度可执行任务"""
        # 1. 获取所有运行中的DagRun
        running_dag_runs = self.metadata_db.get_running_dag_runs()

        for dag_run in running_dag_runs:
            dag = self.dag_bag.get_dag(dag_run.dag_id)

            # 2. 获取所有任务实例
            task_instances = self.metadata_db.get_task_instances(dag_run)

            # 3. 找出可执行任务(上游任务都成功)
            executable_tasks = []
            for task in dag.tasks:
                ti = self.get_task_instance(task_instances, task.task_id)

                if ti.state == 'SCHEDULED':  # 已调度但未执行
                    if self.are_dependencies_met(task, task_instances):
                        executable_tasks.append(ti)

            # 4. 提交任务到执行器
            for ti in executable_tasks:
                ti.state = 'QUEUED'
                self.metadata_db.update_task_instance(ti)
                self.executor.queue_task(ti)

            # 5. 检查DagRun是否完成
            if self.is_dag_run_finished(task_instances):
                dag_run.state = 'SUCCESS' if all(ti.state == 'SUCCESS' for ti in task_instances) else 'FAILED'
                self.metadata_db.update_dag_run(dag_run)

    def are_dependencies_met(self, task, task_instances):
        """检查任务依赖是否满足"""
        for upstream_task_id in task.upstream_task_ids:
            upstream_ti = self.get_task_instance(task_instances, upstream_task_id)
            if upstream_ti.state != 'SUCCESS':
                return False
        return True
```

#### 3.2 时间触发实现

```python
class CronScheduler:
    @staticmethod
    def calculate_next_run_time(dag, last_run_time):
        """根据Cron表达式计算下次执行时间"""
        # 支持Cron表达式: '0 2 * * *' (每天2点)
        # 支持时间间隔: '@daily', '@hourly', timedelta(hours=1)

        if dag.schedule_interval is None:
            return None  # 手动触发

        if dag.schedule_interval == '@once':
            return dag.start_date if last_run_time is None else None

        if dag.schedule_interval == '@hourly':
            return last_run_time + timedelta(hours=1)

        if dag.schedule_interval == '@daily':
            return last_run_time + timedelta(days=1)

        if isinstance(dag.schedule_interval, timedelta):
            return last_run_time + dag.schedule_interval

        # Cron表达式解析(使用croniter库)
        from croniter import croniter
        base_time = last_run_time if last_run_time else dag.start_date
        cron = croniter(dag.schedule_interval, base_time)
        return cron.get_next(datetime)
```

### 四、分布式执行器实现

#### 4.1 基于Celery的分布式执行器

```python
from celery import Celery

celery_app = Celery(
    'scheduler',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1'
)

@celery_app.task
def execute_task(task_instance_dict):
    """Celery任务:执行一个TaskInstance"""
    from scheduler.models import TaskInstance

    # 1. 反序列化TaskInstance
    ti = TaskInstance.from_dict(task_instance_dict)

    # 2. 标记为运行中
    ti.state = 'RUNNING'
    ti.start_date = datetime.now()
    ti.hostname = socket.gethostname()
    ti.pid = os.getpid()
    db.session.merge(ti)
    db.session.commit()

    try:
        # 3. 执行任务
        task = ti.task
        if task.task_type == 'BashOperator':
            result = subprocess.run(task.command, shell=True, capture_output=True)
            if result.returncode != 0:
                raise Exception(result.stderr.decode())

        elif task.task_type == 'PythonOperator':
            task.python_callable()

        elif task.task_type == 'SQLOperator':
            conn = get_db_connection(task.connection_id)
            conn.execute(task.sql)

        # 4. 标记为成功
        ti.state = 'SUCCESS'
        ti.end_date = datetime.now()

    except Exception as e:
        # 5. 失败处理
        ti.state = 'FAILED'
        ti.end_date = datetime.now()
        ti.error_message = str(e)

        # 重试逻辑
        if ti.try_number < ti.max_retries:
            ti.state = 'UP_FOR_RETRY'
            ti.try_number += 1
            # 延迟重试
            execute_task.apply_async(args=[task_instance_dict], countdown=ti.retry_delay.total_seconds())

    finally:
        db.session.merge(ti)
        db.session.commit()

class CeleryExecutor:
    def queue_task(self, task_instance):
        """提交任务到Celery队列"""
        execute_task.apply_async(args=[task_instance.to_dict()])

    def get_task_status(self, task_id):
        """查询任务状态"""
        async_result = AsyncResult(task_id, app=celery_app)
        return async_result.state
```

#### 4.2 基于Kubernetes的执行器

```python
from kubernetes import client, config

class KubernetesExecutor:
    def __init__(self):
        config.load_kube_config()
        self.k8s_client = client.CoreV1Api()
        self.batch_client = client.BatchV1Api()

    def queue_task(self, task_instance):
        """在K8s中创建Job执行任务"""
        job_manifest = self.create_job_manifest(task_instance)
        self.batch_client.create_namespaced_job(
            namespace='scheduler',
            body=job_manifest
        )

    def create_job_manifest(self, ti):
        """生成K8s Job配置"""
        return client.V1Job(
            api_version="batch/v1",
            kind="Job",
            metadata=client.V1ObjectMeta(
                name=f"{ti.dag_id}-{ti.task_id}-{ti.execution_date}",
                labels={
                    "dag_id": ti.dag_id,
                    "task_id": ti.task_id,
                }
            ),
            spec=client.V1JobSpec(
                template=client.V1PodTemplateSpec(
                    spec=client.V1PodSpec(
                        containers=[
                            client.V1Container(
                                name="task",
                                image="scheduler-worker:latest",
                                command=["python", "execute_task.py"],
                                env=[
                                    client.V1EnvVar(name="TASK_ID", value=ti.task_id),
                                    client.V1EnvVar(name="DAG_ID", value=ti.dag_id),
                                    client.V1EnvVar(name="EXECUTION_DATE", value=str(ti.execution_date)),
                                ],
                                resources=client.V1ResourceRequirements(
                                    requests={"memory": "512Mi", "cpu": "500m"},
                                    limits={"memory": "2Gi", "cpu": "2000m"}
                                )
                            )
                        ],
                        restart_policy="Never"
                    )
                ),
                backoff_limit=ti.max_retries
            )
        )
```

### 五、元数据管理

#### 5.1 数据库Schema设计

```sql
-- DAG表
CREATE TABLE dag (
    dag_id VARCHAR(250) PRIMARY KEY,
    description TEXT,
    schedule_interval VARCHAR(100),
    start_date TIMESTAMP,
    end_date TIMESTAMP,
    is_paused BOOLEAN DEFAULT FALSE,
    owners TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- DAG运行实例表
CREATE TABLE dag_run (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    dag_id VARCHAR(250),
    execution_date TIMESTAMP,
    state VARCHAR(50), -- RUNNING/SUCCESS/FAILED
    start_date TIMESTAMP,
    end_date TIMESTAMP,
    external_trigger BOOLEAN DEFAULT FALSE,
    UNIQUE KEY (dag_id, execution_date),
    INDEX idx_state (state),
    INDEX idx_execution_date (execution_date)
);

-- 任务实例表
CREATE TABLE task_instance (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    task_id VARCHAR(250),
    dag_id VARCHAR(250),
    execution_date TIMESTAMP,
    state VARCHAR(50), -- SCHEDULED/QUEUED/RUNNING/SUCCESS/FAILED/UP_FOR_RETRY
    try_number INT DEFAULT 1,
    max_retries INT DEFAULT 0,
    start_date TIMESTAMP,
    end_date TIMESTAMP,
    duration FLOAT,
    hostname VARCHAR(100),
    pid INT,
    error_message TEXT,
    UNIQUE KEY (task_id, dag_id, execution_date, try_number),
    INDEX idx_state_date (state, execution_date)
);

-- 任务日志表
CREATE TABLE task_log (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    task_id VARCHAR(250),
    dag_id VARCHAR(250),
    execution_date TIMESTAMP,
    try_number INT,
    log_content LONGTEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### 5.2 状态机管理

```python
class TaskState:
    SCHEDULED = 'SCHEDULED'    # 已调度
    QUEUED = 'QUEUED'          # 已加入队列
    RUNNING = 'RUNNING'        # 运行中
    SUCCESS = 'SUCCESS'        # 成功
    FAILED = 'FAILED'          # 失败
    UP_FOR_RETRY = 'UP_FOR_RETRY'  # 等待重试
    SKIPPED = 'SKIPPED'        # 跳过
    UPSTREAM_FAILED = 'UPSTREAM_FAILED'  # 上游失败

    # 状态转移规则
    TRANSITIONS = {
        SCHEDULED: [QUEUED, SKIPPED, UPSTREAM_FAILED],
        QUEUED: [RUNNING],
        RUNNING: [SUCCESS, FAILED, UP_FOR_RETRY],
        UP_FOR_RETRY: [QUEUED],
        FAILED: [],
        SUCCESS: [],
    }

    @classmethod
    def can_transition(cls, from_state, to_state):
        return to_state in cls.TRANSITIONS.get(from_state, [])
```

### 六、高可用与容错设计

#### 6.1 调度器高可用

**主备模式**:
```python
class SchedulerHA:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.scheduler_id = str(uuid.uuid4())
        self.heartbeat_interval = 10  # 秒

    def run(self):
        """尝试成为主调度器"""
        while True:
            # 尝试获取分布式锁
            is_leader = self.redis.set(
                'scheduler:leader',
                self.scheduler_id,
                nx=True,  # 只在key不存在时设置
                ex=30     # 30秒过期
            )

            if is_leader:
                logger.info(f"Scheduler {self.scheduler_id} became leader")
                self.run_as_leader()
            else:
                logger.info(f"Scheduler {self.scheduler_id} is standby")
                time.sleep(self.heartbeat_interval)

    def run_as_leader(self):
        """作为主调度器运行"""
        try:
            while True:
                # 执行调度逻辑
                self.schedule()

                # 续约leader锁
                self.redis.expire('scheduler:leader', 30)

                time.sleep(5)
        except Exception as e:
            logger.error(f"Scheduler error: {e}")
            # 主动释放锁
            self.redis.delete('scheduler:leader')
```

#### 6.2 任务失败重试

```python
class RetryPolicy:
    @staticmethod
    def calculate_retry_delay(ti):
        """计算重试延迟(指数退避)"""
        base_delay = ti.task.retry_delay.total_seconds()
        exponential_backoff = ti.task.retry_exponential_backoff

        if exponential_backoff:
            # 指数退避: delay * 2^(try_number - 1)
            return base_delay * (2 ** (ti.try_number - 1))
        else:
            return base_delay

    @staticmethod
    def should_retry(ti):
        """判断是否应该重试"""
        if ti.try_number >= ti.max_retries:
            return False

        # 检查异常类型是否在可重试列表中
        if ti.task.retry_on_exceptions:
            return ti.exception_type in ti.task.retry_on_exceptions

        return True
```

### 七、监控与告警

#### 7.1 监控指标

```python
from prometheus_client import Counter, Histogram, Gauge

# 任务执行次数
task_runs_total = Counter(
    'task_runs_total',
    'Total number of task runs',
    ['dag_id', 'task_id', 'state']
)

# 任务执行时长
task_duration_seconds = Histogram(
    'task_duration_seconds',
    'Task execution duration in seconds',
    ['dag_id', 'task_id']
)

# 队列中的任务数
queued_tasks = Gauge(
    'queued_tasks',
    'Number of tasks in queue'
)

# DAG运行成功率
dag_success_rate = Gauge(
    'dag_success_rate',
    'DAG success rate in last 24 hours',
    ['dag_id']
)
```

#### 7.2 SLA监控与告警

```python
class SLAMonitor:
    def check_sla_miss(self):
        """检查SLA违规"""
        # 查询超过SLA的任务
        overdue_tasks = self.db.query(TaskInstance).filter(
            TaskInstance.state == 'RUNNING',
            TaskInstance.start_date + TaskInstance.sla < datetime.now()
        ).all()

        for ti in overdue_tasks:
            self.send_sla_alert(ti)

    def send_sla_alert(self, task_instance):
        """发送SLA告警"""
        message = f"""
        SLA Miss Alert:
        DAG: {task_instance.dag_id}
        Task: {task_instance.task_id}
        Execution Date: {task_instance.execution_date}
        Expected Duration: {task_instance.sla}
        Actual Duration: {datetime.now() - task_instance.start_date}
        """

        # 发送到多个渠道
        self.send_email(task_instance.owners, message)
        self.send_slack(message)
        self.create_pagerduty_incident(message)
```

### 八、性能优化

#### 8.1 调度性能优化

**批量调度**:
```python
def schedule_tasks_batch(self):
    """批量调度任务,减少数据库查询"""
    # 一次性查询所有运行中的DagRun
    dag_runs = self.db.query(DagRun).filter(DagRun.state == 'RUNNING').all()

    # 批量查询任务实例
    execution_dates = [dr.execution_date for dr in dag_runs]
    all_task_instances = self.db.query(TaskInstance).filter(
        TaskInstance.execution_date.in_(execution_dates)
    ).all()

    # 按DagRun分组
    ti_by_dag_run = defaultdict(list)
    for ti in all_task_instances:
        ti_by_dag_run[(ti.dag_id, ti.execution_date)].append(ti)

    # 批量调度
    tasks_to_queue = []
    for dag_run in dag_runs:
        key = (dag_run.dag_id, dag_run.execution_date)
        task_instances = ti_by_dag_run[key]
        tasks_to_queue.extend(self.find_executable_tasks(dag_run, task_instances))

    # 批量提交到执行器
    self.executor.queue_tasks_batch(tasks_to_queue)
```

#### 8.2 元数据查询优化

```python
# 1. 分区表优化(按execution_date分区)
ALTER TABLE task_instance PARTITION BY RANGE (TO_DAYS(execution_date)) (
    PARTITION p_2025_01 VALUES LESS THAN (TO_DAYS('2025-02-01')),
    PARTITION p_2025_02 VALUES LESS THAN (TO_DAYS('2025-03-01')),
    ...
);

# 2. 索引优化
CREATE INDEX idx_dag_state_date ON dag_run(dag_id, state, execution_date);
CREATE INDEX idx_task_state ON task_instance(dag_id, task_id, state);

# 3. 查询缓存(Redis)
def get_dag_run_cached(self, dag_id, execution_date):
    cache_key = f"dag_run:{dag_id}:{execution_date}"
    cached = redis.get(cache_key)
    if cached:
        return json.loads(cached)

    dag_run = self.db.query(DagRun).filter_by(
        dag_id=dag_id,
        execution_date=execution_date
    ).first()

    redis.setex(cache_key, 300, json.dumps(dag_run.to_dict()))
    return dag_run
```

### 九、实战案例:数据仓库ETL流水线

```python
from datetime import datetime, timedelta
from scheduler import DAG, BashOperator, PythonOperator, SQLOperator, BranchOperator

dag = DAG(
    'dwh_etl_pipeline',
    schedule_interval='0 3 * * *',  # 每天凌晨3点
    start_date=datetime(2025, 1, 1),
    default_args={
        'retries': 2,
        'retry_delay': timedelta(minutes=5),
        'email_on_failure': True,
        'email': ['data-team@company.com'],
        'sla': timedelta(hours=2),  # SLA: 2小时内完成
    }
)

# 1. 数据提取
extract_mysql = BashOperator(
    task_id='extract_from_mysql',
    bash_command='sqoop import --connect jdbc:mysql://db/prod --table orders --target-dir /data/orders',
    dag=dag
)

extract_api = PythonOperator(
    task_id='extract_from_api',
    python_callable=fetch_external_api_data,
    dag=dag
)

# 2. 数据质量检查
def data_quality_check(**context):
    df = pd.read_csv('/data/orders')
    assert len(df) > 0, "Empty data"
    assert df['order_id'].duplicated().sum() == 0, "Duplicate orders"
    return 'transform_data'  # 返回下游任务ID

quality_check = BranchOperator(
    task_id='quality_check',
    python_callable=data_quality_check,
    dag=dag
)

# 3. 数据转换
transform = BashOperator(
    task_id='transform_data',
    bash_command='spark-submit /scripts/transform.py',
    dag=dag
)

# 4. 加载到数仓
load_fact_table = SQLOperator(
    task_id='load_fact_orders',
    sql='''
        INSERT INTO dwh.fact_orders
        SELECT * FROM staging.orders
        WHERE order_date = '{{ ds }}'
    ''',
    conn_id='snowflake_conn',
    dag=dag
)

load_dim_table = SQLOperator(
    task_id='load_dim_customers',
    sql='MERGE INTO dwh.dim_customers ...',
    conn_id='snowflake_conn',
    dag=dag
)

# 5. 数据验证
validate = PythonOperator(
    task_id='validate_results',
    python_callable=validate_dwh_data,
    dag=dag
)

# 6. 发送报告
send_report = PythonOperator(
    task_id='send_daily_report',
    python_callable=send_email_report,
    dag=dag
)

# 定义依赖关系
[extract_mysql, extract_api] >> quality_check >> transform
transform >> [load_fact_table, load_dim_table] >> validate >> send_report
```

### 十、与Airflow对比

| 特性 | Airflow | 自研平台 |
|------|---------|----------|
| DAG定义 | Python代码 | Python/YAML/UI可视化 |
| 执行器 | Local/Celery/K8s/Dask | 可自定义(K8s/容器/虚拟机) |
| 调度算法 | 时间触发+依赖触发 | 增加资源感知调度/优先级队列 |
| 元数据存储 | MySQL/PostgreSQL | 支持分布式数据库(TiDB) |
| 权限管理 | RBAC | 集成公司统一认证系统 |
| 可观测性 | Web UI + 日志 | 集成公司监控平台(Prometheus+Grafana) |
| 资源隔离 | K8s Namespace | 支持多租户资源配额 |

### 十一、技术栈总结

| 组件 | 技术选型 | 说明 |
|------|----------|------|
| Web框架 | Flask/Django | API和UI |
| 调度器 | 自研调度引擎 | 核心调度逻辑 |
| 消息队列 | Redis/RabbitMQ | 任务队列 |
| 分布式执行 | Celery/K8s | Worker集群 |
| 元数据存储 | MySQL/PostgreSQL | DAG和任务状态 |
| 日志存储 | Elasticsearch | 任务执行日志 |
| 监控告警 | Prometheus+Grafana | 指标监控 |
| 配置管理 | Consul/etcd | 动态配置 |

### 十二、面试回答框架

**回答结构**:
1. **架构概述**:调度器+执行器+元数据管理,类似Airflow三层架构
2. **DAG引擎**:DAG定义、解析、循环检测、拓扑排序
3. **调度策略**:时间触发(Cron)+依赖触发+优先级调度
4. **分布式执行**:Celery分布式队列或K8s动态Pod
5. **高可用**:调度器主备+任务重试+状态持久化
6. **监控告警**:SLA监控+任务状态监控+Prometheus指标
7. **性能优化**:批量调度+元数据缓存+分区表

**加分点**:
- 提到具体的循环依赖检测算法(拓扑排序)
- 讲清楚状态机管理和状态转移规则
- 说明与Airflow的差异化设计(如资源感知调度、多租户隔离)
